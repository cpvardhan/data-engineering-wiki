{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":"\ud83d\udcda Data Engineering Wiki \ud83d\udc4b About Me <p>       Welcome to my data-engineering wiki! I'm a Power BI Developer with 3 years of experience,       now diving deep into Azure data engineering and Microsoft Fabric.     </p> <p>       This site is my public knowledge base &amp; portfolio as I up-skill from analyst to full-stack DE.       Let\u2019s connect on LinkedIn.     </p> \ud83d\uddc2\ufe0f Course Library Fundamentals of Data Engineering Python Power BI Service &amp; Admin Cheat Sheets SQL Admin \ud83d\uddfa\ufe0f Learning Road-map (30 weeks) <p>Click any module card for detailed tasks &amp; resources.</p> Module-1 DW &amp; SQL Foundations Module-2 Python ETL &amp; CI Module-3 Spark &amp; Delta Performance Module-4 Streaming &amp; Data Quality Module-5 Azure Lakehouse Module-6 ADF &amp; Synapse Module-7 Fabric Lakehouse &amp; Real-Time Module-8 Interview &amp; System Design"},{"location":"cheat-sheets/cheatsheet-overview/","title":"\ud83d\udcdd Cheat Sheets","text":"<p>Quick-reference pages for busy data pros.</p> Topic Link Power BI Service Admin powerbi-service-cheatsheet <p>More coming soon\u2026</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/","title":"Power BI Service Admin","text":""},{"location":"cheat-sheets/powerbi-service-cheatsheet/#power-bi-service-cheat-sheet","title":"\ud83d\ude80 Power BI Service Cheat Sheet","text":"<p>Master the essential concepts of Power BI Service for publishing, sharing, and collaborating on your data insights. </p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#power-bi-service-overview-the-cloud-command-center","title":"\ud83d\udfe1 Power BI Service Overview: The Cloud Command Center","text":"<p>The Power BI Service is the secure, cloud-based platform by Microsoft that serves as the central hub for your business intelligence ecosystem. It's where the magic of data sharing, collaboration, and consumption truly happens after you've built your reports in Power BI Desktop.</p> <ul> <li>\ud83d\udcc1 Repository: Host your .PBIX files\u2014reports, dashboards, datasets\u2014all in one place.  </li> <li>\ud83d\udd12 Secure Sharing: Seamless collaboration across your organization (and beyond).  </li> <li>\u23f0 Refresh &amp; Alerts: Automated data refreshes and data-driven alerts keep insights up to date.  </li> <li>\ud83c\udf10 Anywhere Access: Consume via web browser or mobile app on any device.</li> </ul> <pre><code>sequenceDiagram\n\n    participant DS   as \"Data Sources\"\n    participant PBD  as \"Power BI Desktop\"\n    participant PBIS as \"Power BI Service\"\n    participant EU   as \"End Users\"\n\n    PBD  -&gt;&gt; PBIS : Publish Report (.PBIX)\n    PBIS -&gt;&gt; EU   : Share &amp; Collaborate\n    EU   -&gt;&gt; PBIS : Consume via Web/Mobile\n    PBIS -&gt;&gt; DS   : Schedule Refresh (via Gateway)\n</code></pre> \ud83c\udf0a Power BI Ecosystem Flow"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#workspaces-your-collaborative-sandboxes","title":"\ud83d\uddc2\ufe0f Workspaces: Your Collaborative Sandboxes","text":"<p>A Workspace is a container for organizing and collaborating on related Power BI content (Reports, Dashboards, Datasets, Dataflows).</p> <ul> <li>My Workspace: Personal sandbox\u2014private, not directly shareable.  </li> <li>Shared/App Workspace: Team collaboration hub; source for Power BI Apps.  </li> <li>Capacity: </li> <li>Pro license: 10 GB per workspace  </li> <li>Premium/Fabric: up to 100 TB  </li> </ul> <pre><code>graph LR\n    A[Workspace]\n    A -- Private --&gt; B(My Workspace)\n    A -- Collaborative --&gt; C(Shared/App Workspace)\n    C -- Publishes --&gt; D(Power BI Apps)</code></pre> \ud83d\udcca Workspace Types <p>\ud83d\udca1 When to Use: - My Workspace: Drafts &amp; personal exploration. - Shared/App Workspace: Team builds &amp; published Apps.</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#reports-the-interactive-storytellers","title":"\ud83d\udcca Reports: The Interactive Storytellers","text":"<p>A Report is a multi-page document of visuals, charts, and tables from one dataset.</p> <ul> <li>Built in Power BI Desktop, published to Service.  </li> <li>Rich interactivity: filters, slicers, drill-through.  </li> <li>Ideal for deep exploration &amp; detailed storytelling.</li> </ul> <p>\ud83d\udca1 Use Reports When: - You need multi-page, interactive analysis. - Users must slice/dice and drill down.</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#dashboards-executive-snapshots","title":"\ud83d\uddbc\ufe0f Dashboards: Executive Snapshots","text":"<p>A Dashboard is a single-page canvas of pinned visuals (tiles) from one or more reports.</p> <ul> <li>Created in Power BI Service only.  </li> <li>Combines visuals across reports/datasets.  </li> <li>Perfect for high-level KPI monitoring and data-driven alerts.</li> </ul> <p>\ud83d\udca1 Use Dashboards When: - You need a one-page executive overview. - Metrics must trigger immediate notifications.</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#report-vs-dashboard-a-quick-comparison","title":"\ud83c\udd9a Report vs Dashboard: A Quick Comparison","text":"Feature Report Dashboard Pages Multi-page for deep analysis Single-page \u201cone-pager\u201d Creation Tool Power BI Desktop Power BI Service only Underlying Data Single dataset Multiple reports/datasets Interactivity High (filters, slicers, drill-through) Limited (clicking tile navigates) Use Case Detailed analysis &amp; deep dives KPI monitoring &amp; quick checks Alerts \u274c \u2705 data-driven alerts"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#power-bi-apps-simplified-distribution","title":"\ud83d\udce6 Power BI Apps: Simplified Distribution","text":"<p>A Power BI App bundles curated reports and dashboards for broad consumption.</p> <ul> <li>Built in App Workspaces; delivers a view-only experience.  </li> <li>Simplifies sharing with large audiences without workspace access.</li> </ul> <p>\ud83d\udca1 When to Use Apps: - To distribute polished content at scale. - For version-controlled, production-ready experiences.</p> <p>\ud83d\udcb3 Viewer Licensing </p> Viewer License Required? Scenario Free \u2705 If App is on Premium/Fabric capacity Pro \u2705 Standard Pro workspace (publisher &amp; viewer both need Pro) PPU \u2705 Personal Premium (paginated reports, AI, 48 refreshes/day)"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#power-bi-license-types-pricing","title":"\ud83d\udcb3 Power BI License Types &amp; Pricing","text":"<p>Microsoft offers flexible licensing for various needs, from individual use to enterprise-wide deployments.</p> License Type Cost (USD) Sharing Key Features / Use Case Free $0 \u274c no sharing Individual analysis; view content in Premium capacity Pro ~$10/user/month \u2705 Pro\u2194Pro Team collaboration; shared workspaces; 8 refreshes/day PPU ~$20/user/month \u2705 PPU\u2194PPU All Pro + Premium features (paginated, AI) ; 48 refreshes/day Premium/Fabric From $262/month \u2705 with Free Dedicated resources; enterprise-scale; advanced AI; auto-scaling <p>Costs approximate; vary by region &amp; enterprise agreements.</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#power-bi-security-safeguarding-your-data","title":"\ud83d\udd10 Power BI Security: Safeguarding Your Data","text":""},{"location":"cheat-sheets/powerbi-service-cheatsheet/#workspace-roles","title":"Workspace Roles","text":"<p>Assign roles to control content management:</p> Role Key Capabilities Admin Full control: manage access, publish apps, delete content Member Create/edit content, publish apps, schedule refreshes; cannot manage access Contributor Create/edit content; cannot publish apps\u2014ideal for devs Viewer View-only access to published content <pre><code>sequenceDiagram\n    participant User\n    participant Report\n    participant Dataset\n    participant RLS as RLS Roles (DAX)\n\n    User-&gt;&gt;Report: Access Report\n    Report-&gt;&gt;Dataset: Request Data\n    Dataset-&gt;&gt;RLS: Check Role\n    RLS--&gt;&gt;Dataset: Apply Filter\n    Dataset--&gt;&gt;Report: Return Filtered Data\n    Report--&gt;&gt;User: Display Filtered View</code></pre> \ud83d\udd12 RLS Flow <p>Row-Level Security (RLS): Defined in Desktop, managed in Service to filter rows per user.</p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#additional-security-layers-best-practices","title":"\ud83d\udee1\ufe0f Additional Security Layers &amp; Best Practices","text":"Security Layer Description Authentication Azure Active Directory (AAD) identity verification Authorization Workspace roles &amp; RLS govern data access Encryption Data encrypted at rest &amp; in transit (HTTPS/TLS) Tenant Settings Admin controls for export, sharing, and sensitivity policies Sensitivity Labels Classification via Microsoft Purview (e.g., \u201cConfidential\u201d) <p>Best Practices: - Principle of least privilege - Use dynamic RLS for scalability - Enforce MFA for all users - Apply sensitivity labels to sensitive reports - Audit workspace access &amp; usage logs regularly  </p>"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#data-refresh-gateways-keeping-insights-fresh","title":"\ud83d\udd04 Data Refresh &amp; Gateways: Keeping Insights Fresh","text":"<ul> <li>Data Refresh: Updates service datasets from sources.  </li> <li>Data Gateway: Secure bridge to on-premises data (SQL Server, local files).  </li> <li>Scheduled Refresh: Pro (8/day), Premium/Fabric (48/day).</li> </ul> <pre><code>sequenceDiagram\n    participant PBIS as Power BI Service\n    participant GW as Data Gateway\n    participant ODS as On-Prem Data Source\n\n    PBIS-&gt;&gt;GW: Request Refresh\n    GW-&gt;&gt;ODS: Fetch Data\n    ODS--&gt;&gt;GW: Return Data\n    GW--&gt;&gt;PBIS: Forward Data\n    PBIS-&gt;&gt;PBIS: Update Dataset</code></pre> \ud83d\udd01 Refresh Flow"},{"location":"cheat-sheets/powerbi-service-cheatsheet/#other-key-service-components","title":"\u2699\ufe0f Other Key Service Components","text":"<ul> <li>Datasets: Import vs DirectQuery modes for data storage &amp; query.  </li> <li>Dataflows: Self-service ETL to prepare &amp; reuse data.  </li> <li>Datamarts: Managed Azure SQL DB for analytics between flows &amp; warehouses.  </li> <li>Deployment Pipelines: (Premium/PPU) Manage content lifecycle across dev\u2192test\u2192prod.</li> </ul> <pre><code>sequenceDiagram\n    participant RawData\n    participant Dataflows\n    participant Datasets\n    participant Datamarts\n    participant Reports\n\n    RawData-&gt;&gt;Dataflows: Prepare Data\n    Dataflows-&gt;&gt;Datasets: Load Data\n    Dataflows-&gt;&gt;Datamarts: Load Data\n    Datasets-&gt;&gt;Reports: Build Reports\n    Datamarts-&gt;&gt;Reports: Build Reports</code></pre> \ud83d\udd27 Advanced Components Flow"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/","title":"\ud83d\ude80 Introduction to Data Engineering","text":"<p>This report provides a comprehensive exploration of Data Engineering, a field that forms the bedrock of modern data-driven organizations. It guides individuals from foundational understanding to expert grasp of data warehousing and broader data engineering principles.</p>"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/#what-is-data-engineering","title":"\ud83e\udde0 What is Data Engineering?","text":"<p>Data Engineering is about taking raw, often messy data, refining it, and delivering it in the form of data models or cleaned, structured formats to stakeholders.</p> <p>Think of it like a skilled chef:</p> <p>Takes raw ingredients \u2192 Prepares &amp; Cooks \u2192 Serves a dish Just as a chef prepares ingredients, a Data Engineer ensures data is ready for analysis.</p> <pre><code>graph LR\n    \"Raw Data\" --&gt; \"Clean &amp; Structured\"\n    \"Clean &amp; Structured\" --&gt; \"Data Model\"\n    \"Data Model\" --&gt; \"Stakeholders\"</code></pre> <p>Purpose: Enable data-driven decision-making by making data usable and understandable.</p> <p>It involves: - Designing &amp; maintaining data systems - Structuring chaotic data - Enabling data scientists and analysts with clean datasets</p> <p>Without data engineering, advanced analytics and machine learning are not feasible.</p>"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/#why-data-engineers-are-essential","title":"\ud83d\udca1 Why Data Engineers are Essential","text":"<pre><code>graph TD\n    \"Big Data Explosion\" --&gt; \"High Volume + Velocity + Variety\"\n    \"High Volume + Velocity + Variety\" --&gt; \"Traditional Methods Fail\"\n    \"Traditional Methods Fail\" --&gt; \"Need for Data Engineers\"\n    \"Data Engineers\" --&gt; \"Reliable Infrastructure\"\n    \"Data Engineers\" --&gt; \"Accessible, Clean Data\"\n    \"Data Engineers\" --&gt; \"Enable Analytics, ML, BI\"</code></pre> <p>Modern businesses: - Generate colossal amounts of data from diverse sources - Need real-time insights to stay competitive - Rely on Data Engineers to ensure quality, availability, and performance</p>"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/#the-core-business-and-data-generation","title":"\ud83c\udfe2 The Core Business and Data Generation","text":"<p>Internet companies (Amazon, Netflix, Zomato) operate by: - Understanding customers - Increasing profits - Detecting fraud/anomalies</p> <p>Every interaction (click, purchase, like) generates data.</p> <pre><code>sequenceDiagram\n    participant User\n    participant App as \"Application (e.g., Amazon)\"\n    participant DB as \"OLTP Database\"\n    participant DE as \"Data Engineering\"\n    participant Analytics as \"Analytics Tools\"\n\n    User-&gt;&gt;App: Interaction (click/search/buy)\n    App-&gt;&gt;DB: Store data (CRUD ops)\n    DB-&gt;&gt;DE: Extract Raw Data\n    DE-&gt;&gt;Analytics: Provide Curated Data\n    Analytics--&gt;&gt;User: Reports / Recommendations</code></pre>"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/#diverse-data-sources","title":"\ud83d\udd17 Diverse Data Sources","text":"<p>Data Engineers work with:</p> <ul> <li>Transactional Systems (RDBMS): Real-time operations  </li> <li>IoT Devices: Continuous data streams  </li> <li>Web/Social Media: Semi-structured user activity  </li> <li>Machine Logs: Unstructured operational data  </li> </ul> <pre><code>graph TD\n    subgraph \"Source Systems\"\n        A[\"Transactional DBs\"]\n        B[\"IoT Devices\"]\n        C[\"Social Media\"]\n        D[\"Logs &amp; Events\"]\n    end\n\n    A --&gt;|\"Structured\"| P[\"Pipeline\"]\n    B --&gt;|\"Streaming\"| P\n    C --&gt;|\"Semi-structured\"| P\n    D --&gt;|\"Unstructured\"| P\n    P --&gt; S[\"Storage (Warehouse/Lake)\"]\n    S --&gt; R[\"Reporting &amp; Analytics\"]</code></pre>"},{"location":"courses/fundamentals/chapter_1_Introduction_to_Data_Engineering/#summary","title":"\ud83e\udded Summary","text":"<p>Data Engineering is the backbone of modern enterprises: - Structures chaos into insight - Powers analytics and AI - Builds scalable, trusted data pipelines</p> <p>\u2705 You can now upload this file to GitHub Pages as a clean, duplicate-free, and visually organized Markdown file.</p>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/","title":"\ud83d\ude80 Chapter 2: Data Engineering Workflow &amp; Core Concepts","text":"<p>This chapter outlines the core workflow of data engineering and highlights the key roles, concepts, and processes that underpin the modern data lifecycle.</p>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#three-main-pillars-of-data-engineering","title":"\ud83e\uddf1 Three Main Pillars of Data Engineering","text":"<p>The data engineering workflow can be conceptualized through three main pillars: 1. Data Production (Ingestion) 2. Data Transformation 3. Data Serving</p>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#1-data-production-ingestion","title":"1\ufe0f\u20e3 Data Production (Ingestion)","text":"<p>The entry point of raw data into an organization\u2019s ecosystem.</p> <p>Sources: - User interactions (e.g., apps, web, IoT) - APIs and external systems - Application logs, relational databases</p> <p>Characteristics: Messy, incomplete, unstructured or semi-structured.</p> <pre><code>graph TD\n    \"Mobile App Usage\" --&gt; \"Raw Data Sources\"\n    \"E-commerce Transactions\" --&gt; \"Raw Data Sources\"\n    \"IoT Sensors\" --&gt; \"Raw Data Sources\"\n    \"External APIs\" --&gt; \"Raw Data Sources\"\n    \"Databases (OLTP)\" --&gt; \"Raw Data Sources\"\n    \"Application Logs\" --&gt; \"Raw Data Sources\"\n\n    \"Raw Data Sources\" --&gt; \"Data Ingestion\"\n    \"Data Ingestion\" --&gt; \"Raw Data Lake\"\n\n    style \"Raw Data Lake\" fill:#f9f,stroke:#333,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#2-data-transformation","title":"2\ufe0f\u20e3 Data Transformation","text":"<p>Often consumes 70\u201380% of a data engineer\u2019s effort.</p> <p>Processes: - Cleaning, validating, deduplication - Standardizing and formatting - Applying business logic - Aggregation and filtering</p> <pre><code>graph TD\n    \"Raw Data Lake\" --&gt; \"ETL/ELT Engine\"\n    \"ETL/ELT Engine\" --&gt; \"Curated Data Warehouse\"\n\n    subgraph \"ETL/ELT Engine\"\n        \"Clean Data\"\n        \"Validate Data\"\n        \"Standardize Formats\"\n        \"Remove Duplicates\"\n        \"Aggregate &amp; Filter\"\n        \"Apply Business Logic\"\n    end\n\n    style \"Curated Data Warehouse\" fill:#bbf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#3-data-serving","title":"3\ufe0f\u20e3 Data Serving","text":"<p>Delivering refined, trusted data to downstream consumers.</p> <p>Consumers: - Data Analysts - Data Scientists - ML Engineers - Business Leaders</p> <pre><code>graph TD\n    \"Curated Data Warehouse\" --&gt; \"Data Serving Layer\"\n    \"Data Serving Layer\" --&gt; \"Data Consumers\"\n\n    subgraph \"Data Serving Layer\"\n        \"Custom Models\"\n        \"APIs\"\n        \"Reporting Tools\"\n    end\n\n    subgraph \"Data Consumers\"\n        \"Analysts\"\n        \"Scientists\"\n        \"ML Engineers\"\n        \"Business Leaders\"\n    end\n\n    style \"Data Consumers\" fill:#ccf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#key-roles-in-the-data-ecosystem","title":"\ud83d\udc65 Key Roles in the Data Ecosystem","text":"<pre><code>graph TD\n    \"Data Sources\" --&gt; \"Data Engineers\"\n    \"Data Engineers\" --&gt; \"Data Analysts\"\n    \"Data Engineers\" --&gt; \"Data Scientists\"\n    \"Data Scientists\" --&gt; \"ML Engineers\"\n    \"Software Engineers\" --&gt; \"Data Sources\"\n    \"DBAs\" --&gt; \"Data Sources\"\n\n    \"Data Engineers\" -- \"Enable\" --&gt; \"Data Analysts\"\n    \"Data Engineers\" -- \"Support\" --&gt; \"Data Scientists\"\n    \"Data Scientists\" -- \"Feed Models to\" --&gt; \"ML Engineers\"\n    \"ML Engineers\" -- \"Integrate into\" --&gt; \"Software Engineers\"</code></pre>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#upstream-vs-downstream-data","title":"\ud83c\udf0a Upstream vs. Downstream Data","text":"<p>Upstream = Data producers Downstream = Data consumers Bridge = Data Engineers</p> <pre><code>graph LR\n    \"User Actions\" --&gt; \"Front-end Apps\"\n    \"Front-end Apps\" --&gt; \"OLTP Systems\"\n    \"OLTP Systems\" --&gt; \"Data Engineers\"\n    \"Data Engineers\" --&gt; \"Data Warehouse / Lake\"\n    \"Data Warehouse / Lake\" --&gt; \"Data Analysts\"\n    \"Data Warehouse / Lake\" --&gt; \"Data Scientists\"\n    \"Data Warehouse / Lake\" --&gt; \"Business Leaders\"\n\n    style \"OLTP Systems\" fill:#add8e6,stroke:#333,stroke-width:2px\n    style \"Data Engineers\" fill:#ffb3ba,stroke:#333,stroke-width:2px\n    style \"Data Warehouse / Lake\" fill:#90ee90,stroke:#333,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_2_Data_Engineering_Workflow_and_Core_Concepts/#summary","title":"\ud83e\udde0 Summary","text":"<p>This chapter emphasized: - The three main stages of data handling - The collaborative roles in the data ecosystem - The flow of data from source to consumption</p> <p>Understanding these fundamentals ensures that every data engineer can design systems that are scalable, collaborative, and business-impactful.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/","title":"\ud83d\udce6 Chapter 3: Data Storage and Processing Systems","text":"<p>This section delves into the foundational systems employed for storing and processing data, which are indispensable for building robust data architectures, including data warehouses. A thorough understanding of these components is crucial for any aspiring data expert.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#a-database-management-systems-dbms","title":"A. Database Management Systems (DBMS)","text":"<p>A Database Management System (DBMS) is a software solution meticulously designed to efficiently manage, organize, and retrieve data in a structured manner. It stands as a critical component in modern computing, empowering organizations to effectively store, manipulate, and secure their data, thereby supporting data-driven decision-making and operational efficiency.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#1-purpose-and-operations","title":"1. Purpose and Operations","text":"<p>The primary purpose of a DBMS is to provide a structured environment for data storage, facilitating easy querying and efficient handling of vast quantities of records. A well-implemented DBMS: - Minimizes data redundancy - Prevents inconsistencies - Supports CRUD operations (Create, Read, Update, Delete) - Supports multiple database languages:   - DDL (CREATE, ALTER, DROP)   - DML (SELECT, INSERT, UPDATE, DELETE)   - DCL (GRANT, REVOKE)   - TCL (COMMIT, ROLLBACK, SAVEPOINT)</p> <p>Without a robust DBMS, data would exist in a state of chaos.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#2-types-relational-sql-vs-nosql","title":"2. Types: Relational (SQL) vs. NoSQL","text":"<pre><code>graph TD\n  Relational[\"Relational Databases (SQL)\"]\n  NoSQL[\"NoSQL Databases\"]\n\n  Relational --&gt; R1[\"Structured Tables\"]\n  Relational --&gt; R2[\"Predefined Schema\"]\n  Relational --&gt; R3[\"ACID Properties\"]\n  Relational --&gt; R_Ex[\"PostgreSQL, MySQL, SQL Server\"]\n\n  NoSQL --&gt; N1[\"Flexible Schemas\"]\n  NoSQL --&gt; N2[\"Various Formats (Key-Value, Document, Graph)\"]\n  NoSQL --&gt; N3[\"Scalability &amp; Availability\"]\n  NoSQL --&gt; N_Ex[\"MongoDB, Cassandra, Redis\"]\n\n  style Relational fill:#e0f2fe,stroke:#007bff,stroke-width:2px\n  style NoSQL fill:#fff3cd,stroke:#ffc107,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#3-the-importance-of-sql","title":"3. The Importance of SQL","text":"<p>SQL (Structured Query Language) is the standardized language used to communicate with and manage relational databases. It\u2019s considered the backbone of a data career.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#b-oltp-online-transaction-processing-systems","title":"B. OLTP (Online Transaction Processing) Systems","text":"<p>OLTP systems are optimized for fast processing of small, frequent transactions (e.g., ATM, e-commerce). Key characteristics: - Row-based storage - Normalization (1NF, 2NF, 3NF) - Real-time inserts/updates - Managed by DBAs</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#c-olap-online-analytical-processing-systems","title":"C. OLAP (Online Analytical Processing) Systems","text":"<p>OLAP systems (a.k.a. Data Warehouses) are optimized for reading and aggregating large historical datasets.</p> <pre><code>graph TD\n  OLAP[\"OLAP (Online Analytical Processing)\"]:::analytical\n  OLAP --&gt; A1[\"Process Analytical Data\"]\n  OLAP --&gt; A2[\"Efficient for Large Historical Data\"]\n  OLAP --&gt; A3[\"Column-Based Storage\"]\n  OLAP --&gt; A4[\"De-normalization/Dimensional Modeling\"]\n  OLAP --&gt; DE[\"Managed by Data Engineers\"]\n  A1 --&gt; EA1[\"Business Intelligence\"]\n  A1 --&gt; EA2[\"Trend Analysis\"]\n\n  classDef analytical fill:#bbdefb,stroke:#2196f3,stroke-width:2px;</code></pre>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#table-oltp-vs-olap-comparison","title":"\ud83d\udd01 Table: OLTP vs OLAP Comparison","text":"Criteria OLTP OLAP (Data Warehouse) Purpose Real-time transactions Historical analysis &amp; decision-making Data Model Normalized Star/Snowflake schema Performance Millisecond response Seconds to minutes Volume GB TB\u2013PB Examples ATM, order processing BI tools, financial analysis"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#d-etl-extract-transform-load-pipelines","title":"D. ETL (Extract, Transform, Load) Pipelines","text":"<p>ETL is the traditional data integration approach where transformation happens before loading into the warehouse.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#1-etl-process-stages","title":"1. ETL Process Stages","text":"<ul> <li>Extract: Pull from SQL/NoSQL, JSON, flat files</li> <li>Transform: Cleanse, standardize, deduplicate, derive columns</li> <li>Load: Insert into data warehouse</li> </ul>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#2-data-warehouse-layers","title":"2. Data Warehouse Layers","text":"<ul> <li>Staging Layer: Raw data holding zone</li> <li>Core/Modeling Layer: Business logic, schemas (Star, Snowflake)</li> </ul>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#e-elt-extract-load-transform","title":"E. ELT (Extract, Load, Transform)","text":"<p>ELT shifts transformation after loading into cloud-native warehouses like Snowflake or BigQuery.</p>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#elt-advantages","title":"\u2705 ELT Advantages:","text":"<ul> <li>Faster (parallel transformations)</li> <li>Handles structured &amp; unstructured data</li> <li>Raw data preserved</li> <li>Lower infrastructure cost</li> <li>Compatible with data lakes</li> </ul>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#elt-disadvantages","title":"\u26a0\ufe0f ELT Disadvantages:","text":"<ul> <li>Privacy concerns (raw data access)</li> <li>Still maturing (fewer tools/docs)</li> </ul>"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#table-etl-vs-elt-comparison","title":"\ud83d\udd01 Table: ETL vs ELT Comparison","text":"Criteria ETL ELT Transform Location Outside warehouse Inside warehouse Speed Slower Faster Flexibility Low High Cost Higher Lower Raw Data Access No Yes Best For Complex legacy systems Cloud-native scale systems"},{"location":"courses/fundamentals/chapter_3_Data_Storage_and_Processing_Systems/#f-file-formats-for-big-data","title":"F. File Formats for Big Data","text":"<ul> <li>Row-Based: CSV, Avro (fast full row scans)</li> <li>Column-Based: Parquet, ORC (faster read/query)</li> <li>Delta Format: Layer on top of Parquet with:</li> <li>ACID Transactions</li> <li>Schema evolution</li> <li>Time travel</li> <li>Transaction logs</li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/","title":"\ud83d\ude80 Chapter 4: Data Warehousing \u2013 A Deep Dive","text":"<p>This chapter provides a structured overview of data warehousing concepts, architectural patterns, modeling techniques, tools, and industry applications.</p>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#a-core-concepts-and-architecture","title":"\ud83e\udde0 A. Core Concepts and Architecture","text":""},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#1-data-warehouse-vs-data-marts","title":"1. Data Warehouse vs. Data Marts","text":"<ul> <li>Data Warehouse (DW): Centralized repository integrating data across the enterprise. Ideal for organization-wide analytics.</li> <li>Data Mart (DM): Department-focused subset of the DW. Optimized for specific team analytics.</li> </ul> <p>A Data Mart is a subset of a Data Warehouse.</p>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#2-kimball-vs-inmon-methodologies","title":"2. Kimball vs. Inmon Methodologies","text":""},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#kimball-bottom-up","title":"Kimball (Bottom-Up)","text":"<ul> <li>Starts with business requirements and builds data marts.</li> <li>Uses denormalized star schemas.</li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#inmon-top-down","title":"Inmon (Top-Down)","text":"<ul> <li>Begins with a normalized enterprise DW.</li> <li>Builds data marts downstream.</li> </ul> <pre><code>graph TD\n  subgraph \"Kimball (Bottom-Up)\"\n    KR[\"Business Requirements\"] --&gt; DM1[\"Build Data Mart 1 (Star Schema)\"]\n    KR --&gt; DM2[\"Build Data Mart 2 (Star Schema)\"]\n    DM1 --&gt; DWK[\"Integrated Data Warehouse\"]\n    DM2 --&gt; DWK\n  end\n\n  subgraph \"Inmon (Top-Down)\"\n    IS[\"Source Systems\"] --&gt; DWI[\"Normalized Enterprise DW\"]\n    DWI --&gt; DM1I[\"Data Mart 1\"]\n    DWI --&gt; DM2I[\"Data Mart 2\"]\n  end\n\n  style DWK fill:#add8e6,stroke:#007bff,stroke-width:2px\n  style DWI fill:#add8e6,stroke:#007bff,stroke-width:2px</code></pre>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#3-dimensional-modeling-star-vs-snowflake-schema","title":"3. Dimensional Modeling: Star vs. Snowflake Schema","text":"<ul> <li>Star Schema: Denormalized; simpler, faster queries.</li> <li>Snowflake Schema: Normalized; less redundancy, more complex joins.</li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#4-slowly-changing-dimensions-scds","title":"4. Slowly Changing Dimensions (SCDs)","text":"Type Description SCD 0 No change allowed SCD 1 Overwrite value (no history kept) SCD 2 Add new row with history tracking SCD 3 Keep partial history in separate columns SCD 6 Hybrid of SCD 1, 2, and 3"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#b-data-warehouse-tools-and-technologies","title":"\ud83d\udee0\ufe0f B. Data Warehouse Tools and Technologies","text":"<ul> <li>Astera DW Builder \u2013 Meta-driven automation</li> <li>Snowflake \u2013 Fully managed cloud DW with Time Travel</li> <li>SAP DW Cloud \u2013 Semantic layer + drag-and-drop modeling</li> <li>Oracle ADW \u2013 Cloud-native analytics platform</li> <li>Panoply \u2013 Managed ELT + SQL/Python transformations</li> <li>Teradata Vantage \u2013 Large scale SQL analytics</li> <li>Azure Synapse \u2013 Unified analytics platform (via Azure)</li> <li>Hevo Data \u2013 ETL to integrate various DWs (not DW itself)</li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#c-real-world-use-cases","title":"\ud83c\udf10 C. Real-World Use Cases","text":"<ul> <li>Retail: Target \u2013 Personalized marketing via Guest Data Platform  </li> <li>Healthcare: Kaiser Permanente \u2013 Patient risk analysis and care improvement  </li> <li>Finance: JPMorgan \u2013 Real-time fraud detection  </li> <li>Manufacturing: Siemens \u2013 Predictive maintenance &amp; inventory optimization  </li> <li>Telecom: Verizon \u2013 Network optimization &amp; churn prevention  </li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#d-security-and-performance-optimization","title":"\ud83d\udd10 D. Security and Performance Optimization","text":""},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#1-security-layers","title":"1. Security Layers","text":"<ul> <li>Access control (MFA, IAM, RBAC)</li> <li>Encryption (data at rest + transit)</li> <li>Compliance (GDPR, HIPAA, CCPA)</li> <li>SIEM, audits, patching, training</li> </ul>"},{"location":"courses/fundamentals/chapter_4_Data_Warehousing/#2-performance-best-practices","title":"2. Performance Best Practices","text":"<ul> <li>Optimized schema design (Star/Snowflake)</li> <li>Streamlined ETL/ELT pipelines</li> <li>Indexing + partitioning + compression</li> <li>Materialized views + caching</li> <li>Horizontal scaling + monitoring tools</li> </ul> <p>Data Warehousing is a strategic asset \u2014 powering analytics, enabling insights, and driving value across industries.</p>"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/","title":"\ud83c\udf1f Chapter 5: Modern Data Concepts","text":"<p>The rapid evolution of data technologies has given rise to several modern concepts and architectures designed to address new challenges and unlock further opportunities in data management and analytics. These paradigms extend beyond traditional data warehousing to handle diverse data types, real-time demands, and distributed organizational structures.</p>"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/#a-data-lakes-and-data-lakehouses","title":"A. Data Lakes and Data Lakehouses","text":"<p>The landscape of data storage has evolved significantly, leading to distinct architectures tailored for different needs. Understanding the nuances between data lakes, data warehouses, and the hybrid data lakehouse is crucial.</p> <ul> <li>Data Lake: A storage repository designed to hold vast volumes of raw data in its native format, regardless of structure (structured, semi-structured, unstructured, e.g., logs, sensor data, social media feeds). Data lakes employ a schema-on-read approach, meaning structure is applied only when data is accessed, offering immense flexibility for evolving data types.</li> <li>Benefits: High scalability (petabytes), cost-effectiveness, inherent flexibility.</li> <li> <p>Challenges: Can devolve into \"data swamps\" without governance, typically requires data science expertise, challenging for real-time queries.</p> </li> <li> <p>Data Warehouse: Optimized for analyzing structured, historical data for business intelligence and reporting. It uses a schema-on-write approach, enforcing a predefined structure at load time.</p> </li> <li> <p>Challenges: Can be costly to implement/maintain, less flexible for unstructured data, potential scaling challenges for extremely large datasets.</p> </li> <li> <p>Data Lakehouse: A hybrid architecture combining the best of lakes and warehouses. It stores raw data like a data lake but offers warehouse features (ACID transactions, schema enforcement, time travel).</p> </li> <li>Benefits: Unified platform, improved data quality, reduced duplication, support for BI, ML, and real-time analytics.</li> <li>Limitations: Complex implementation, requires technical expertise.</li> </ul> <pre><code>graph TD\n    DL[Data Lake] --&gt; DL_RAW[Raw, All Data Types]\n    DL --&gt; DL_SCHEMA[Schema-on-Read]\n    DL --&gt; DL_USE[ML &amp; Data Science]\n\n    DW[Data Warehouse] --&gt; DW_STRUCT[Structured, Historical Data]\n    DW --&gt; DW_SCHEMA[Schema-on-Write]\n    DW --&gt; DW_USE[BI &amp; Reporting]\n\n    DLH[Data Lakehouse] --&gt; DLH_HYBRID[Hybrid Architecture]\n    DLH --&gt; DLH_TXN[ACID Transactions + Time Travel]\n    DLH --&gt; DLH_ALL[All Data Types + Analytics]</code></pre> <p>Figure 5.1: Data Lake vs. Data Warehouse vs. Data Lakehouse</p> Criteria Data Lake Data Warehouse Data Lakehouse Data Type All (structured, semi/unstructured) Structured, historical All + curated Schema Schema-on-read Schema-on-write Hybrid (schema evolution) Cost Low Higher Balanced Use Cases ML, exploratory analysis BI, dashboards Unified workloads"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/#b-data-streaming","title":"B. Data Streaming","text":"<p>Data streaming processes data in motion for real-time insights. Architectures use durable, replayable storage (Kafka, Kinesis) and stream processors (Flink, Spark Streaming) for on-the-fly transformations.</p> <ul> <li>Storage Layer: Ensures low-latency, ordered reads/writes.</li> <li>Processing Layer: Applies windowed computations, aggregations.</li> </ul> <p>Use Cases: - Fraud detection - Live analytics dashboards - IoT monitoring  </p> <pre><code>graph TD\n    Sources[Event Sources] --&gt; Broker[Streaming Platform]\n    Broker --&gt; Processor[Stream Processing]\n    Processor --&gt; Sink[Databases &amp; Dashboards]</code></pre>"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/#c-data-mesh","title":"C. Data Mesh","text":"<p>Data Mesh decentralizes data ownership, treating data as a product managed by domain teams with federated governance.</p> <ol> <li>Domain Ownership: Teams own their data lifecycle.  </li> <li>Data as a Product: Discoverable, documented, SLA-backed.  </li> <li>Self-Serve Platform: Shared infrastructure for pipelines and catalogs.  </li> <li>Federated Governance: Global standards, local autonomy.</li> </ol> <pre><code>graph TD\n    A[Data Mesh Principles]\n    A --&gt; B[Domain-Oriented Data Ownership]\n    A --&gt; C[Data as a Product]\n    A --&gt; D[Self-Serve Platform]\n    A --&gt; E[Federated Governance]</code></pre>"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/#d-data-virtualization","title":"D. Data Virtualization","text":"<p>Data Virtualization provides a unified view across disparate sources without data movement. It abstracts underlying systems, allowing real-time access via SQL or API.</p> <ul> <li>Benefits: Instant access, lower storage costs, agility.  </li> <li>Use Cases: Ad-hoc analytics, live dashboards.</li> </ul> <pre><code>graph TD\n    Src1[DB1] --&gt;|Virtual| View[Unified View]\n    Src2[API] --&gt;|Virtual| View\n    View --&gt;|SQL| BI[Analytics Tools]</code></pre>"},{"location":"courses/fundamentals/chapter_5_modern_data_concepts/#e-medallion-architecture","title":"E. Medallion Architecture","text":"<p>Medallion Architecture defines layers in lakes/lakehouses for progressive data quality:</p> <ol> <li>Bronze Layer: Raw, as-ingested data.  </li> <li>Silver Layer: Cleaned, enriched data.  </li> <li>Gold Layer: Curated, business-ready data.</li> </ol> <pre><code>graph TD\n    Source[Data Sources] --&gt; Bronze[\"Bronze Layer (Raw/Landing Zone)\"]\n    Bronze -- \"Ingested  as is\" --&gt; RawData[Raw Data]\n\n    Bronze --&gt; Silver[\"Silver Layer (Transformed/Curated)\"]\n    Silver -- \"Transformation, Cleaning, Aggregation\" --&gt; CleanedData[Cleaned &amp; Structured Data]\n    Silver --&gt; DefineSchema[Define Structure, Enforce Schema, Evolve Schema]\n\n    Silver --&gt; Gold[\"Gold Layer (Business-ready/Aggregated)\"]\n    Gold -- Facts, Dimensions, Aggregations --&gt; ReadyToServe[Ready for Downstream Users &amp; Apps]</code></pre> <p>Figure 5.3: Bronze \u2192 Silver \u2192 Gold \u2013 Progressive Data Maturity</p> <p>End of Chapter 5: Modern Data Concepts</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/","title":"Big Data & Distributed Computing","text":"<p>Here's the refined Markdown for Chapter 5, focusing on clarity, conciseness, and consistent formatting, while retaining all your valuable original content and integrating the Mermaid diagrams and tables.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#v-modern-data-concepts","title":"V. Modern Data Concepts","text":"<p>The rapid evolution of data technologies has given rise to several modern concepts and architectures designed to address new challenges and unlock further opportunities in data management and analytics. These paradigms extend beyond traditional data warehousing to handle diverse data types, real-time demands, and distributed organizational structures.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#a-data-lakes-and-data-lakehouses","title":"A. Data Lakes and Data Lakehouses","text":"<p>The landscape of data storage has evolved significantly, leading to distinct architectures tailored for different needs. Understanding the nuances between data lakes, data warehouses, and the hybrid data lakehouse is crucial.</p> <ul> <li> <p>Data Lake: A storage repository designed to hold vast volumes of raw data in its native format, regardless of structure (structured, semi-structured, unstructured, e.g., logs, sensor data, social media feeds). Data lakes employ a \"schema-on-read\" approach, meaning structure is applied only when data is accessed, offering immense flexibility for evolving data types.</p> <ul> <li>Benefits: High scalability (petabytes), cost-effectiveness (low-cost storage), inherent flexibility.</li> <li>Challenges: Can devolve into \"data swamps\" without proper governance, typically requires data science expertise for analysis, challenging for real-time queries.</li> </ul> </li> <li> <p>Data Warehouse (Reiterated for comparative context): Optimized for analyzing structured, historical data for business intelligence and reporting. It adheres to a \"schema-on-write\" approach, requiring data to conform to a predefined, rigid structure before loading. This upfront structuring ensures data consistency and facilitates faster querying for structured data, leading to high data quality.</p> <ul> <li>Challenges: Can be costly to implement/maintain, less flexible for unstructured data, potential horizontal scaling challenges for extremely large datasets.</li> </ul> </li> <li> <p>Data Lakehouse: A modern, hybrid architecture combining the best features of both data lakes and data warehouses. It leverages the low-cost, flexible storage of a data lake for all raw data types, while simultaneously providing the data structures, management features, and powerful analytical capabilities traditionally found in data warehouses. Data lakehouses support \"schema evolution,\" allowing flexible structures with structured organization.</p> <ul> <li>Benefits: Simplified architecture (single repository), improved data quality (schema enforcement), lower costs (no separate platforms), increased reliability (reduced transfers), enhanced data governance, reduced data duplication, support for diverse workloads (BI, ML, SQL, data science) from a single source, high scalability (decoupled compute/storage).</li> <li>Limitation: Increased complexity in implementation and management compared to standalone data lakes or warehouses, often requiring high technical expertise.</li> </ul> </li> </ul> <pre><code>graph TD\n    DL[Data Lake] --&gt; DL_RAW[Stores Raw, All Data Types];\n    DL --&gt; DL_SCHEMA[Schema-on-Read (Flexibility)];\n    DL --&gt; DL_ML[Good for ML &amp; Data Science];\n\n    DW[Data Warehouse] --&gt; DW_STRUCT[Stores Structured, Historical Data];\n    DW --&gt; DW_SCHEMA[Schema-on-Write (Rigid)];\n    DW --&gt; DW_BI[Optimized for BI &amp; Reporting];\n\n    DLH[Data Lakehouse] --&gt; DLH_HYBRID[Hybrid of Data Lake &amp; Data Warehouse];\n    DLH --&gt; DLH_ALL[Stores all data types (Raw &amp; Processed)];\n    DLH --&gt; DLH_STRUCT[Provides Data Warehouse features on Lake storage];\n    DLH --&gt; DLH_EVO[Supports Schema Evolution];\n\n    style DL fill:#add8e6,stroke:#007bff,stroke-width:2px;\n    style DW fill:#f8d7da,stroke:#dc3545,stroke-width:2px;\n    style DLH fill:#d4edda,stroke:#28a745,stroke-width:2px;</code></pre> <p>Figure 5.1: Comparison of Data Lake, Data Warehouse, and Data Lakehouse</p> <p>The emergence of the data lakehouse signifies a clear trend towards converging previously siloed data architectures. This convergence is driven by the increasing need to support diverse workloads\u2014from traditional business intelligence to advanced machine learning\u2014on a unified platform, while maintaining cost-efficiency and data quality. An expert understands that the data lakehouse is an architectural response to the historical challenges of managing disparate data types and workloads, aiming to provide a more streamlined, versatile, and future-proof data platform.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#table-comparison-of-data-lake-vs-data-warehouse-vs-data-lakehouse","title":"Table: Comparison of Data Lake vs. Data Warehouse vs. Data Lakehouse","text":"Criteria Data Lake Data Warehouse Data Lakehouse Data Type Raw, all types (structured, semi-structured, unstructured). Structured, historical data. All types (structured, semi-structured, unstructured). Schema Schema-on-read (flexibility). Schema-on-write (predefined, rigid). Hybrid; supports schema evolution. Querying/Performance Slower due to on-read schema; challenging for real-time. Faster for structured data; more prep time. Fast queries + efficient processing; quick loading. Cost Cost-effective (low-cost storage). Can be costly to implement/maintain. Lower costs (eliminates need for both). Data Quality Can degrade if not governed (\"data swamp\"). High (enforced by schema-on-write). Better (enforce schemas, data integrity). Use Cases Streaming data, ML, data science, raw data storage. Business intelligence, reporting, historical analysis. BI, ML, SQL, data science on unified platform. Complexity Less complex than data warehouse/lakehouse. Moderate. High (requires significant technical expertise). Scalability High (handles petabytes). Can face challenges with horizontal scaling. High (decoupled compute/storage)."},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#b-data-streaming","title":"B. Data Streaming","text":"<p>Data streaming refers to data that continuously flows from a source system to a target, generated simultaneously and at high speed by numerous sources (e.g., applications, IoT sensors, log files, servers). The architecture supporting data streaming is designed for real-time consumption, storage, enrichment, and analysis of this continuously flowing data. This capability for real-time analysis provides deeper insights, enabling organizations to react quickly to changing conditions, market events, and customer issues.</p> <p>The architecture for streaming data typically comprises two primary layers: a Storage layer and a Processing layer.</p> <ul> <li>The storage layer must support low-cost, quick, and replayable reads and writes of large data streams, ensuring strong consistency and record ordering.</li> <li>The processing layer consumes data from the storage layer, performs computations, and manages data no longer needed.</li> </ul> <p>The emphasis on \"continuously flowing,\" \"high speed,\" and \"real-time consumption\" signifies a strong industry shift from purely batch-oriented processing to real-time capabilities, driven by business needs for immediate insights.</p> <p>Key tools in the data streaming ecosystem include stream processors like Apache Kafka, Amazon Kinesis, Google Pub/Sub, and Azure Event Hubs. For querying data streams directly, KSQL (ksqlDB) is utilized. For storing streamed data, options include Amazon S3, Amazon Redshift, and Google Storage.</p> <p>Data streaming finds application in a wide array of real-world scenarios: streaming media, stock trading, real-time analytics for BI, fraud detection, IT monitoring, instant messaging, IoT device monitoring, and personalized customer experiences. This highlights that modern data engineering increasingly involves designing systems that handle data in motion, not just data at rest, necessitating a different set of tools and architectural patterns than traditional batch processing.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#c-data-mesh","title":"C. Data Mesh","text":"<p>Data Mesh is a decentralized data architecture paradigm that fundamentally shifts how data is managed and consumed within large organizations. It proposes treating data as a product, with ownership and responsibility distributed among domain-oriented teams, aiming to overcome scalability and autonomy challenges in traditional, centralized data architectures.</p> <p>The Data Mesh is built upon several key principles:</p> <ul> <li>Domain-Oriented Data Ownership: Accountability for data is distributed to cross-functional teams aligned with specific business domains (e.g., sales, marketing, product). Each domain team is responsible for its data's entire lifecycle, including ingestion, cleaning, and aggregation, effectively managing its own data pipelines.</li> <li>Data as a Product: Data assets produced by domain teams are treated as high-quality products. This means they are designed to be discoverable, addressable, trustworthy, self-describing, and secure, complete with defined Service Level Agreements (SLAs) for their consumers.</li> <li>Self-Serve Data Platform: To prevent duplication of effort, a central platform provides domain-agnostic data infrastructure capabilities. This platform offers tools and services for storage, cataloging, access controls, and pipeline execution, enabling domain teams to easily serve their data autonomously.</li> <li>Federated Computational Governance: A universal set of data standards and agreed-upon SLAs are established across all domains, facilitating seamless cross-domain collaboration and interoperability. This ensures consistency in areas like data formatting, discoverability, and metadata in a decentralized environment.</li> </ul> <pre><code>graph TD\n    A[Data Mesh Principles]\n\n    A --&gt; B[Domain-Oriented Data Ownership]\n    B --&gt; B1{Domain Team 1}\n    B --&gt; B2{Domain Team 2}\n    B1 -- Owns &amp; Manages --&gt; B1a[Data Product 1]\n    B2 -- Owns &amp; Manages --&gt; B2a[Data Product 2]\n\n    A --&gt; C[Data as a Product]\n    C --&gt; C1[Discoverable, Addressable, Trustworthy, Self-describing]\n    C1 --&gt; C2[With SLAs]\n\n    A --&gt; D[Self-Serve Data Platform]\n    D --&gt; D1[Provides Infrastructure Capabilities]\n    D1 --&gt; D2[Tools &amp; Services for Storage, Cataloging, Access, Pipeline Execution]\n\n    A --&gt; E[Federated Computational Governance]\n    E --&gt; E1[Universal Data Standards]\n    E1 --&gt; E2[Cross-domain Interoperability &amp; Consistency]\n\n    style A fill:#d4edda,stroke:#28a745,stroke-width:2px;\n    style B1 fill:#bbdefb,stroke:#2196f3,stroke-width:2px;\n    style B2 fill:#bbdefb,stroke:#2196f3,stroke-width:2px;</code></pre> <p>Figure 5.2: Data Mesh Principles</p> <p>The principles of Data Mesh directly challenge the traditional centralized data warehouse model. The emphasis on \"domain-oriented ownership\" and \"data as a product\" reflects a significant shift in organizational thinking, viewing data not merely as a technical byproduct but as a core business asset with its own lifecycle and dedicated consumers. An expert understands that Data Mesh is not just a technical architecture but a profound organizational and cultural transformation. It aims to unlock data's value by empowering domain teams, but also introduces complexities in maintaining global consistency and coordination across a distributed data landscape.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#d-data-virtualization","title":"D. Data Virtualization","text":"<p>Data virtualization is a modern data engineering concept that enables organizations to access and integrate data from disparate sources into a single, unified view without the need for physical data movement or replication. It creates a single semantic virtual layer that sits atop various physical data sources, allowing users to interact with the data as if it resided in one centralized location, regardless of its actual physical format or location.</p> <p>The core benefits of data virtualization are numerous:</p> <ul> <li>Significantly simplifies data access by providing a unified interface to diverse data sources.</li> <li>Substantial savings in storage space and a reduction in data management complexity by eliminating manual data movement and replication.</li> <li>Fosters increased data agility, allowing quicker access and analysis without the overhead of physical integration.</li> </ul> <p>At a high level, data virtualization operates by connecting to various relational and non-relational data sources. Users virtualize specific tables or datasets, and these virtual objects can then be joined to create a consolidated view. Once established, these virtual objects can be queried using standard SQL and consumed by other applications, dashboards, or data catalogs. The primary promise of data virtualization is to access data without physically moving it. This directly addresses the challenges of traditional ETL processes (time, cost, complexity) and the growing need for immediate access to diverse, distributed data. It enables a more agile approach to data integration. An expert recognizes data virtualization as a powerful tool for specific use cases, particularly when real-time access to disparate, distributed data is critical and physical data movement is undesirable or impractical. It serves as a complement to, rather than a replacement for, traditional data warehousing.</p>"},{"location":"courses/fundamentals/chapter_6_big_data_distributed_computing/#e-medallion-architecture-cloud-data-engineering","title":"E. Medallion Architecture (Cloud Data Engineering)","text":"<p>Medallion Architecture (also known as the Bronze, Silver, Gold architecture) is a robust data architecture pattern for cloud environments, akin to data warehouse layers. It defines distinct data quality and transformation stages for data as it flows through a data lake or lakehouse.</p> <ul> <li> <p>Bronze Layer (Raw Layer/Landing Zone):</p> <ul> <li>Data is ingested \"as is from our source without any changes, without any transformation.\"</li> <li>Acts as a raw data repository or landing zone.</li> <li>\"No schema needed\" initially; schema-on-read is typical here.</li> <li>Ensures all raw data is captured and available for auditing or reprocessing.</li> </ul> </li> <li> <p>Silver Layer:</p> <ul> <li>This is where significant transformation, cleaning, and aggregation occur.</li> <li>Data is \"transformed\" from its raw state.</li> <li>Structure is \"defined structure,\" \"enforce schema,\" and \"evolve schema.\" This layer often applies quality checks and initial business rules.</li> <li>Data here is more refined, consistent, and ready for further analysis.</li> </ul> </li> <li> <p>Gold Layer:</p> <ul> <li>Contains \"data stored in the form of facts and dimensions or maybe aggregated tables,\" ready to be \"served\" to \"Downstream users and applications.\"</li> <li>Highly refined, denormalized, and optimized for specific business intelligence and analytical use cases.</li> <li>Provides a clean, trustworthy source for reports, dashboards, and machine learning models.</li> </ul> </li> </ul> <pre><code>graph TD\n    Source[Data Sources] --&gt; Bronze[Bronze Layer (Raw/Landing Zone)]\n    Bronze -- Ingested \"as is\" --&gt; RawData[Raw Data]\n\n    Bronze --&gt; Silver[Silver Layer (Transformed/Curated)]\n    Silver -- Transformation, Cleaning, Aggregation --&gt; CleanedData[Cleaned &amp; Structured Data]\n    Silver --&gt; DefineSchema[Define Structure, Enforce Schema, Evolve Schema]\n\n    Silver --&gt; Gold[Gold Layer (Business-ready/Aggregated)]\n    Gold -- Facts, Dimensions, Aggregations --&gt; ReadyToServe[Ready for Downstream Users &amp; Apps]</code></pre> <p>Figure 5.3: Medallion Architecture Layers</p> <p>This layered approach ensures data quality progressively improves at each stage, making it easier to manage data governance, lineage, and access controls. It provides a clear path from raw, immutable data to highly refined, business-ready information, supporting diverse consumption patterns.</p>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/","title":"\u2601\ufe0f Chapter 7: Cloud Data Engineering","text":"<p>Cloud computing has profoundly transformed data engineering, offering unmatched scalability, flexibility, and efficiency. This chapter explores the benefits of cloud adoption, compares major providers, and dives into cloud-native paradigms.</p>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#a-benefits-of-cloud-data-engineering","title":"A. Benefits of Cloud Data Engineering","text":"<p>Adopting cloud platforms revolutionizes how data systems are built and managed:</p> <ul> <li>Scalability &amp; Flexibility   Automatically scale resources up/down to match workloads.  </li> <li>Cost Efficiency   Pay-per-use model avoids over-provisioning and idle resources.  </li> <li>Reduced Operational Overhead   Providers manage hardware, OS updates, patching\u2014freeing teams to focus on data logic.  </li> <li>Deployment Speed &amp; Agility   Rapid provisioning and CI/CD pipelines accelerate time-to-market.  </li> <li>Global Reach   Data centers worldwide ensure low latency, high availability, and disaster recovery.</li> </ul>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#b-major-cloud-platforms-aws-azure-gcp","title":"B. Major Cloud Platforms: AWS, Azure, GCP","text":"<p>Three leading providers each offer unique strengths for data engineering.</p> <pre><code>graph TD\n    AWS[Amazon Web Services (AWS)] --&gt; Redshift[AWS Redshift]\n    AWS --&gt; Glue[AWS Glue]\n    AWS --&gt; S3[AWS S3]\n\n    Azure[Microsoft Azure] --&gt; Synapse[Azure Synapse Analytics]\n    Azure --&gt; ADF[Azure Data Factory]\n    Azure --&gt; Blob[Azure Blob Storage]\n\n    GCP[Google Cloud Platform (GCP)] --&gt; BigQuery[Google BigQuery]\n    GCP --&gt; Dataflow[Google Dataflow]\n    GCP --&gt; GCS[Google Cloud Storage]\n\n    style AWS fill:#fff3cd,stroke:#ffc107,stroke-width:2px\n    style Azure fill:#add8e6,stroke:#007bff,stroke-width:2px\n    style GCP fill:#d4edda,stroke:#28a745,stroke-width:2px</code></pre> Provider Key Services Strengths Considerations AWS S3, Glue, Redshift, Kinesis, QuickSight Mature ecosystem, global reach Complex pricing, steep learning curve Azure Blob Storage, Data Factory, Synapse, Functions, Power BI Enterprise integration, hybrid support Service complexity, cost management GCP Cloud Storage, Dataflow, BigQuery, Pub/Sub, Looker Advanced analytics &amp; ML, simplicity Smaller market share, integration maturity"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#c-serverless-data-processing","title":"C. Serverless Data Processing","text":"<p>Serverless abstracts infrastructure management, providing:</p> <ul> <li>Cost Efficiency: Pay only for compute used.  </li> <li>Automatic Scaling: Instant resource provisioning based on demand.  </li> <li>Reduced Overhead: Provider handles maintenance and patching.  </li> <li>Faster Deployment: Simplified CI/CD, focus on code.</li> </ul> <p>Ideal for event-driven ETL (e.g., AWS Lambda, Azure Functions, Google Cloud Functions).</p>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#d-managed-data-services","title":"D. Managed Data Services","text":"<p>Managed services delegate operations to specialists, offering:</p> <ul> <li>Resource Optimization: Expert tuning for performance and cost.  </li> <li>Seamless Integration: Smooth hybrid/multi-cloud deployments.  </li> <li>Predictable Spending: Subscription models with tiered support.  </li> <li>Security &amp; Compliance: MCSP manages certifications and audits.</li> </ul> <p>Considerations: Service cost adds overhead; multi-tenancy risks require strong vendor trust.</p>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#e-essential-tools-services-azure-focus","title":"E. Essential Tools &amp; Services (Azure Focus)","text":"<p>Core Skills: Python, SQL, Linux, Cloud Fundamentals.</p> <p>Azure Services: - Event Hub: Real-time event ingestion. - SQL Database: OLTP workloads. - ADLS Gen2: Scalable data lake. - Data Factory: ETL orchestration (low-code). - Databricks: Managed Spark analytics. - Synapse Analytics: Unified data warehouse/lakehouse. - Power BI: BI and visualization. - Purview: Data governance and lineage. - DevOps: CI/CD pipelines. - Key Vault &amp; Entra ID: Security and identity. - Monitor &amp; Cost Management: Observability and budgeting.</p> <p>Supporting Tools: - Kafka: Real-time streaming buffer. - Spark: Distributed ETL engine. - Airflow: Pipeline orchestration. - DBT: SQL-based transformation framework. - Reverse ETL: Operational analytics. - Data Masking: Privacy protection techniques.</p>"},{"location":"courses/fundamentals/chapter_7_cloud_data_engineering/#f-end-to-end-architecture-azure-example","title":"F. End-to-End Architecture (Azure Example)","text":"<pre><code>flowchart TD\n    subgraph Ingestion\n        Clicks[Customer Streams] --&gt; EventHub[Azure Event Hub]\n        Files[Batch Files] --&gt; ADF[Azure Data Factory]\n    end\n    subgraph Storage\n        EventHub --&gt; ADLS[ADLS Gen2]\n        ADF --&gt; ADLS\n    end\n    subgraph Transformation\n        ADLS --&gt; Databricks[Azure Databricks (Spark)]\n    end\n    subgraph Warehouse\n        Databricks --&gt; Synapse[Azure Synapse Analytics]\n    end\n    subgraph Consumption\n        Synapse --&gt; BI[Power BI Dashboards]\n        Synapse --&gt; MLOps[Data Science &amp; MLOps]\n    end\n    subgraph Governance_and_Support\n        Purview[Azure Purview] &amp; DevOps[Azure DevOps] &amp; Vault[Key Vault] &amp; ID[Entra ID] &amp; Monitor[Azure Monitor] &amp; Budget[Cost Management]\n    end</code></pre> <p>A cohesive data platform leverages serverless, managed, and orchestration services to deliver reliable analytics, governance, and operational excellence.</p> <p>End of Chapter 7 \u2013 Next: Orchestration &amp; Automation Patterns.</p>"},{"location":"courses/fundamentals/overview/","title":"Fundamentals of Data Engineering: Your Guide to the Data-Driven World","text":"<p>Welcome to the heart of how modern businesses thrive on data! In an era where data is often called the \"new oil,\" Data Engineering is the critical field that refines this raw resource into something truly valuable.</p> <p>This guide will walk you through the core concepts, workflow, and essential tools of Data Engineering, using real-world examples to make complex ideas clear.</p>"},{"location":"courses/fundamentals/overview/#1-what-is-data-engineering","title":"1. What is Data Engineering?","text":"<p>Data Engineering is essentially about taking raw, often messy data, refining it, and delivering it in the form of data models or cleaned, structured formats to stakeholders.</p> <p>Think of it like a skilled chef who takes raw ingredients, meticulously prepares them, cooks them to perfection, and serves a delicious, well-prepared dish. Just as a chef ensures every ingredient is ready for consumption, a Data Engineer ensures data is ready for analysis and decision-making.</p> <p>The core purpose of Data Engineering is to enable data-driven decision-making for businesses by making data usable and understandable.</p>      graph TD         DE_Concept[\"Data Engineering: The 'Data Chef'\"]:::main-topic         DE_Concept --&gt; Raw[\"Raw, Messy Data (Ingredients)\"]         DE_Concept --&gt; Refine[\"Refine &amp; Structure (Cooking Process)\"]         DE_Concept --&gt; Deliver[\"Deliver Data Models (Well-prepared Dish)\"]         Refine --&gt; MakeUsable(\"Make Data Usable &amp; Understandable\")         Deliver --&gt; EnableDecisions(\"Enable Data-Driven Decisions\")"},{"location":"courses/fundamentals/overview/#2-why-is-data-engineering-needed","title":"2. Why is Data Engineering Needed?","text":"<p>In today's competitive landscape, businesses can no longer afford to operate on assumptions. They need to understand their customers, increase profit, detect bottlenecks, and improve processes. While many decisions are based on gut feelings, data provides a factual understanding, leading to more accurate and impactful decisions.</p> <p>The demand for Data Engineers is skyrocketing due to the exponential increase in data generation. Companies are flooded with information, and without Data Engineers, this vast ocean of data remains untapped potential. Data Engineers are crucial for handling massive amounts of data and serving it to various consumers within an organization.</p>"},{"location":"courses/fundamentals/overview/#3-the-data-engineering-life-cycle-workflow","title":"3. The Data Engineering Life Cycle / Workflow","text":"<p>The data engineering process can be broken down into a clear, iterative workflow. At its core, this workflow is built upon three main \"Pillars of Data Engineering\": Data Production, Data Transformation, and Data Serving.</p>      graph LR         subgraph DE_Workflow[\"Data Engineering Workflow Lifecycle\"]             A[Data Production] --&gt; B[Data Ingestion]             B --&gt; C[Data Storage]             C --&gt; D[Data Transformation]             D --&gt; E[Data Serving]         end          subgraph Pillars[\"Pillars of Data Engineering\"]             P1(Data Production) --&gt; P2(Data Transformation)             P2 --&gt; P3(Data Serving)         end  <p>Let's dive into each stage:</p>"},{"location":"courses/fundamentals/overview/#31-data-production-data-generation","title":"3.1. Data Production (Data Generation)","text":"<p>This is the initial stage where data is created from various sources. Think of every interaction you have with a digital product or service.</p> <p>Example: Amazon When you browse products, click a \"Buy Now\" button, add items to your cart, or leave a review on Amazon, you're constantly generating data. Beyond your clicks, Amazon's internal systems (inventory, shipping, server logs) and third-party partners (like FedEx tracking) also produce enormous amounts of data. This is all the \"raw ingredients\" arriving in the data kitchen.</p>"},{"location":"courses/fundamentals/overview/#32-data-ingestion","title":"3.2. Data Ingestion","text":"<p>Once data is generated, it needs to be collected. Data ingestion involves aggregating data from different sources and bringing it into a centralized system. This often means setting up automated connections to continuously fetch new data.</p>"},{"location":"courses/fundamentals/overview/#33-data-storage","title":"3.3. Data Storage","text":"<p>After ingestion, the data needs a place to live. Data storage refers to storing the ingested data at a specific location, which can vary depending on the data type and its intended use (more on this in Section 5).</p>"},{"location":"courses/fundamentals/overview/#34-data-transformation","title":"3.4. Data Transformation","text":"<p>This is often considered the core of data engineering and where Data Engineers spend a significant portion of their time (often 70-80%). It involves cleaning, validating, structuring, and enriching raw data based on specific business logic.</p> <p>Example: Amazon Imagine combining product details from one Amazon system with order details from another. If product dates are in \"YYYY-MM-DD\" and order dates are in \"MM-DD-YYYY\", a Data Engineer transforms them into a single, consistent format. They also clean up errors, remove duplicate entries, and ensure all data makes sense. This is like the chef peeling, chopping, and cooking the vegetables to make them edible and delicious.</p>"},{"location":"courses/fundamentals/overview/#35-data-serving-data-modelinganalysisreporting","title":"3.5. Data Serving (Data Modeling/Analysis/Reporting)","text":"<p>Finally, after all the processing, the transformed data is made available for downstream users like Data Scientists, Data Analysts, or for building dashboards and machine learning models. This is like the chef plating the cooked dish beautifully, ready for the customers (business stakeholders) to enjoy and make decisions from.</p>"},{"location":"courses/fundamentals/overview/#4-key-roles-in-the-data-ecosystem","title":"4. Key Roles in the Data Ecosystem","text":"<p>The data world is a team sport, with various roles collaborating to extract value from data.</p>      graph TD         DataEcosystem[\"Key Roles in the Data Ecosystem\"]:::main-topic          DataEcosystem --&gt; SWE(\"Software Engineers\")         SWE --&gt; SWE_Desc(\"Develop core applications, manage smaller databases\")          DataEcosystem --&gt; DBA(\"Database Administrators (DBAs)\")         DBA --&gt; DBA_Desc(\"Design &amp; manage databases, especially OLTP systems\")          DataEcosystem --&gt; DE(\"Data Engineers\")         DE --&gt; DE_Desc(\"Build ETL/ELT pipelines, DW, Big Data systems, ensure quality &amp; governance\")         DE --&gt; DE_Analogy(\"The 'Data Plumber' connecting data sources to consumers\")          DataEcosystem --&gt; DA(\"Data Analysts\")         DA --&gt; DA_Desc(\"Answer 'what happened?', find patterns in historical data\")          DataEcosystem --&gt; DS(\"Data Scientists\")         DS --&gt; DS_Desc(\"Predict 'what can happen?', build predictive models\")          DataEcosystem --&gt; MLE(\"Machine Learning Engineers\")         MLE --&gt; MLE_Desc(\"Automate processes, deploy ML models to production\")          DataEcosystem --&gt; DataOps(\"DevOps / DataOps\")         DataOps --&gt; DataOps_Desc(\"Automate deployment, monitor systems, ensure observability\")  <ul> <li>Software Engineers: Primarily develop applications and write code for deployment. In smaller companies, they might also manage databases.</li> <li>Database Administrators (DBAs): Design and manage databases, including building tables and columns. They typically manage OLTP (Online Transactional Processing) databases.</li> <li>Data Engineers: The data \"plumbers.\" They extract, transform, and load (ETL/ELT) data, build data warehouses, work with big data processing systems (like Spark, Hadoop, Kafka), handle data integration, quality checks, and governance.</li> <li>Data Analysts: Answer questions about past events (e.g., \"What was the sales revenue last quarter?\") by finding patterns in historical data.</li> <li>Data Scientists: Predict future outcomes based on past patterns (e.g., \"What products are customers likely to buy next month?\").</li> <li>Machine Learning Engineers: Automate processes and deploy machine learning models (e.g., recommendation systems, fraud detection) into production systems.</li> <li>DevOps/DataOps: Focus on automating deployment processes, monitoring data systems, ensuring observability, and incident reporting for data pipelines.</li> </ul>"},{"location":"courses/fundamentals/overview/#5-data-generation-and-storage-systems","title":"5. Data Generation and Storage Systems","text":""},{"location":"courses/fundamentals/overview/#51-data-generation-sources","title":"5.1. Data Generation Sources","text":"<p>Data is constantly being generated from a multitude of sources within and outside an organization.</p>      graph TD         Sources[\"Data Generation Sources\"]:::main-topic         Sources --&gt; A[\"Applications (Websites, Mobile Apps)\"]         A --&gt; A_Ex(\"User clicks, purchases, Browse history, reviews (e.g., Amazon.com, Netflix)\")         Sources --&gt; B[\"APIs (Application Programming Interfaces)\"]         B --&gt; B_Ex(\"Connectors to fetch info from apps or external services\")         Sources --&gt; C[\"RDBMS (Relational Database Management Systems)\"]         C --&gt; C_Ex(\"Transactional systems where application data is stored\")         Sources --&gt; D[\"IoT Devices/Sensors\"]         D --&gt; D_Ex(\"Data from smart devices, moving trucks, industrial sensors\")         Sources --&gt; E[\"Web &amp; Social Media\"]         E --&gt; E_Ex(\"User activity, content, interactions\")         Sources --&gt; F[\"Logs &amp; Machine Data\"]         F --&gt; F_Ex(\"Data from servers, networks, applications for monitoring\")         Sources --&gt; G[\"Third-Party Data\"]         G --&gt; G_Ex(\"Data from vendors (e.g., FedEx for shipping), external partners\")"},{"location":"courses/fundamentals/overview/#52-data-storage-systems","title":"5.2. Data Storage Systems","text":"<p>Once generated, data needs to be stored efficiently. Different types of data and usage patterns require different storage solutions.</p>      graph TD         StorageSystems[\"Data Storage Systems\"]:::main-topic          StorageSystems --&gt; DBMS(\"Database Management Systems (DBMS)\")         DBMS --&gt; RelationalDB(\"Relational Databases (SQL DBs)\")         RelationalDB --&gt; Rel_Char(\"Structured, tabular (rows/cols), predefined schema\")         RelationalDB --&gt; Rel_Lang(\"SQL: SELECT, INSERT, UPDATE, DELETE\")         RelationalDB --&gt; Rel_Model(\"Data Modeling: Normalization (1NF, 2NF, 3NF)\")         RelationalDB --&gt; Rel_Ex(\"PostgreSQL, MySQL, SQL Server, Oracle\")          DBMS --&gt; NoSQLDB(\"NoSQL Databases\")         NoSQLDB --&gt; NoSQL_Char(\"Flexible schema, various formats (key-value, document, graph)\")         NoSQLDB --&gt; NoSQL_Use(\"Good for unstructured/semi-structured data, specific workloads\")         NoSQLDB --&gt; NoSQL_Ex(\"MongoDB, Cassandra, Couchbase DB\")          StorageSystems --&gt; DataWarehouse(\"Data Warehouses\")         DataWarehouse --&gt; DW_Char(\"Optimized for analytical queries, structured data, column-based\")          StorageSystems --&gt; ObjectStorage(\"Object Storage (Data Lakes)\")         ObjectStorage --&gt; OS_Char(\"Stores raw data (any format) without predefined schema (schema-on-read)\")         ObjectStorage --&gt; OS_Use(\"Cost-effective, Big Data handling, flexible\")         ObjectStorage --&gt; OS_Ex(\"Amazon S3, Azure Data Lake Storage Gen2, Google Cloud Storage\")  <ul> <li>Database Management Systems (DBMS): General-purpose systems for storing structured data, allowing easy querying, retrieval, updating, and deletion.<ul> <li>Relational Databases (SQL Databases): Store data in structured tables (rows and columns) with predefined schemas and relationships.<ul> <li>Examples: PostgreSQL, MySQL, Microsoft SQL Server, Oracle.</li> <li>Language: SQL (Structured Query Language) is used to communicate with them (SELECT, INSERT, UPDATE, DELETE).</li> <li>Data Modeling (in OLTP): Often involves Normalization to reduce data redundancy by creating many linked tables.</li> </ul> </li> <li>NoSQL Databases: Store data in various formats other than traditional rows and columns (\"Not Only SQL\").<ul> <li>Formats: Key-value, column family, document, graph data.</li> <li>Use Cases: Good for unstructured or semi-structured data and specific workloads where relational structures aren't ideal.</li> <li>Examples: MongoDB, Cassandra.</li> </ul> </li> </ul> </li> <li>Data Warehouses: Optimized for analytical workloads, storing structured data for efficient querying of large datasets.</li> <li>Object Storage (Data Lakes): Store vast amounts of raw data in its native format (structured, semi-structured, unstructured) without a predefined schema.<ul> <li>Concept: \"Dump all data as it is\" into a storage location. Users then query the data directly, defining the schema \"on read.\"</li> <li>Advantages: Extreme flexibility, cost-effective storage, handles massive volumes, supports diverse users (Data Scientists).</li> <li>Examples: Amazon S3, Azure Data Lake Storage Gen2 (ADLS Gen2), Google Cloud Storage (GCS).</li> </ul> </li> </ul>"},{"location":"courses/fundamentals/overview/#6-data-processing-systems-oltp-vs-olap","title":"6. Data Processing Systems: OLTP vs. OLAP","text":"<p>Data storage systems are often categorized by how they process data: for transactions or for analysis.</p>      graph TD         ProcessingSystems[\"Data Processing Systems\"]:::main-topic          ProcessingSystems --&gt; OLTP(\"OLTP (Online Transactional Processing)\")         OLTP --&gt; OLTP_Purp(\"Purpose: Day-to-day operations, transaction processing\")         OLTP --&gt; OLTP_Char(\"Characteristics:\")         OLTP_Char --&gt; RowBased(\"Row-based storage\")         OLTP_Char --&gt; FastCRUD(\"Fast inserts, updates, quick reads (individual records)\")         OLTP_Char --&gt; Normalized(\"Normalized data models\")         OLTP --&gt; OLTP_Use(\"Use Cases: E-commerce transactions, banking, inventory\")         OLTP --&gt; OLTP_Mgmt(\"Management: DBAs, Software Engineers\")          ProcessingSystems --&gt; OLAP(\"OLAP (Online Analytical Processing)\")         OLAP --&gt; OLAP_Purp(\"Purpose: Analysis workloads, historical data querying\")         OLAP --&gt; OLAP_Char(\"Characteristics:\")         OLAP_Char --&gt; ColBased(\"Column-based storage (faster aggregations)\")         OLAP_Char --&gt; OptimizedReads(\"Optimized for complex analytical queries\")         OLAP_Char --&gt; Denormalized(\"Often uses denormalized/dimensional models\")         OLAP --&gt; OLAP_Use(\"Use Cases: BI reporting, ML, historical analysis\")         OLAP --&gt; OLAP_Mgmt(\"Management: Data Engineers\")"},{"location":"courses/fundamentals/overview/#61-oltp-online-transactional-processing","title":"6.1. OLTP (Online Transactional Processing)","text":"<ul> <li>Purpose: Designed for processing transactional data and managing day-to-day operations.<ul> <li>Example: Amazon's checkout system. When you buy something, that single purchase needs to be recorded quickly and accurately.</li> </ul> </li> <li>Characteristics:<ul> <li>Row-based storage: Data is stored row by row, making it efficient for quickly inserting or updating individual records.</li> <li>Fast inserts, updates, and quick reads: Optimized for CRUD (Create, Read, Update, Delete) operations on single records.</li> <li>Normalized data models: Data is highly normalized to reduce redundancy.</li> </ul> </li> <li>Use Cases: E-commerce transactions, banking applications, inventory updates.</li> <li>Management: Typically managed by DBAs or software engineers.</li> </ul>"},{"location":"courses/fundamentals/overview/#62-olap-online-analytical-processing","title":"6.2. OLAP (Online Analytical Processing)","text":"<ul> <li>Purpose: Designed for analysis workloads, enabling analysis of large datasets (e.g., all sales over the last five years).<ul> <li>Example: Amazon's sales dashboard. To see total revenue for all products in a specific region over the last quarter, an OLAP system would be used.</li> </ul> </li> <li>Characteristics:<ul> <li>Column-based storage: Data is stored column by column, allowing for faster aggregation and analysis queries by reading only necessary columns without scanning full rows.</li> <li>Optimized for reads: Efficient for complex analytical queries that aggregate data across many rows.</li> <li>Denormalized data models: Often uses dimensional modeling (see Section 8).</li> </ul> </li> <li>Use Cases: Business intelligence (BI) reporting, machine learning, historical analysis.</li> <li>Management: Typically managed by Data Engineers.</li> </ul>"},{"location":"courses/fundamentals/overview/#7-etl-extract-transform-load-pipeline","title":"7. ETL (Extract, Transform, Load) Pipeline","text":"<p>ETL is the foundational process of moving data from operational systems (like OLTP databases) to analytical systems (like data warehouses).</p>      graph TD         ETL_Process[\"ETL Process (Extract, Transform, Load)\"]:::main-topic         Extract(\"1. Extract\") --&gt; Extract_Desc(\"Collect raw data from sources (DBs, APIs, Sensors)\")         Transform(\"2. Transform\") --&gt; Trans_Desc(\"Clean, validate, structure, enrich data based on business logic\")         Load(\"3. Load\") --&gt; Load_Desc(\"Move transformed data into target system (DW, Object Storage)\")          Extract_Desc --&gt; DataSources[Source Systems]         Trans_Desc --&gt; DataQuality[Data Quality &amp; Consistency]         Load_Desc --&gt; TargetSystems[Data Warehouse / Data Lake]          subgraph Transformation_Examples[\"Transformation Examples\"]             T1(\"Remove Duplicates\")             T2(\"Handle Null Values\")             T3(\"Standardize Formats (Dates, Data Types)\")             T4(\"Aggregate Data\")             T5(\"Merge Datasets\")             T6(\"Generate New Columns\")             T7(\"Filter Data\")         end         Transform --&gt; Transformation_Examples  <ul> <li>Extract: Collecting raw data from various sources (DBMS, analytics tools, IoT sensors, APIs).</li> <li>Transform: This is the crucial step where raw data becomes valuable. It involves cleaning, validating, structuring, and enriching data to make it suitable for analysis. This is where business rules are applied.<ul> <li>Examples of Transformation: Removing duplicates, handling null values, standardizing data formats (e.g., dates, data types), aggregating data (e.g., total sales per day), merging datasets (e.g., combining customer info with order history), generating new columns (e.g., calculating profit margin), and filtering data (e.g., removing test transactions).</li> </ul> </li> <li>Load: Moving the transformed data into a target system for querying and analysis.<ul> <li>Load Destinations: Typically data warehouses (Snowflake, BigQuery, Redshift) or object storage (S3, GCS, Azure Data Lake).</li> </ul> </li> <li>ETL Pipelines: These are automated processes that perform ETL operations regularly, ensuring data is fresh and available.</li> </ul>"},{"location":"courses/fundamentals/overview/#etl-vs-elt","title":"ETL vs. ELT","text":"<p>While ETL has been traditional, ELT (Extract, Load, Transform) is gaining popularity with cloud-based data warehouses.</p>      graph TD         Compare_ELT[\"ETL vs. ELT\"]:::main-topic          Compare_ELT --&gt; ETL_Flow(\"ETL: Extract -&gt; Transform -&gt; Load\")         ETL_Flow --&gt; ETL_Desc(\"Data transformed before loading into DW. Highly structured approach.\")          Compare_ELT --&gt; ELT_Flow(\"ELT: Extract -&gt; Load -&gt; Transform\")         ELT_Flow --&gt; ELT_Desc(\"Raw data loaded to staging/DW. Transformations applied 'on the fly' using SQL in DW.\")         ELT_Flow --&gt; ELT_Adv(\"Leverages DW compute power. Less initial processing for messy data.\")  <ul> <li>ETL: Data is processed and transformed before loading into the data warehouse. This is a highly structured approach.</li> <li>ELT: Raw data is loaded into a staging area or directly into the data warehouse, and transformations are applied on the fly using SQL queries within the warehouse. This leverages the powerful compute capabilities of modern cloud data warehouses.</li> </ul>"},{"location":"courses/fundamentals/overview/#8-data-warehousing-concepts","title":"8. Data Warehousing Concepts","text":"<p>Data warehouses are central to analytical data processing. Understanding how they're structured is key.</p>"},{"location":"courses/fundamentals/overview/#81-dimensional-modeling","title":"8.1. Dimensional Modeling","text":"<p>This is a primary method to store data in a data warehouse, built around two types of tables:</p>      graph TD         DM_Main[\"Dimensional Modeling\"]:::main-topic         DM_Main --&gt; FactTable[\"Fact Table\"]         FactTable --&gt; FT_Char(\"Quantitative, measurable data (e.g., sales amount, quantity)\")         FactTable --&gt; FT_Keys(\"Foreign keys to Dimension Tables\")         FactTable --&gt; FT_Central(\"Central table in a dimensional model\")          DM_Main --&gt; DimTable[\"Dimension Tables\"]         DimTable --&gt; DT_Char(\"Descriptive attributes, categorical info (e.g., product name, city, date)\")         DimTable --&gt; DT_Link(\"Multiple dimension tables linked to fact table\")  <ul> <li>Fact Table: Stores quantitative, measurable data points (e.g., sales amount, product quantity, revenue, profit) and foreign keys to dimension tables. It's the central table in a dimensional model.</li> <li>Dimension Tables: Store descriptive attributes and categorical information about the business (e.g., product name, product category, user name, user city, date attributes). Multiple dimension tables usually link to a central fact table.</li> </ul>"},{"location":"courses/fundamentals/overview/#82-schema-types","title":"8.2. Schema Types","text":"<p>Two common ways to arrange fact and dimension tables:</p>      graph LR         SchemaTypes[\"Data Warehouse Schema Types\"]:::main-topic          SchemaTypes --&gt; StarSchema[\"Star Schema\"]         StarSchema --&gt; SS_Desc(\"Central fact table directly connected to multiple dimension tables\")         StarSchema --&gt; SS_Adv(\"Highly performant, faster queries, easier to manage (Most common)\")          SchemaTypes --&gt; SnowflakeSchema[\"Snowflake Schema\"]         SnowflakeSchema --&gt; SFS_Desc(\"More normalized star schema; dimension tables have sub-dimension tables\")         SnowflakeSchema --&gt; SFS_Disadv(\"Harder to manage, less common in practice\")  <ul> <li>Star Schema: A simple dimensional model with a single fact table at the center directly connected to multiple dimension tables, resembling a star. It's highly performant, faster for querying, and easier to manage, making it the most used in the industry.</li> <li>Snowflake Schema: A more normalized version of the star schema where dimension tables have sub-dimension tables, forming a snowflake shape. It's harder to manage and less common in practice.</li> </ul>"},{"location":"courses/fundamentals/overview/#83-slowly-changing-dimensions-scds","title":"8.3. Slowly Changing Dimensions (SCDs)","text":"<p>SCDs are strategies for handling changes in dimension values over time. For example, if a customer's address changes, how do you track that in historical sales data?</p>      graph TD         SCDs[\"Slowly Changing Dimensions (SCDs)\"]:::main-topic          SCDs --&gt; SCD0(\"SCD Type 0: Fixed\")         SCD0 --&gt; SCD0_Desc(\"Dimension values don't change (e.g., Birth Date)\")          SCDs --&gt; SCD1(\"SCD Type 1: Overwrite (Upsert)\")         SCD1 --&gt; SCD1_Desc(\"New value overwrites old; no history maintained\")         SCD1 --&gt; SCD1_Use(\"Most frequently used (70-80%) due to simplicity/performance\")          SCDs --&gt; SCD2(\"SCD Type 2: New Row\")         SCD2 --&gt; SCD2_Desc(\"Adds a new row for each change; full history maintained (start/end date, active flag)\")          SCDs --&gt; SCD3(\"SCD Type 3: Partial History\")         SCD3 --&gt; SCD3_Desc(\"Stores current &amp; previous values in separate columns in the same row\")          SCDs --&gt; SCD6(\"SCD Type 6: Hybrid\")         SCD6 --&gt; SCD6_Desc(\"Combines Type 1, 2, and 3 for comprehensive tracking\")  <ul> <li>SCD Type 0: Assumes the dimension will not change. Values are fixed (e.g., a product's SKU).</li> <li>SCD Type 1 (Upsert): Overwrites old values with new ones, maintaining no history. This is the most frequently used SCD type due to its simplicity and performance (combines UPDATE for existing records and INSERT for new ones).</li> <li>SCD Type 2: Maintains a complete history of changes by adding a new row for each change, using attributes like <code>start_date</code>, <code>expiry_date</code>, and an <code>is_active</code> flag. This allows tracking historical states.</li> <li>SCD Type 3: Maintains partial history by storing the current and previous values in separate columns.</li> <li>SCD Type 6: A combination of SCD1, SCD2, and SCD3 for comprehensive tracking.</li> </ul>"},{"location":"courses/fundamentals/overview/#84-data-marts","title":"8.4. Data Marts","text":"graph TD         DataMarts[\"Data Marts\"]:::main-topic         DataMarts --&gt; DM_Desc(\"Subsets of a Data Warehouse\")         DataMarts --&gt; DM_Purpose(\"Serve analytical needs of specific departments/teams\")         DataMarts --&gt; DM_Benefit(\"Improve performance &amp; relevance by focusing data access\")  <ul> <li>Data Marts: Subsets of a data warehouse designed to serve the analytical needs of specific departments or teams within an organization. They contain only the data relevant to that particular group, improving performance and relevance.</li> </ul>"},{"location":"courses/fundamentals/overview/#85-data-warehouse-layers","title":"8.5. Data Warehouse Layers","text":"<p>Data warehouses often use layers to organize data as it flows through the system.</p>      graph TD         DW_Layers[\"Data Warehouse Layers\"]:::main-topic          DW_Layers --&gt; StagingLayer(\"1. Staging Layer\")         StagingLayer --&gt; SL_Purpose(\"Intermediate layer for initially dumped extracted data\")         StagingLayer --&gt; SL_Types(\"Transient (temporary, truncated after use) OR Persistent (historical preservation)\")          DW_Layers --&gt; CoreLayer(\"2. Core Layer\")         CoreLayer --&gt; CL_Purpose(\"Transformed data from staging, structured for consumption\")         CoreLayer --&gt; CL_Content(\"Contains Dimensional Data Model (Fact &amp; Dimension Tables)\")  <ul> <li>Staging Layer: An intermediate layer where extracted data is initially dumped. This layer can be Transient (temporary, deleted after use, most common) or Persistent (preserves historical data).</li> <li>Core Layer: Where data is transformed from the staging layer and structured into fact and dimension tables for consumption by reports and analytics.</li> </ul>"},{"location":"courses/fundamentals/overview/#86-incremental-loading","title":"8.6. Incremental Loading","text":"graph TD         IncrementalLoading[\"Incremental Loading\"]:::main-topic         IncrementalLoading --&gt; IL_Concept(\"Only new or changed data loaded from source to DW\")         IncrementalLoading --&gt; IL_Benefit(\"Saves computational resources &amp; time\")         IncrementalLoading --&gt; IL_Standard(\"Standard practice in data engineering\")  <ul> <li>Incremental Loading: A data fetching strategy where only new or changed data is loaded from the source system into the data warehouse, rather than reloading the entire dataset. This saves computational resources and time and is standard practice.</li> </ul>"},{"location":"courses/fundamentals/overview/#9-data-lake","title":"9. Data Lake","text":"<p>A Data Lake is a centralized repository that stores a vast amount of raw data in its native format (structured, semi-structured, and unstructured).</p>      graph TD         DL_Main[\"Data Lake\"]:::main-topic         DL_Main --&gt; Concept(\"Concept: Store all raw data 'as is'\")         Concept --&gt; SchemaOnRead(\"Schema on Read: Define schema when you query, not when you store\")         DL_Main --&gt; Advantages(\"Advantages\")         Advantages --&gt; Flex(\"Flexibility: Any data type, no prior schema needed\")         Advantages --&gt; CostEff(\"Cost-effective: Significantly lower storage costs\")         Advantages --&gt; BigData(\"Big Data Handling: Efficient for massive volumes\")         Advantages --&gt; DiverseUsers(\"Supports diverse users: Data Scientists for ML, Analysts for raw exploration\")         DL_Main --&gt; Examples(\"Examples: Amazon S3, Azure Data Lake Storage Gen2 (ADLS Gen2), Google Cloud Storage (GCS)\")  <ul> <li>Concept: The idea is to \"dump all data as it is\" (e.g., CSV, Parquet, JSON) into a storage location. Users can then query the data from the data lake itself, defining the schema \"on read\" (meaning the schema is applied at query time, not at ingest time).</li> <li>Advantages:<ul> <li>Flexibility: Can store any type of data without prior schema definition.</li> <li>Cost-effective: Storage costs are significantly lower compared to data warehouses.</li> <li>Big Data Handling: Efficiently deals with massive volumes of data.</li> <li>Supports diverse users: Ideal for data scientists and data analysts who need access to raw data for various modeling and analysis tasks.</li> </ul> </li> <li>Examples: Amazon S3, Azure Data Lake Storage Gen2 (ADLS Gen2), Google Cloud Storage (GCS).</li> </ul>"},{"location":"courses/fundamentals/overview/#10-data-lake-vs-data-warehouse-comparison","title":"10. Data Lake vs. Data Warehouse Comparison","text":"<p>Understanding the differences is crucial for choosing the right tool for the job.</p> Feature Data Warehouse Data Lake Data Structure Structured Unstructured, semi-structured, structured Schema Pre-defined (schema on write) Schema on read (schema defined after data stored) Users Business Analysts, BI Developers Data Scientists, Data Analysts Use Cases Batch processing, BI reporting, traditional analytics Stream processing, machine learning, real-time analysis, raw data exploration Data State Processed, clean, refined Raw, undefined Data Volume Smaller data (typically millions of rows) Large data (billions of rows, petabytes) Cost Higher storage and processing costs Lower storage costs Performance Optimized for structured queries and reports Flexible, better for diverse workloads"},{"location":"courses/fundamentals/overview/#11-lakehouse-architecture","title":"11. Lakehouse Architecture","text":"<p>The Lakehouse is a new paradigm that combines the best aspects of data lakes and data warehouses. It's designed to give you the flexibility and cost-effectiveness of a data lake with the performance and structured querying capabilities of a data warehouse.</p>      graph TD         Lakehouse[\"Lakehouse Architecture\"]:::main-topic         Lakehouse --&gt; Concept(\"Concept: Fuses DW &amp; DL benefits\")         Concept --&gt; Mechanism(\"Data in low-cost Data Lake, with a 'metadata/abstraction layer' on top\")         Mechanism --&gt; AddsFeatures(\"Adds DW functionalities: Dimensional Modeling, Schema Enforcement, ACID Transactions\")         Lakehouse --&gt; Benefits(\"Benefits:\")         Benefits --&gt; LowCost(\"Low storage cost of Data Lake\")         Benefits --&gt; DW_Perf(\"High performance &amp; structured querying of Data Warehouse\")  <ul> <li>Concept: Data is stored in a cost-effective data lake, but a \"metadata layer\" or \"abstraction layer\" is applied on top to enable data warehousing functionalities like dimensional modeling, schema enforcement, and ACID transactions.</li> <li>Benefits: Offers the low cost and flexibility of data lakes with the performance and structured querying capabilities of data warehouses.</li> </ul>"},{"location":"courses/fundamentals/overview/#medallion-architecture-cloud-data-lakehouse-pattern","title":"Medallion Architecture (Cloud Data Lakehouse Pattern)","text":"<p>A common implementation of the Lakehouse architecture, defining data quality layers:</p>      graph LR         Medallion[\"Medallion Architecture\"]:::main-topic         Medallion --&gt; Bronze(\"Bronze (Raw) Layer\")         Bronze --&gt; B_Desc(\"Stores raw, unfiltered data, no changes\")          Medallion --&gt; Silver(\"Silver (Transformed/Cleaned) Layer\")         Silver --&gt; S_Desc(\"Applies transformations, cleaning, basic aggregations\")         Silver --&gt; S_Schema(\"Schema enforced and evolved here\")          Medallion --&gt; Gold(\"Gold (Curated/Consumption) Layer\")         Gold --&gt; G_Desc(\"Highly refined, aggregated, business-ready data\")         Gold --&gt; G_Use(\"Suitable for BI, ML (Fact &amp; Dimension tables)\")          Bronze -- Data Flow --&gt; Silver         Silver -- Data Flow --&gt; Gold  <ul> <li>Bronze (Raw) Layer: Stores raw, unfiltered data as it arrives from sources with no changes. This is the initial landing zone.</li> <li>Silver (Transformed/Cleaned) Layer: Applies transformations, cleaning, and basic aggregations to the bronze layer data. This is where schema is enforced and evolved.</li> <li>Gold (Curated/Consumption) Layer: Stores highly refined, aggregated, and business-ready data, often in fact and dimension tables. This layer is suitable for direct consumption by business intelligence and machine learning applications.</li> </ul>"},{"location":"courses/fundamentals/overview/#12-file-formats-for-big-data","title":"12. File Formats for Big Data","text":"<p>How data is stored on disk significantly impacts query performance, especially with large datasets.</p>      graph TD         FileFormats[\"File Formats for Big Data\"]:::main-topic          FileFormats --&gt; RowBased(\"Row-Based File Formats\")         RowBased --&gt; RB_Char(\"Store data row by row\")         RB_Char --&gt; RB_Adv(\"Efficient for writing and updating individual records\")         RB_Char --&gt; RB_Use(\"Used more in OLTP systems\")         RB_Char --&gt; RB_Ex(\"Examples: CSV, Avro\")          FileFormats --&gt; ColumnBased(\"Column-Based File Formats\")         ColumnBased --&gt; CB_Char(\"Store data column by column\")         CB_Char --&gt; CB_Adv(\"Highly efficient for analytical queries (read only necessary columns)\")         CB_Char --&gt; CB_Use(\"Used more in OLAP/Big Data analytics\")         CB_Char --&gt; CB_Ex(\"Examples: Parquet, ORC\")          FileFormats --&gt; DeltaFormat[\"Delta Format (on Parquet)\"]         DeltaFormat --&gt; DF_Main(\"Open table format built on top of Parquet\")         DeltaFormat --&gt; DF_Features(\"Key Features:\")         DF_Features --&gt; TL(\"Transaction Log: Enables Data Time Travel (versioning)\")         DF_Features --&gt; SE(\"Schema Evolution: Add/change columns without breaking pipelines\")         DF_Features --&gt; ACID(\"ACID Transactions: Atomicity, Consistency, Isolation, Durability (data reliability)\")         DF_Features --&gt; CostEff(\"Cost-effective: Leverages low-cost Parquet storage + transactional capabilities\")  <ul> <li>Row-Based File Formats: Store data row by row (e.g., CSV, Avro).<ul> <li>Advantages: Efficient for writing and updating individual records. Used more in OLTP systems.</li> </ul> </li> <li>Column-Based File Formats: Store data column by column (e.g., Parquet, ORC).<ul> <li>Advantages: Highly efficient for analytical queries as it allows reading only necessary columns without scanning entire rows. Used more in OLAP/Big Data analytics.</li> </ul> </li> <li>Delta Format: An open table format built on top of Parquet.<ul> <li>Key Features:<ul> <li>Transaction Log: Stores metadata and history of operations, enabling Data Time Travel (versioning) to revert to previous states of data.</li> <li>Schema Evolution: Allows adding or changing columns to a table without breaking existing data pipelines.</li> <li>ACID Transactions: Provides Atomicity, Consistency, Isolation, and Durability, ensuring data reliability and integrity, similar to traditional databases.</li> <li>Cost-effective: Leverages low-cost storage of Parquet files while adding transactional capabilities.</li> </ul> </li> </ul> </li> </ul>"},{"location":"courses/fundamentals/overview/#13-cloud-data-engineering","title":"13. Cloud Data Engineering","text":"<p>Cloud computing involves renting computational resources (servers, storage, networking) from third-party providers (like AWS, Azure, GCP) via the internet, instead of owning and maintaining them on-premises.</p>      graph TD         CloudDE[\"Cloud Data Engineering\"]:::main-topic          CloudDE --&gt; Benefits(\"Benefits of Cloud Computing\")         Benefits --&gt; Scalability(\"Scalability: Easily scale resources up/down\")         Benefits --&gt; CostEff(\"Cost-effectiveness: Pay-per-use model\")         Benefits --&gt; Maint(\"Reduced Maintenance: Provider handles infra, security\")         Benefits --&gt; DR(\"Disaster Recovery: Built-in resilience\")          CloudDE --&gt; ServiceModels(\"Cloud Service Models\")         ServiceModels --&gt; IaaS(\"IaaS (Infrastructure as a Service): VMs, Networks (e.g., AWS EC2)\")         ServiceModels --&gt; PaaS(\"PaaS (Platform as a Service): App running platform (e.g., AWS Lambda)\")         ServiceModels --&gt; SaaS(\"SaaS (Software as a Service): Ready-to-use software (e.g., Google Suite)\")          CloudDE --&gt; Providers(\"Major Cloud Providers &amp; Data Services\")         Providers --&gt; AWS(\"Amazon Web Services (AWS)\")         AWS --&gt; AWS_Services(\"S3, Redshift, Glue, EMR, Kinesis, RDS, DynamoDB, Athena\")         AWS --&gt; AWS_Example(\"Example Architecture (Dream11): Kafka -&gt; S3 -&gt; Spark -&gt; Redshift -&gt; BI\")          Providers --&gt; Azure(\"Microsoft Azure\")         Azure --&gt; Azure_Services(\"ADLS Gen2, Synapse Analytics, Data Factory, Databricks, Event Hub, Power BI\")         Azure --&gt; Azure_Example(\"Example Architecture: Event Hub/IoT Hub -&gt; ADLS Gen2 (Bronze via ADF) -&gt; Databricks (Silver) -&gt; Synapse (Gold) -&gt; Power BI\")          Providers --&gt; GCP(\"Google Cloud Platform (GCP)\")         GCP --&gt; GCP_Services(\"Cloud Storage, BigQuery, DataProc, Pub/Sub, Cloud Functions\")          CloudDE --&gt; HybridCloud(\"Hybrid Cloud: Multiple clouds / On-premise + Cloud\")  <ul> <li>Benefits:<ul> <li>Scalability: Easily scale resources up or down as needed, without physical hardware limitations.</li> <li>Cost-effectiveness: Pay-per-use model, only paying for consumed resources.</li> <li>Reduced Maintenance: Cloud providers handle infrastructure maintenance, security, and replication.</li> <li>Disaster Recovery: Built-in resilience against hardware failures or natural disasters.</li> </ul> </li> <li>Service Models:<ul> <li>IaaS (Infrastructure as a Service): Provides raw computing infrastructure (e.g., virtual machines like AWS EC2).</li> <li>PaaS (Platform as a Service): Provides a platform for running applications, abstracting away infrastructure management (e.g., AWS Lambda).</li> <li>SaaS (Software as a Service): Provides ready-to-use software applications over the internet (e.g., Google Suite).</li> </ul> </li> <li>Major Cloud Providers:<ul> <li>Amazon Web Services (AWS): Widely used, especially by startups.<ul> <li>Key Data Services: S3 (object storage/data lake), Redshift (data warehouse), Glue (serverless Spark ETL), EMR (managed Spark), Kinesis (real-time streaming), RDS (relational databases), DynamoDB (NoSQL), Athena (ad-hoc query engine).</li> <li>Example Architecture (Dream11): Uses Kafka for ingestion, S3 as data lake, Spark (EMR/Glue) for ETL, Redshift as data warehouse, Athena for ad-hoc analysis, Looker/Jupyter for visualization/data science.</li> </ul> </li> <li>Microsoft Azure: Growing rapidly, popular with enterprise-level companies.<ul> <li>Key Data Services: Azure Data Lake Storage Gen2 (ADLS Gen2) (data lake), Azure Synapse Analytics (data warehouse, SQL pool), Azure Data Factory (ETL, orchestration, low-code), Azure Databricks (managed Apache Spark), Azure Event Hub (streaming), Power BI (reporting).</li> <li>Example Architecture (Typical Azure Lakehouse): Data streams from Event Hub/IoT Hub to ADLS Gen2 (bronze layer via ADF), processed by Databricks (silver layer), then loaded to Synapse Analytics (gold layer for facts/dimensions), consumed by Power BI and data scientists.</li> </ul> </li> <li>Google Cloud Platform (GCP): Also offers robust data services.<ul> <li>Key Data Services: Cloud Storage (data lake), BigQuery (data warehouse, serverless), DataProc (managed Spark), Pub/Sub (messaging/streaming), Cloud Functions (serverless compute).</li> </ul> </li> </ul> </li> <li>Hybrid Cloud: Using services from multiple cloud providers or a combination of on-premise and cloud resources.</li> </ul>"},{"location":"courses/fundamentals/overview/#14-important-tools-for-data-engineering","title":"14. Important Tools for Data Engineering","text":"<p>To become a proficient Data Engineer, mastering certain tools and technologies is essential.</p>      graph TD         DE_Tools[\"Important Tools for Data Engineering\"]:::main-topic          DE_Tools --&gt; Langs(\"Programming Languages\")         Langs --&gt; Python(\"Python (Pandas, ETL, Ingestion)\")         Langs --&gt; ScalaJava(\"Scala / Java (for Spark, Flink, Kafka)\")          DE_Tools --&gt; SQL_Tool(\"SQL (Structured Query Language)\")         SQL_Tool --&gt; SQL_Core(\"Non-negotiable: SELECT, INSERT, UPDATE, DELETE, Joins, Aggs, Window Funcs, Data Modeling\")          DE_Tools --&gt; Linux(\"Linux Commands (Basic)\")         Linux --&gt; Linux_Use(\"Interact with servers (most run Linux)\")          DE_Tools --&gt; DW_Tools(\"Data Warehouses\")         DW_Tools --&gt; Snowflake(\"Snowflake (Cloud-agnostic, highly demanded)\")         DW_Tools --&gt; BigQuery(\"BigQuery (Google's DW)\")         DW_Tools --&gt; Redshift(\"Redshift (AWS's DW)\")          DE_Tools --&gt; ProcessingTools(\"Data Processing Frameworks\")         ProcessingTools --&gt; Spark(\"Apache Spark (Big Data: Batch &amp; Streaming)\")         Spark --&gt; Spark_Concepts(\"DataFrames, Transformations, Actions, Lazy Evaluation, UDFs, Partitioning\")         Spark --&gt; Databricks_M(\"Databricks (Managed Spark)\")         ProcessingTools --&gt; Kafka(\"Apache Kafka (Real-time Streaming &amp; Ingestion)\")         ProcessingTools --&gt; Flink(\"Apache Flink (Real-time Analytics, Stream Processing)\")          DE_Tools --&gt; OrchestrationTools(\"Data Orchestration Tools\")         OrchestrationTools --&gt; Airflow(\"Apache Airflow (Widely used scheduler)\")         OrchestrationTools --&gt; ModernOrch(\"Modern Tools: Dagster, Mage, Prefect\")          DE_Tools --&gt; ModernStackTools(\"Modern Data Stack Tools\")         ModernStackTools --&gt; Ingestion(\"Ingestion: Fivetran, Airbyte, Stitch\")         ModernStackTools --&gt; Transformation(\"Transformation: DBT (Data Build Tool)\")         ModernStackTools --&gt; BI_Viz(\"BI/Visualization: Tableau, Power BI, Looker\")         ModernStackTools --&gt; Quality(\"Data Quality: Great Expectations\")         ModernStackTools --&gt; Metadata(\"Metadata: OpenLineage, DataHub\")  <ul> <li>Programming Languages:<ul> <li>Python: Highly recommended due to its ease of learning, extensive packages (e.g., Pandas for data transformation, working with various file formats), and widespread industry use for ETL and data ingestion.</li> <li>Scala/Java: Also relevant, especially since many open-source big data frameworks (like Apache Spark) are written in Java.</li> </ul> </li> <li>SQL (Structured Query Language): Non-negotiable. The backbone for communicating with databases. Essential for SELECT, INSERT, UPDATE, DELETE operations, data types, table creation, joins, aggregation functions, subqueries, CTEs, window functions, and data modeling.</li> <li>Linux Commands: Basic commands (<code>cd</code>, <code>ls</code>, <code>cp</code>, <code>mv</code>, <code>rm</code>, <code>cat</code>, <code>grep</code>, <code>ssh</code>) for interacting with servers, as most online servers run on Linux.</li> <li>Data Warehouses:<ul> <li>Snowflake: Cloud-independent, highly demanded, excellent for learning data warehousing concepts.</li> <li>BigQuery: Google's data warehouse, a popular choice.</li> <li>Redshift: AWS's data warehouse.</li> </ul> </li> <li>Data Processing Frameworks:<ul> <li>Apache Spark: Very important for processing big data (batch and streaming). Concepts include DataFrames, transformations, actions, lazy evaluation, UDFs, partitioning, and RDDs. Databricks provides a managed Spark environment.</li> <li>Apache Kafka: Essential for real-time data streaming and ingestion. Data producers send data to Kafka, and consumers process it.</li> <li>Apache Flink: Used for real-time analytics and stream processing.</li> </ul> </li> <li>Data Orchestration Tools: Manage and schedule data pipelines (ETL jobs) to run in a coordinated manner, especially when dependencies exist.<ul> <li>Apache Airflow: One of the most widely used tools.</li> <li>Modern Tools: Dagster, Mage, Prefect (often quicker to learn than Airflow).</li> </ul> </li> <li>Modern Data Stack Tools: A combination of new tools designed to simplify data operations.<ul> <li>Ingestion: Fivetran, Airbyte, Stitch.</li> <li>Transformation: DBT (Data Build Tool).</li> <li>BI/Visualization: Tableau, Power BI, Looker, Data Studio.</li> <li>Data Quality: Great Expectations.</li> <li>Metadata: OpenLineage, DataHub.</li> </ul> </li> </ul>"},{"location":"courses/fundamentals/overview/#15-undercurrents-and-important-concepts","title":"15. Undercurrents and Important Concepts","text":"<p>These are fundamental aspects to consider when building robust and reliable data solutions.</p>      graph TD         Undercurrents[\"Undercurrents &amp; Supporting Concepts\"]:::main-topic          Undercurrents --&gt; DataSecurity(\"Data Security\")         DataSecurity --&gt; SecurityConcepts(\"Confidentiality, Integrity, Availability\")         DataSecurity --&gt; SecurityMeasures(\"Encryption, Access Control, Data Classification, Network Security\")         DataSecurity --&gt; DataMasking(\"Data Masking: Hide sensitive info\")          Undercurrents --&gt; DataMgmt(\"Data Management / Governance\")         DataMgmt --&gt; GovernanceConcepts(\"Discoverability, Definitions, Accountability\")          Undercurrents --&gt; DataOps(\"DataOps\")         DataOps --&gt; DataOps_Desc(\"Automating &amp; monitoring data systems, observability, incident reporting\")          Undercurrents --&gt; DataArch(\"Data Architecture\")         DataArch --&gt; OperationalArch(\"Operational Arch. ('Why'): Business Goals (CX, Efficiency)\")         DataArch --&gt; TechnicalArch(\"Technical Arch. ('How'): Technologies, Methodologies\")         TechnicalArch --&gt; ArchPrinciples(\"Principles: Simplicity, Right Tools, Scale, Automation, Security\")          Undercurrents --&gt; Orchestration(\"Orchestration\")         Orchestration --&gt; Orchestration_Desc(\"Coordinating, scheduling, managing dependent tasks in pipelines\")          Undercurrents --&gt; SWE_BP(\"Software Engineering Best Practices\")         SWE_BP --&gt; SWE_BP_Desc(\"Applying SWE principles to ETL/transform code (Scalability, Maintainability, Testing)\")          Undercurrents --&gt; UpDownStream(\"Upstream &amp; Downstream\")         UpDownStream --&gt; Upstream(\"Upstream: Data Sources (DBAs, SWEs)\")         UpDownStream --&gt; Downstream(\"Downstream: Data Consumers (Analysts, DS, Reporting)\")  <ul> <li>Data Security: Ensuring data is protected from unauthorized access, modification, or destruction.<ul> <li>Concepts: Confidentiality (authorized access only), Integrity (accuracy and completeness), Availability (data accessible when needed).</li> <li>Measures: Encryption, access control, data classification, network security.</li> <li>Data Masking: A technique to hide sensitive information (e.g., credit card numbers, SSNs) by replacing it with random or masked data.</li> </ul> </li> <li>Data Management/Governance: Ensuring data is organized, discoverable, and understood.<ul> <li>Concepts: Data Discoverability (easy to find data), Definitions (understanding column meanings), Accountability (identifying data owners).</li> </ul> </li> <li>DataOps: Focuses on automating and monitoring data systems and pipelines, including observability and incident reporting.</li> <li>Data Architecture: Designing systems to support evolving data needs, involving careful evaluation of trade-offs.<ul> <li>Operational Architecture (\"Why\"): Focuses on defining business goals and requirements (e.g., improving customer experience, increasing operational efficiency).</li> <li>Technical Architecture (\"How\"): Focuses on choosing specific technologies and methodologies to meet operational goals. Principles include simplicity, choosing the right tools, building for scale and flexibility, embedding automation, and prioritizing security.</li> </ul> </li> <li>Orchestration: Coordinating, scheduling, and managing tasks within data pipelines, especially when they are dependent on each other.</li> <li>Software Engineering Best Practices: Applying principles of programming, software design (scalability, design patterns), testing, and debugging when writing ETL and transformation code.</li> <li>Upstream and Downstream: Describes the flow of data relative to a Data Engineer.<ul> <li>Upstream: Sources of data (e.g., DBAs, Software Engineers managing source databases/APIs).</li> <li>Downstream: Consumers of data (e.g., Data Analysts, Data Scientists, reporting systems, BI tools).</li> </ul> </li> </ul>"},{"location":"courses/fundamentals/overview/#data-engineering-explained-with-amazon","title":"Data Engineering: Explained with Amazon","text":"<p>Let's break down the fundamentals of data engineering using the example of a familiar business like Amazon.</p> <p>Imagine Amazon as a vast, bustling marketplace, operating entirely online. Every action a customer takes, and every internal process the company runs, generates an enormous amount of information. Data engineering is essentially about making sense of all this information so that Amazon can run its business better and serve its customers more effectively.</p> <p>You can think of a data engineer as a skilled chef or a plumber for data.</p> <p>Here\u2019s why Amazon needs data engineering and how it works:</p>"},{"location":"courses/fundamentals/overview/#1-why-amazon-needs-data-engineering-making-smarter-decisions","title":"1. Why Amazon Needs Data Engineering \u2013 Making Smarter Decisions:","text":"<ul> <li>Understanding Customers: Amazon wants to know what you like, what you buy, and how you interact with their website to provide better services and personalized recommendations.</li> <li>Increasing Profit: By understanding trends and customer behavior, Amazon can optimize its operations, potentially leading to more sales and higher profits.</li> <li>Improving Operations: They need to identify problems (like delays in shipping or issues with payment processing) and fix them. Data can show exactly where bottlenecks are.</li> <li>Moving Beyond Assumptions: Instead of just guessing what might be happening, Amazon wants to make decisions based on solid facts provided by data. Every action taken in Amazon's data ecosystem aims to create value for the business, whether it's saving costs, improving processes, or understanding customers.</li> </ul>"},{"location":"courses/fundamentals/overview/#2-data-generation-the-raw-ingredients","title":"2. Data Generation \u2013 The Raw Ingredients:","text":"<ul> <li>Every Click and Action Creates Data: When you browse products on Amazon, click on a \"Buy Now\" button, add items to your cart, or leave a review, you are generating data.</li> <li>Beyond Customer Interactions: Data also comes from other sources, like Amazon's internal systems (inventory management, shipping logistics), logs from their servers, and interactions with third-party vendors (like FedEx).</li> <li>Think of this as all the raw, sometimes messy, ingredients arriving in the chef's kitchen \u2013 potatoes, onions, carrots, spices, but all mixed together.</li> </ul>"},{"location":"courses/fundamentals/overview/#3-data-storage-the-pantry","title":"3. Data Storage \u2013 The Pantry:","text":"<ul> <li>Initially, all this raw data gets stored in specialized systems called Database Management Systems (DBMS), often referred to as OLTP (Online Transactional Processing) databases. These databases are like Amazon's cash registers, very good at quickly recording individual transactions (like a single purchase). They store data in rows, making it fast to insert or update a specific record.</li> <li>However, these OLTP systems aren't designed for quickly analyzing large amounts of historical data (e.g., all sales over the last five years).</li> <li>To handle that, Amazon moves and stores data in data warehouses (also called OLAP - Online Analytical Processing databases). These are specifically designed for fast analysis of vast amounts of data, usually storing data in columns rather than rows, which is more efficient for large-scale queries. Examples include Amazon Redshift or Snowflake.</li> <li>Amazon also uses data lakes (like AWS S3), which are massive storage areas where all raw data \u2013 structured, unstructured, or semi-structured (like text from customer reviews or images) \u2013 can be dumped as is, before any major cleaning or structuring. Data lakes are very cheap for storage and offer flexibility because you decide how to structure the data when you read it, not when you store it.</li> </ul>"},{"location":"courses/fundamentals/overview/#4-data-engineerings-role-the-chef-at-work","title":"4. Data Engineering's Role \u2013 The Chef at Work:","text":"<ul> <li>The data engineer's main job is to build the \"pipes\" and \"processes\" that extract this raw data from its various sources (OLTP databases, sensor data, third-party APIs), transform it into a clean, consistent, and usable format, and then load it into the data warehouse or data lake for analysis. This process is known as ETL (Extract, Transform, Load) or sometimes ELT (Extract, Load, Transform).</li> <li>Transformation is key: Imagine combining your product details from one system with order details from another. If product dates are in \"YYYY-MM-DD\" and order dates are in \"MM-DD-YYYY\", a data engineer transforms them into a single, consistent format. They also clean up errors, remove duplicate entries, and ensure all data makes sense. This is like the chef peeling, chopping, and cooking the vegetables to make them edible and delicious.</li> <li>The goal of transformation is to organize data into a proper structure so that it can be easily visualized and understood.</li> </ul>"},{"location":"courses/fundamentals/overview/#5-data-serving-the-plated-dish","title":"5. Data Serving \u2013 The Plated Dish:","text":"<ul> <li>Once the data is clean, transformed, and loaded into the data warehouse or data lake, it's ready to be served.</li> <li>Data Analysts at Amazon use this data to understand what has happened (e.g., \"What was the sales revenue of Product X last year compared to the last five years?\") and create dashboards and reports for business leaders.</li> <li>Data Scientists and Machine Learning Engineers use this data to predict what will happen (e.g., \"How many units of Product X will be sold in the next six months?\") and build things like Amazon's personalized recommendation system, showing you products you might like based on your past behavior.</li> <li>This is like the chef plating the cooked dish beautifully, ready for the customers (business stakeholders) to enjoy and make decisions from.</li> </ul> <p>In essence, data engineering at Amazon ensures that all the vast amounts of raw data generated are effectively collected, processed, and made available in a usable format to help the company make data-driven decisions, optimize operations, and improve customer experience. It's the backbone that enables other data professionals (like data analysts and data scientists) to extract valuable insights.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/","title":"Cheat Sheet","text":""},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#power-bi-service-cheat-sheet","title":"\ud83d\ude80 Power BI Service Cheat Sheet","text":"<p>Master the essential concepts of Power BI Service for publishing, sharing, and collaborating on your data insights. </p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#power-bi-service-overview-the-cloud-command-center","title":"\ud83d\udfe1 Power BI Service Overview: The Cloud Command Center","text":"<p>The Power BI Service is the secure, cloud-based platform by Microsoft that serves as the central hub for your business intelligence ecosystem. It's where the magic of data sharing, collaboration, and consumption truly happens after you've built your reports in Power BI Desktop.</p> <ul> <li>\ud83d\udcc1 Repository: Host your .PBIX files\u2014reports, dashboards, datasets\u2014all in one place.  </li> <li>\ud83d\udd12 Secure Sharing: Seamless collaboration across your organization (and beyond).  </li> <li>\u23f0 Refresh &amp; Alerts: Automated data refreshes and data-driven alerts keep insights up to date.  </li> <li>\ud83c\udf10 Anywhere Access: Consume via web browser or mobile app on any device.</li> </ul> <pre><code>sequenceDiagram\n\n    participant DS   as \"Data Sources\"\n    participant PBD  as \"Power BI Desktop\"\n    participant PBIS as \"Power BI Service\"\n    participant EU   as \"End Users\"\n\n    PBD  -&gt;&gt; PBIS : Publish Report (.PBIX)\n    PBIS -&gt;&gt; EU   : Share &amp; Collaborate\n    EU   -&gt;&gt; PBIS : Consume via Web/Mobile\n    PBIS -&gt;&gt; DS   : Schedule Refresh (via Gateway)\n</code></pre> \ud83c\udf0a Power BI Ecosystem Flow"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#workspaces-your-collaborative-sandboxes","title":"\ud83d\uddc2\ufe0f Workspaces: Your Collaborative Sandboxes","text":"<p>A Workspace is a container for organizing and collaborating on related Power BI content (Reports, Dashboards, Datasets, Dataflows).</p> <ul> <li>My Workspace: Personal sandbox\u2014private, not directly shareable.  </li> <li>Shared/App Workspace: Team collaboration hub; source for Power BI Apps.  </li> <li>Capacity: </li> <li>Pro license: 10 GB per workspace  </li> <li>Premium/Fabric: up to 100 TB  </li> </ul> <pre><code>graph LR\n    A[Workspace]\n    A -- Private --&gt; B(My Workspace)\n    A -- Collaborative --&gt; C(Shared/App Workspace)\n    C -- Publishes --&gt; D(Power BI Apps)</code></pre> \ud83d\udcca Workspace Types <p>\ud83d\udca1 When to Use: - My Workspace: Drafts &amp; personal exploration. - Shared/App Workspace: Team builds &amp; published Apps.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#reports-the-interactive-storytellers","title":"\ud83d\udcca Reports: The Interactive Storytellers","text":"<p>A Report is a multi-page document of visuals, charts, and tables from one dataset.</p> <ul> <li>Built in Power BI Desktop, published to Service.  </li> <li>Rich interactivity: filters, slicers, drill-through.  </li> <li>Ideal for deep exploration &amp; detailed storytelling.</li> </ul> <p>\ud83d\udca1 Use Reports When: - You need multi-page, interactive analysis. - Users must slice/dice and drill down.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#dashboards-executive-snapshots","title":"\ud83d\uddbc\ufe0f Dashboards: Executive Snapshots","text":"<p>A Dashboard is a single-page canvas of pinned visuals (tiles) from one or more reports.</p> <ul> <li>Created in Power BI Service only.  </li> <li>Combines visuals across reports/datasets.  </li> <li>Perfect for high-level KPI monitoring and data-driven alerts.</li> </ul> <p>\ud83d\udca1 Use Dashboards When: - You need a one-page executive overview. - Metrics must trigger immediate notifications.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#report-vs-dashboard-a-quick-comparison","title":"\ud83c\udd9a Report vs Dashboard: A Quick Comparison","text":"Feature Report Dashboard Pages Multi-page for deep analysis Single-page \u201cone-pager\u201d Creation Tool Power BI Desktop Power BI Service only Underlying Data Single dataset Multiple reports/datasets Interactivity High (filters, slicers, drill-through) Limited (clicking tile navigates) Use Case Detailed analysis &amp; deep dives KPI monitoring &amp; quick checks Alerts \u274c \u2705 data-driven alerts"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#power-bi-apps-simplified-distribution","title":"\ud83d\udce6 Power BI Apps: Simplified Distribution","text":"<p>A Power BI App bundles curated reports and dashboards for broad consumption.</p> <ul> <li>Built in App Workspaces; delivers a view-only experience.  </li> <li>Simplifies sharing with large audiences without workspace access.</li> </ul> <p>\ud83d\udca1 When to Use Apps: - To distribute polished content at scale. - For version-controlled, production-ready experiences.</p> <p>\ud83d\udcb3 Viewer Licensing </p> Viewer License Required? Scenario Free \u2705 If App is on Premium/Fabric capacity Pro \u2705 Standard Pro workspace (publisher &amp; viewer both need Pro) PPU \u2705 Personal Premium (paginated reports, AI, 48 refreshes/day)"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#power-bi-license-types-pricing","title":"\ud83d\udcb3 Power BI License Types &amp; Pricing","text":"<p>Microsoft offers flexible licensing for various needs, from individual use to enterprise-wide deployments.</p> License Type Cost (USD) Sharing Key Features / Use Case Free $0 \u274c no sharing Individual analysis; view content in Premium capacity Pro ~$10/user/month \u2705 Pro\u2194Pro Team collaboration; shared workspaces; 8 refreshes/day PPU ~$20/user/month \u2705 PPU\u2194PPU All Pro + Premium features (paginated, AI) ; 48 refreshes/day Premium/Fabric From $262/month \u2705 with Free Dedicated resources; enterprise-scale; advanced AI; auto-scaling <p>Costs approximate; vary by region &amp; enterprise agreements.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#power-bi-security-safeguarding-your-data","title":"\ud83d\udd10 Power BI Security: Safeguarding Your Data","text":""},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#workspace-roles","title":"Workspace Roles","text":"<p>Assign roles to control content management:</p> Role Key Capabilities Admin Full control: manage access, publish apps, delete content Member Create/edit content, publish apps, schedule refreshes; cannot manage access Contributor Create/edit content; cannot publish apps\u2014ideal for devs Viewer View-only access to published content <pre><code>sequenceDiagram\n    participant User\n    participant Report\n    participant Dataset\n    participant RLS as RLS Roles (DAX)\n\n    User-&gt;&gt;Report: Access Report\n    Report-&gt;&gt;Dataset: Request Data\n    Dataset-&gt;&gt;RLS: Check Role\n    RLS--&gt;&gt;Dataset: Apply Filter\n    Dataset--&gt;&gt;Report: Return Filtered Data\n    Report--&gt;&gt;User: Display Filtered View</code></pre> \ud83d\udd12 RLS Flow <p>Row-Level Security (RLS): Defined in Desktop, managed in Service to filter rows per user.</p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#additional-security-layers-best-practices","title":"\ud83d\udee1\ufe0f Additional Security Layers &amp; Best Practices","text":"Security Layer Description Authentication Azure Active Directory (AAD) identity verification Authorization Workspace roles &amp; RLS govern data access Encryption Data encrypted at rest &amp; in transit (HTTPS/TLS) Tenant Settings Admin controls for export, sharing, and sensitivity policies Sensitivity Labels Classification via Microsoft Purview (e.g., \u201cConfidential\u201d) <p>Best Practices: - Principle of least privilege - Use dynamic RLS for scalability - Enforce MFA for all users - Apply sensitivity labels to sensitive reports - Audit workspace access &amp; usage logs regularly  </p>"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#data-refresh-gateways-keeping-insights-fresh","title":"\ud83d\udd04 Data Refresh &amp; Gateways: Keeping Insights Fresh","text":"<ul> <li>Data Refresh: Updates service datasets from sources.  </li> <li>Data Gateway: Secure bridge to on-premises data (SQL Server, local files).  </li> <li>Scheduled Refresh: Pro (8/day), Premium/Fabric (48/day).</li> </ul> <pre><code>sequenceDiagram\n    participant PBIS as Power BI Service\n    participant GW as Data Gateway\n    participant ODS as On-Prem Data Source\n\n    PBIS-&gt;&gt;GW: Request Refresh\n    GW-&gt;&gt;ODS: Fetch Data\n    ODS--&gt;&gt;GW: Return Data\n    GW--&gt;&gt;PBIS: Forward Data\n    PBIS-&gt;&gt;PBIS: Update Dataset</code></pre> \ud83d\udd01 Refresh Flow"},{"location":"courses/power-bi-service/powerbi-service-cheatsheet/#other-key-service-components","title":"\u2699\ufe0f Other Key Service Components","text":"<ul> <li>Datasets: Import vs DirectQuery modes for data storage &amp; query.  </li> <li>Dataflows: Self-service ETL to prepare &amp; reuse data.  </li> <li>Datamarts: Managed Azure SQL DB for analytics between flows &amp; warehouses.  </li> <li>Deployment Pipelines: (Premium/PPU) Manage content lifecycle across dev\u2192test\u2192prod.</li> </ul> <pre><code>sequenceDiagram\n    participant RawData\n    participant Dataflows\n    participant Datasets\n    participant Datamarts\n    participant Reports\n\n    RawData-&gt;&gt;Dataflows: Prepare Data\n    Dataflows-&gt;&gt;Datasets: Load Data\n    Dataflows-&gt;&gt;Datamarts: Load Data\n    Datasets-&gt;&gt;Reports: Build Reports\n    Datamarts-&gt;&gt;Reports: Build Reports</code></pre> \ud83d\udd27 Advanced Components Flow"},{"location":"courses/power-bi-service/powerbi-service-docs/","title":"Powerbi service docs","text":""},{"location":"courses/power-bi-service/powerbi-service-docs/#mastering-power-bi-service-comprehensive-enterprise-playbook","title":"\ud83d\ude80 Mastering Power BI Service \u2013 Comprehensive Enterprise Playbook","text":""},{"location":"courses/power-bi-service/powerbi-service-docs/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Power BI Service Interface Tour</li> <li>2. Workspaces &amp; Roles Deep Dive</li> <li>3. Content Types &amp; Endorsement</li> <li>4. Sharing &amp; Distribution Options</li> <li>5. Admin Portal &amp; Tenant Settings</li> <li>6. Usage Metrics &amp; Monitoring</li> <li>7. Data Lineage &amp; Impact Analysis</li> <li>8. Dataflows &amp; Incremental Refresh</li> <li>9. Object-Level &amp; Column-Level Security</li> <li>10. Sensitivity Labels &amp; MIP</li> <li>11. REST API, CLI &amp; Automation</li> <li>12. Subscriptions &amp; Alerts</li> <li>13. Scorecards (Goals) &amp; Metrics</li> <li>14. Paginated Reports Integration</li> <li>15. Mobile App Experience</li> <li>16. Real-Time Dashboards &amp; Streaming</li> <li>17. Embedding Scenarios</li> <li>18. AI-Powered Features</li> <li>19. Microsoft Fabric &amp; OneLake</li> <li>20. Consolidated Best Practices</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#1-power-bi-service-interface-tour","title":"1. Power BI Service Interface Tour","text":"<p>The Power BI Service is your cloud-based hub for business intelligence. Its interface, while evolving,  maintains a core structure designed for intuitive navigation and efficient content management.  Understanding these key areas is the first step to mastering the service, especially for new users.  Think of it as the cockpit from which you'll pilot your data exploration and sharing efforts.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#11-home-your-personalized-bi-hub","title":"1.1 Home: Your Personalized BI Hub","text":"<p>Home is your personalized landing page. It acts as a dynamic dashboard, surfacing  frequently accessed items and recommended content tailored to your usage patterns, powered by an  underlying AI model. It's designed to get you to your most relevant content quickly.</p> <ul> <li>Quick Access cards: Pin your most critical reports or dashboards here for one-click  access. This is like creating shortcuts to your favorite tools.</li> <li>Recent: A list of artifacts (reports, dashboards, datasets, etc.) you've opened in  the last 30 days. This history spans across different browsers and the Power BI mobile app, making  it easy to pick up where you left off.</li> <li>Favorite: Items you manually \"star\" for importance. These favorites are synced  across all your devices, ensuring your key content is always at your fingertips.</li> <li>Recommended: AI-driven suggestions based on your activity and popular content  within your organization.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#12-browse-discovering-content","title":"1.2 Browse: Discovering Content","text":"<p>The Browse hub is your primary tool for finding and exploring content across the Power  BI service. When you need to locate specific reports, datasets, or other artifacts, or simply want to  see what's available, Browse is the place to go.</p> <p>You can filter content by:</p> <ul> <li>Type: Narrow down by dashboards, reports, datasets, dataflows, workbooks, etc.</li> <li>Endorsement status: Find trusted data by filtering for \"Promoted\" or \"Certified\"  content.</li> <li>Owner: Locate content created by specific individuals.</li> </ul> <p>Power users often leverage the advanced search capabilities within Browse, sometimes in conjunction with  lineage views, to trace data dependencies and discover relevant assets.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#13-create-bringing-data-to-life","title":"1.3 Create: Bringing Data to Life","text":"<p>The Create hub is your starting point for adding new content or initiating data analysis  within the Power BI Service. It offers guided experiences for various creation scenarios:</p> <ol> <li>Upload a file: The most common method for bringing in pre-built content. Supports:<ul> <li><code>.pbix</code> files (from Power BI Desktop)</li> <li><code>.xlsx</code> files (Excel workbooks, which can be connected to or imported)</li> <li><code>.rdl</code> files (Paginated Report Definition Language files for pixel-perfect  reports)</li> </ul> </li> <li>Paste or manually enter data: For quick, ad-hoc analysis, you can paste data  directly into a grid (similar to Excel) or type it in. Power BI then helps you create a dataset and  basic visuals.</li> <li>Pick a published dataset: Create new reports based on existing, trusted datasets  already published to the service. This promotes data consistency and reusability.</li> <li>Create a Data mart (Premium/Fabric feature): An advanced option that auto-generates  an Azure SQL Database, a dataset, and a dataflow, providing a managed, self-service relational data  store for analysis.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#14-left-navigation-your-constant-guide","title":"1.4 Left Navigation: Your Constant Guide","text":"<p>The persistent left navigation pane provides access to the main functional areas of Power BI Service.  Understanding these nodes is key to efficient navigation:</p> Node Description Key Use Cases &amp; Tips Home Your personalized landing page. Start your Power BI session here for quick access to relevant content. Favorites User-curated list of important items. Star critical reports/dashboards. Use keyboard shortcut <code>Shift+F</code> to favorite. Recent Chronological list of recently accessed items. Quickly return to items you were working on. Apps Curated collections of dashboards and reports, bundled for specific audiences or purposes. Ideal for distributing polished content to business users or executives. Apps provide a simplified, focused view. Shared with me Artifacts that other users have directly shared with you. Review periodically (e.g., quarterly) to remove clutter or outdated shares. Workspaces Collaborative containers for creating and managing BI content (datasets, reports, dashboards, dataflows). Use clear naming conventions (e.g., <code>FIN\u2011PRD\u2011SalesAnalytics</code>) for organization. Essential for team-based development. Deployment pipelines (Premium/Fabric) Manage the lifecycle of content (Dev, Test, Prod). Crucial for enterprise governance and ensuring quality before production release. Data hub Central place to discover, manage, and reuse data artifacts like datasets and dataflows. Promotes data discovery and helps identify endorsed/certified data. Metrics (Formerly Goals) Track key performance indicators (KPIs) and objectives through scorecards. Monitor business performance and align teams on strategic targets. 1.4 Left Navigation: Your Constant Guide Diagram <pre><code>graph TD\nsubgraph \"Navigating the Power BI Service\"\ndirection LR\nUserLogin[\"User Logs In\"] --&gt; HomePage[\"Home Page (Personalized Start)\"]\n\nHomePage --&gt; QuickAccess[\"Quick Access (Pinned)\"]\nHomePage --&gt; RecentItems[\"Recent Items\"]\nHomePage --&gt; FavoriteItems[\"Favorites\"]\nHomePage --&gt; RecommendedContent[\"Recommended (AI)\"]\n\nLeftNav[\"Left Navigation Pane\"]\nLeftNav --&gt; Nav\\_Home[\"Home\"]\nLeftNav --&gt; Nav\\_Favorites[\"Favorites\"]\nLeftNav --&gt; Nav\\_Recent[\"Recent\"]\nLeftNav --&gt; Nav\\_Apps[\"Apps (Consumed Content)\"]\nLeftNav --&gt; Nav\\_Shared[\"Shared with Me\"]\nLeftNav --&gt; Nav\\_Workspaces[\"Workspaces (Collaboration)\"]\nLeftNav --&gt; Nav\\_Pipelines[\"Deployment Pipelines\"]\nLeftNav --&gt; Nav\\_DataHub[\"Data Hub (Discover Data)\"]\nLeftNav --&gt; Nav\\_Metrics[\"Metrics (Track Goals)\"]\nLeftNav --&gt; Nav\\_Create[\"Create Hub\"]\nLeftNav --&gt; Nav\\_Browse[\"Browse Hub\"]\n\nNav\\_Create --&gt; UploadFile[\"Upload PBIX, XLSX, RDL\"]\nNav\\_Create --&gt; PasteData[\"Paste/Enter Data\"]\nNav\\_Create --&gt; FromDataset[\"From Published Dataset\"]\n\nNav\\_Browse --&gt; FilterContent[\"Filter by Type, Endorsement, etc.\"]\nFilterContent --&gt; DiscoveredArtifact[\"View Discovered Artifact\"]\n\nNav\\_Workspaces --&gt; WorkspaceContent[\"Workspace Content List (Reports, Datasets)\"]\nWorkspaceContent --&gt; ArtifactDetail[\"View/Edit Artifact Detail Pane\"]\nend\n\nstyle HomePage fill:#cce5ff,stroke:#0056b3,stroke-width:2px\nstyle LeftNav fill:#e6f2ff,stroke:#0056b3,stroke-width:2px\nstyle Nav\\_Create fill:#d4edda,stroke:#155724,stroke-width:2px\nstyle Nav\\_Browse fill:#fff3cd,stroke:#856404,stroke-width:2px\n\n**Shortcut Tip:** Press `F` in any list view (e.g., in a workspace or Browse) to\nquickly focus the filter box. Combine this with `T` to toggle between *Tile view* and\n*List view* for content display.\n\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#2-workspaces-roles-deep-dive","title":"2. Workspaces &amp; Roles Deep Dive","text":"<p>Workspaces are the cornerstone of collaboration, content organization, and security in Power BI Service.  Think of them as shared folders or team sites specifically designed for BI development and distribution.  A well-architected workspace strategy is fundamental for scaling governance without stifling agility.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#21-builtin-workspace-roles","title":"2.1 Built\u2011in Workspace Roles","text":"<p>Power BI provides four standard roles to control permissions (Create, Read, Update, Delete - CRUD  operations) within a workspace. Understanding these roles is crucial for setting up proper data  governance and collaboration structures. Assigning the correct role ensures users have the access they  need while adhering to the principle of least privilege.</p> <ul> <li>Admin:<ul> <li>Full control over the workspace.</li> <li>Can add/remove users (including other Admins), update/delete the workspace itself.</li> <li>Can publish, update, and delete any content.</li> <li>Manages workspace settings, including premium capacity assignment.</li> <li>Can configure and manage deployment pipelines for the workspace.</li> </ul> </li> <li>Member:<ul> <li>Can publish, update, and delete their own content and content published by others.</li> <li>Can add users with Contributor or Viewer roles (if allowed by workspace settings).</li> <li>Can create and manage Apps from the workspace content.</li> <li>Can share items within the workspace.</li> <li>Cannot delete the workspace or manage Admins.</li> </ul> </li> <li>Contributor:<ul> <li>Can publish and update content (reports, datasets).</li> <li>Can create, edit, and delete content within the workspace.</li> <li>Cannot modify workspace settings, add/remove users, or publish/update Apps.</li> <li>Typically, this role is for developers who need to build and manage BI assets.</li> </ul> </li> <li>Viewer:<ul> <li>Read\u2011only access to content within the workspace.</li> <li>Can view reports and dashboards, and interact with them (e.g., use slicers, filters).</li> <li>Can export data from visuals (if allowed by report settings and tenant settings).</li> <li>Cannot publish, edit, or delete content.</li> <li>This role is for consumers of information who don't need development access.</li> </ul> </li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#22-advanced-role-patterns-using-azure-ad-groups","title":"2.2 Advanced Role Patterns using Azure AD Groups","text":"<p>For larger enterprises, managing individual user access to numerous workspaces can become cumbersome. A  best practice is to leverage Azure Active Directory (Azure AD) security groups. Instead of assigning  individual users to workspace roles, you assign Azure AD groups. This approach simplifies user  management, especially in large organizations, as permissions are managed at the Azure AD group level  (by IT or group owners) rather than individually in Power BI by workspace admins.</p> <p>Example structure:</p> <ul> <li>Azure AD Group: <code>PBI_Sales_Admins</code> \u2192 Assigned to Workspace Admin role.</li> <li>Azure AD Group: <code>PBI_Sales_Members</code> \u2192 Assigned to Workspace Member role.</li> <li>Azure AD Group: <code>PBI_Sales_ContentAuthors</code> \u2192 Assigned to Workspace  Contributor role.</li> <li>Azure AD Group: <code>PBI_Sales_ReportViewers</code> \u2192 Assigned to Workspace Viewer  role.</li> </ul> 2.2 Advanced Role Patterns using Azure AD Groups Diagram <pre><code>graph TD\nsubgraph \"Workspace Access via Azure AD Groups\"\ndirection LR\nAAD\\_Admin[\"Azure AD Group: 'PBI\\_Finance\\_Admins'\"] -- \"Assigned To\" --&gt; Role\\_Admin[\"Workspace Admin\nRole\"]\nAAD\\_Member[\"Azure AD Group: 'PBI\\_Finance\\_Collaborators'\"] -- \"Assigned To\" --&gt; Role\\_Member[\"Workspace\nMember Role\"]\nAAD\\_Contrib[\"Azure AD Group: 'PBI\\_Finance\\_Publishers'\"] -- \"Assigned To\" --&gt; Role\\_Contrib[\"Workspace\nContributor Role\"]\nAAD\\_Viewer[\"Azure AD Group: 'PBI\\_Finance\\_Consumers'\"] -- \"Assigned To\" --&gt; Role\\_Viewer[\"Workspace Viewer\nRole\"]\n\nRole\\_Admin -- \"Manages &amp; Governs\" --&gt; FinanceWS[\"Finance Analytics Workspace\"]\nRole\\_Member -- \"Full Content Collaboration In\" --&gt; FinanceWS\nRole\\_Contrib -- \"Publishes &amp; Edits Content In\" --&gt; FinanceWS\nRole\\_Viewer -- \"Views &amp; Interacts With Content In\" --&gt; FinanceWS\nend\nstyle FinanceWS fill:#d1ecf1,stroke:#0c5460,stroke-width:2px\nstyle AAD\\_Admin fill:#fff3cd,stroke:#856404\nstyle AAD\\_Member fill:#fff3cd,stroke:#856404\nstyle AAD\\_Contrib fill:#fff3cd,stroke:#856404\nstyle AAD\\_Viewer fill:#fff3cd,stroke:#856404\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#23-workspace-naming-conventions-lifecycle-management","title":"2.3 Workspace Naming Conventions &amp; Lifecycle Management","text":"<p>A consistent naming convention for workspaces is vital for organization and discoverability. A common  pattern is <code>BusinessUnit\u2011Environment\u2011Purpose</code> or <code>Team\u2011Project\u2011Status</code>.</p> <p>Example: <code>HR\u2011DEV\u2011RecruitmentAnalytics</code>, <code>SALES\u2011PROD\u2011QuarterlyReporting</code>,  <code>MKTG\u2011UAT\u2011CampaignAnalysis</code>.</p> <p>Align workspaces with Deployment Pipelines (a Premium/Fabric feature) to manage the  content lifecycle from development to production. This typically involves a 1-to-1 mapping of workspaces  to pipeline stages:</p> 2.3 Workspace Naming Conventions &amp; Lifecycle Management Diagram <pre><code>graph LR\nsubgraph \"Deployment Pipeline Stages\"\nDEV[\"Development Workspace (e.g., FIN-DEV-DataMarts)\"] -- \"Deploy To\" --&gt; TEST[\"Test/UAT Workspace\n(e.g., FIN-TEST-DataMarts)\"]\nTEST -- \"Deploy To\" --&gt; PROD[\"Production Workspace (e.g., FIN-PROD-DataMarts)\"]\nend\nsubgraph \"Activities in Stages\"\nDEV\\_Activity[\"Content Creation &amp; Unit Testing\"]\nTEST\\_Activity[\"User Acceptance Testing &amp; Integration Testing\"]\nPROD\\_Activity[\"Live Content for Business Users\"]\nend\nDEV --&gt; DEV\\_Activity\nTEST --&gt; TEST\\_Activity\nPROD --&gt; PROD\\_Activity\n\nstyle DEV fill:#fff3cd,stroke:#856404,stroke-width:2px,color:#000\nstyle TEST fill:#d1ecf1,stroke:#0c5460,stroke-width:2px,color:#000\nstyle PROD fill:#d4edda,stroke:#155724,stroke-width:2px,color:#000\n\n**Automation Tip:** Use the Power BI REST API (e.g., `POST /groups` to create\nworkspaces) or PowerShell cmdlets/CLI to automate workspace creation and configuration as part of your\nDevOps or MLOps pipelines. When automating, ensure you tag owner metadata and other relevant information\nusing the *workspace settings \u2192 Advanced \u2192 Contact list* or workspace description.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#3-content-types-certification-endorsement","title":"3. Content Types, Certification &amp; Endorsement","text":"<p>The Power BI ecosystem supports a diverse range of content types (artifacts) beyond just traditional  datasets and reports. Understanding these types and how to govern them through endorsement is key to  building a trusted and scalable BI environment.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#31-artifact-catalog-understanding-power-bi-content","title":"3.1 Artifact Catalog: Understanding Power BI Content","text":"<p>Here's a breakdown of common artifact types and their roles:</p> <ul> <li>Datasets: The semantic data models that power your reports and dashboards. They  define relationships, measures (DAX calculations), and hierarchies. Think of them as curated,  analysis-ready data packages. Datasets can use various storage modes (Import, DirectQuery,  Composite, Direct Lake).</li> <li>Reports: Interactive collections of visuals (charts, tables, maps) that display  data from one or more datasets. Users explore data and gain insights through reports. These are  typically created in Power BI Desktop and published to the service.</li> <li>Dashboards: Single-page canvases that provide a high-level, consolidated view of  key metrics and visuals, often by pinning tiles from multiple reports. Dashboards are designed for  monitoring and quick insights.</li> <li>Dataflows (Gen1 &amp; Gen2): Reusable Power Query ETL (Extract, Transform, Load)  processes executed in the Power BI service. They allow you to ingest, clean, transform, and prepare  data, storing the results in Azure Data Lake Storage (OneLake for Gen2). Prepared data can then be  used by multiple datasets.<ul> <li>Gen1 Dataflows: The original dataflows, primarily for Power Query transformations.</li> <li>Gen2 Dataflows (Fabric): Enhanced dataflows with more capabilities, outputting to a  Lakehouse or Warehouse, offering better performance and integration within the Fabric  ecosystem.</li> </ul> </li> <li>Data marts (Premium/Fabric): A self-service solution that combines a dataflow, an  Azure SQL Database (for relational storage of the prepared data), and an auto-generated dataset.  This simplifies the creation of relational data warehouses for specific business needs without  requiring extensive IT involvement.</li> <li>Metrics (Scorecards/Goals): Used to define, track, and share progress on key  business objectives and Key Performance Indicators (KPIs). Metrics are organized into Scorecards.</li> <li>Paginated Reports (.RDL): Designed for pixel-perfect, fixed-layout reports  optimized for printing or PDF generation (e.g., invoices, financial statements, operational  reports).</li> <li>Workbooks (Excel): Excel files can be uploaded and viewed within Power BI, or  connected to as data sources.</li> <li>Notebooks (Fabric): For data scientists and engineers using Spark or Python within  the Microsoft Fabric environment for advanced data preparation, exploration, and machine learning  model development.</li> </ul> 3.1 Artifact Catalog: Understanding Power BI Content Diagram <pre><code>graph TD\nsubgraph \"Power BI Content Ecosystem\"\nDataSources[\"External Data Sources (DBs, Files, APIs)\"] --&gt; Dataflows[\"Dataflows (ETL &amp; Data Prep)\"]\nDataflows --&gt; Datasets[\"Datasets (Semantic Models)\"]\nDataSources --&gt; Datasets\n\nDataMart[\"Data Marts (SQL DB + Dataset)\"]\nDataflows --&gt; DataMart\nDataSources --&gt; DataMart\n\nDatasets --&gt; Reports[\"Reports (Interactive Visuals)\"]\nDatasets --&gt; PaginatedReports[\"Paginated Reports (Pixel-Perfect)\"]\n\nReports --&gt; Dashboards[\"Dashboards (Monitoring Tiles)\"]\nPaginatedReports ---&gt; Dashboards\n\nMetrics[\"Metrics/Goals\"] -- \"Organized In\" --&gt; Scorecards[\"Scorecards\"]\nScorecards -- \"Can Link Data From\" --&gt; Reports\n\nWorkbooks[\"Excel Workbooks\"] -- \"Can Be Data Source For\" --&gt; Datasets\nWorkbooks -- \"Viewable In\" --&gt; PBI\\_Service[\"Power BI Service\"]\n\nNotebooks\\_Fabric[\"Notebooks (Fabric - Spark/Python)\"] -- \"Process Data For\" --&gt; Dataflows\nNotebooks\\_Fabric -- \"Process Data For\" --&gt; Datasets\nNotebooks\\_Fabric -- \"Process Data For\" --&gt; Lakehouse\\_WH[\"Fabric Lakehouse/Warehouse\"]\nLakehouse\\_WH --&gt; Datasets\nend\n\nstyle Datasets fill:#cde4ff,stroke:#0056b3,stroke-width:2px\nstyle Reports fill:#d4edda,stroke:#155724,stroke-width:2px\nstyle Dashboards fill:#fff3cd,stroke:#856404,stroke-width:2px\nstyle Dataflows fill:#d1ecf1,stroke:#0c5460,stroke-width:2px\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#32-endorsement-workflow-building-trust-in-your-data","title":"3.2 Endorsement Workflow: Building Trust in Your Data","text":"<p>Power BI's endorsement feature helps users identify reliable and high-quality content. It establishes a  clear signal of trust. A typical two-tier model is adopted:</p> <ol> <li>Promoted:<ul> <li>Content authors or owners can mark their artifacts (datasets, reports, apps, dataflows) as  \"Promoted.\"</li> <li>This indicates that the content is considered valuable, quality-checked by its creator/team,  and ready for wider use.</li> <li>It's a self-service endorsement, signifying a good starting point for users.</li> </ul> </li> <li>Certified:<ul> <li>This is the highest level of endorsement, typically governed by a central data governance  board, IT department, or a BI Center of Excellence (CoE).</li> <li>Only authorized personnel can certify content.</li> <li>Certification implies that the artifact meets stringent organizational standards for  accuracy, reliability, refresh frequency, documentation, and support. It often requires a  documented Service Level Agreement (SLA).</li> <li>Certified datasets are often considered the \"single source of truth\" for specific data  domains.</li> </ul> </li> </ol> <p>Endorsed content appears more prominently in search results and discovery areas like the Data Hub.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#33-certification-checklist-criteria-for-top-tier-content","title":"3.3 Certification Checklist: Criteria for Top-Tier Content","text":"<p>To achieve \"Certified\" status, artifacts usually need to meet a rigorous checklist. This ensures  consistency and quality. Examples include:</p> <ul> <li>Data Source Validation: Verified and approved data sources.</li> <li>Lineage Documentation: Clear documentation of data sources and transformations,  often stored in a shared location like the workspace's OneDrive or SharePoint site.</li> <li>Refresh Reliability: Refresh schedules must use robust authentication methods like  Managed Identity or credentials stored securely in Azure Key  Vault, rather than personal credentials.</li> <li>Security Implementation: Appropriate Row-Level Security (RLS), Object-Level  Security (OLS), or Column-Level Security (CLS) must be implemented if sensitive data is present.</li> <li>Performance Optimization: Dataset and report performance should be optimized (e.g.,  efficient DAX, query folding in dataflows).</li> <li>Clear Ownership &amp; Support: Defined owners and support contacts.</li> <li>CI/CD Practices (for datasets/reports): If applicable, Power BI Project  (<code>.pbip</code>) files or Tabular Model (<code>.bim</code>) files should be committed to a Git  repository for version control and automated deployment.</li> <li>Documentation: Comprehensive documentation of measures, columns, relationships, and  business logic.</li> </ul> 3.3 Certification Checklist: Criteria for Top-Tier Content Diagram <pre><code>graph TD\nsubgraph \"Content Endorsement Journey\"\nCreator[\"Content Creator/Team\"] -- \"Develops &amp; Validates\" --&gt; Artifact[\"Dataset, Report, App, Dataflow\"]\n\nArtifact -- \"Self-Attests Quality\" --&gt; PromoteAction{\"Promote Content?\"}\nPromoteAction -- \"Yes\" --&gt; PromotedArtifact[\"Artifact Marked 'Promoted' \u2b50\"]\n\nPromotedArtifact -- \"Submit for Central Review\" --&gt; GovernanceReview[\"Data Governance Board / CoE\nReview\"]\nGovernanceReview -- \"Meets Org Standards &amp; Checklist?\" --&gt; CertifyAction{\"Certify Content?\"}\nCertifyAction -- \"Yes\" --&gt; CertifiedArtifact[\"Artifact Marked 'Certified' \ud83d\udd12 (Single Source of Truth)\"]\nCertifyAction -- \"No\" --&gt; FeedbackLoop[\"Feedback to Creator for Improvement\"]\nFeedbackLoop --&gt; Creator\n\nPromotedArtifact --&gt; Discovery[\"Easier Discovery for Users\"]\nCertifiedArtifact --&gt; HighTrustDiscovery[\"Highest Trust &amp; Visibility for Users\"]\nend\nstyle PromotedArtifact fill:#e6ffed,stroke:#28a745\nstyle CertifiedArtifact fill:#d4edda,stroke:#155724,stroke-width:3px\nstyle GovernanceReview fill:#fff3cd,stroke:#856404\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#4-sharing-distribution-options","title":"4. Sharing &amp; Distribution Options","text":"<p>Power BI offers a versatile suite of sharing and distribution mechanisms, each tailored to different  audience needs, collaboration styles, and governance requirements. Choosing the right method is crucial  for effective information dissemination.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#41-direct-share-link-based-or-user-specific","title":"4.1 Direct Share (Link-Based or User-Specific)","text":"<p>The simplest method for sharing individual reports or dashboards. You select Share on an  artifact and can then:</p> <ul> <li>Share with specific people or groups: Enter email addresses or Azure AD group  names. You can control if they can reshare or build content with the underlying data.</li> <li>Generate a shareable link: Create links that work for \"People in your  organization,\" \"People with existing access,\" or \"Specific people.\"</li> </ul> <p>Pros: Quick, intuitive for ad-hoc sharing.</p> <p>Cons: Can lead to \"sharing sprawl\" if overused, making it hard to manage permissions  centrally. Lacks versioning control for the shared view (users always see the latest). No custom  branding or navigation for the consumer.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#42-workspace-access-collaboration-focused","title":"4.2 Workspace Access (Collaboration-Focused)","text":"<p>Granting users access to a workspace by assigning them a role (Admin, Member, Contributor, Viewer). This  is primarily for collaboration among content creators and managers.</p> <p>Pros: Enables team-based development and management of BI assets. Version control is  inherent as users work on the same artifacts.</p> <p>Cons: Not ideal for distributing content to a wide audience of read-only consumers, as  it gives them access to the entire workspace environment. Viewers in a workspace see all content,  including work-in-progress items.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#43-power-bi-apps-broad-distribution-consumption","title":"4.3 Power BI Apps (Broad Distribution &amp; Consumption)","text":"<p>Apps are the recommended way to distribute finalized content to a broad audience of business users. An  App bundles content (reports, dashboards, links) from one or more workspaces into a polished,  consumer-friendly package.</p> <ul> <li>Custom Navigation &amp; Branding: Define a navigation structure for the app, add  branding (logo, theme color).</li> <li>Audience Segmentation: (With Premium/Fabric) Create multiple audience groups within  a single app publication, showing different content to different user groups.</li> <li>Controlled Updates: Publish updates to the app without changing the URL or access  for users. Users get a notification about the update.</li> <li>Native Usage Analytics: Apps come with built-in reports on their usage.</li> </ul> <p>Pros: Excellent for broad, controlled distribution. Provides a better user experience  for consumers. Centralized management of what users see.</p> <p>Cons: Requires publishing step; not for real-time co-editing.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#44-embedding-integrating-bi-into-other-applications","title":"4.4 Embedding (Integrating BI into Other Applications)","text":"<p>Integrate Power BI reports, dashboards, or tiles directly into other web applications, portals, or  websites. Two main scenarios:</p> <ul> <li>Embed for your organization (Secure Embed / User-Owns-Data): For internal users.  Typically uses an iframe and Azure AD authentication (SSO). Users need Power BI licenses (Pro/PPU or  content in Premium/Fabric). Ideal for SharePoint, Teams, or custom internal portals.</li> <li>Embed for your customers (App-Owns-Data): For external users of a SaaS application.  The application authenticates users and uses a service principal or master user account to embed  Power BI content. External users don't need Power BI licenses; the organization pays for Power BI  Embedded capacity (A SKUs) or Fabric capacity (F SKUs).</li> </ul> <p>Pros: Seamlessly integrates analytics into user workflows. Highly customizable user  experience.</p> <p>Cons: Requires development effort (especially App-Owns-Data). Licensing and capacity  planning are critical.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#45-publish-to-web-public-unauthenticated-access","title":"4.5 Publish to Web (Public, Unauthenticated Access)","text":"<p>Generates an embed code that makes a report publicly accessible on the internet to anyone with the link.  This feature should be used with extreme caution and is often disabled by default at the tenant  level by administrators due to security risks.</p> <p>Pros: Useful for sharing non-sensitive data with a very broad, anonymous audience (e.g.,  public dashboards on a government website, embedded charts in a public blog post).</p> <p>Cons: Data is public and not secure. No authentication. Potential for data exposure if  used inappropriately. Requires careful governance and explicit tenant-level enablement.</p> 4.5 Publish to Web (Public, Unauthenticated Access) Diagram <pre><code>graph TD\nsubgraph \"Power BI Sharing &amp; Distribution Methods\"\nContent[\"Report or Dashboard Artifact\"] --&gt; DirectShare[\"Direct Share (Specific Users/Links)\"]\nContent --&gt; WorkspaceAccess[\"Workspace Role Assignment (Collaboration)\"]\nContent --&gt; AppPublish[\"Publish as Power BI App (Broad Consumption)\"]\nContent --&gt; Embedding[\"Embedding (In other Apps/Portals)\"]\nContent --&gt; PublishToWeb[\"Publish to Web (Public/Unauthenticated)\"]\n\nDirectShare --&gt; UserA[\"User A (Specific Access)\"]\nWorkspaceAccess --&gt; DeveloperTeam[\"Developer Team (Collaborative Access)\"]\nAppPublish --&gt; BusinessUsers[\"Business Users (Curated View)\"]\nEmbedding --&gt; InternalAppUsers[\"Internal App Users (Secure Embed)\"]\nEmbedding --&gt; ExternalSaaSUsers[\"External SaaS Users (App-Owns-Data)\"]\nPublishToWeb --&gt; GeneralPublic[\"General Public (Anonymous Access)\"]\nend\nstyle Content fill:#cde4ff,stroke:#0056b3\nstyle PublishToWeb fill:#ffdddd,stroke:#dc3545,stroke-width:2px\nstyle AppPublish fill:#d4edda,stroke:#155724,stroke-width:2px\n\n**License and Capacity Considerations:**\n\n\n\n</code></pre> Method Typical Viewer License Capacity Needed by Publisher/Org Key Use Case Direct Share Power BI Pro / PPU (or Free if content in Premium/Fabric) Shared (default) or Premium/Fabric (for performance/scale) Ad-hoc sharing with individuals/small groups. Workspace Access (Viewer) Power BI Pro / PPU (or Free if content in Premium/Fabric) Shared or Premium/Fabric Internal team members needing to see work-in-progress or all content. Power BI App Power BI Pro / PPU (or Free if content in Premium/Fabric and app audience has access) Workspace can be in Shared, but Premium/Fabric recommended for large scale distribution and advanced app features. Broad distribution of finalized content to business users. Embed for your organization (Secure Embed) Power BI Pro / PPU Content must be in a workspace in Premium/Fabric capacity, or user needs Pro/PPU. Internal portals, SharePoint, Teams. Embed for your customers (App-Owns-Data) None for end-users Requires Power BI Embedded (A SKU) or Fabric (F SKU) capacity. ISVs embedding BI in their SaaS apps for external customers. Publish to Web None for end-users Shared or Premium/Fabric Publicly sharing non-sensitive data. (Use with extreme caution!)"},{"location":"courses/power-bi-service/powerbi-service-docs/#5-admin-portal-tenant-settings","title":"5. Admin Portal &amp; Tenant Settings","text":"<p>The Power BI Admin Portal is the central command center for users assigned the Power BI Service  Administrator role (or Microsoft 365 Global Administrator role). This portal provides tools  and settings to govern, monitor, and manage the entire Power BI tenant. It's crucial for maintaining a  secure, efficient, and compliant BI environment.</p> <p>The portal is typically organized into several key blades or sections:</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#51-tenant-settings-the-governance-rulebook","title":"5.1 Tenant Settings: The Governance Rulebook","text":"<p>This is arguably the most critical section. It contains a multitude of toggles (over 130+) that control  various features and functionalities across the entire Power BI tenant. These settings can often be  applied to the entire organization or scoped to specific Azure AD security groups (allowing for  exceptions or phased rollouts).</p> <p>Key categories of tenant settings include:</p> <ul> <li>Export and sharing settings: Control who can share content externally, publish to  web, export data to Excel/CSV, print, etc. (e.g., \"Share content with external users,\" \"Publish to  web,\" \"Export data\").</li> <li>Content pack and app settings: Manage creation and use of organizational content  packs and apps.</li> <li>Integration settings: Configure integrations with services like SharePoint, Teams,  PowerPoint, Azure Maps, ArcGIS Maps.</li> <li>R and Python visual settings: Enable or disable the use of R and Python visuals.</li> <li>Audit and usage settings: Enable creation of audit logs for Power BI activities.</li> <li>Dashboard settings: Control features like data classification for dashboards.</li> <li>Developer settings: Manage embedding capabilities, service principal access, and  API usage.</li> <li>Workspace settings: Control who can create workspaces.</li> <li>Information protection settings: Manage sensitivity labels, default labeling, and  mandatory labeling.</li> <li>AI features settings: Enable/disable features like Q&amp;A, Quick Insights, AI visuals,  Text/Vision AI functions in Premium/Fabric.</li> <li>Fabric settings: Control Fabric-specific features like enabling Fabric for users,  OneLake access, etc.</li> </ul> <p>Default Stance &amp; Best Practice: For new features, a cautious approach is often best:  disable them by default for the entire organization, pilot them with a specific security group (e.g., a  BI CoE or early adopters), gather feedback, and then plan a wider rollout if deemed beneficial and  stable.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#52-usage-metrics-tenant-level","title":"5.2 Usage Metrics (Tenant-Level)","text":"<p>Provides high-level adoption charts and dashboards summarizing Power BI usage across the tenant. This can  show trends in active users, popular reports/dashboards, and workspace activity. While useful for a  quick overview, for granular, auditable logs, administrators should rely on Audit logs (via  Microsoft Purview compliance portal) or Activity Logs (via Power BI REST API or PowerShell).</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#53-capacity-settings-premiumfabric-management","title":"5.3 Capacity Settings (Premium/Fabric Management)","text":"<p>If your organization uses Power BI Premium (P, EM, A SKUs) or Fabric (F SKUs) capacities, this section is  where administrators manage them:</p> <ul> <li>Assign workspaces to capacities: Link specific workspaces to a purchased capacity  to leverage its dedicated resources.</li> <li>Configure capacity workloads: Allocate resources within a capacity to different  workloads (e.g., Datasets, Dataflows, Paginated Reports, AI).</li> <li>Set up autoscale (for Premium Gen2 &amp; Fabric): Configure automatic scaling of  capacity resources based on demand.</li> <li>Monitor capacity utilization: View metrics on memory, CPU usage, query performance,  and potential throttling events. The Fabric Capacity Metrics app provides detailed insights.</li> <li>Manage capacity administrators.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#54-embed-codes","title":"5.4 Embed Codes","text":"<p>Allows administrators to review and manage active \"Publish to Web\" embed codes generated within the  tenant. If a public embed code needs to be revoked (e.g., due to data sensitivity concerns or accidental  publishing), administrators can delete it here, which will disable the public link.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#55-organizational-visuals","title":"5.5 Organizational Visuals","text":"<p>Administrators can manage custom visuals (<code>.pbiviz</code> files) for the entire organization. This  involves:</p> <ul> <li>Adding and certifying organizational visuals: Upload custom visuals developed  in-house or by trusted third parties. Certified visuals are vetted by the admin.</li> <li>Controlling access: Ensure that only approved and vetted custom visuals are  available for use by report creators in the organization, enhancing security and consistency.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#56-protection-metrics-information-protection","title":"5.6 Protection Metrics (Information Protection)","text":"<p>Provides insights into how Microsoft Information Protection (MIP) sensitivity labels are being applied  and used across Power BI content (datasets, reports, dashboards, dataflows). Administrators can track:</p> <ul> <li>The distribution of content by sensitivity label (e.g., how many items are \"Confidential,\"  \"General,\" etc.).</li> <li>Trends in labeling activity.</li> <li>This helps monitor compliance with data protection policies and understand the sensitivity landscape  of the BI environment.</li> </ul> 5.6 Protection Metrics (Information Protection) Diagram <pre><code>graph TD\nsubgraph \"Power BI Admin Portal - Key Sections\"\nAdminUser[\"Power BI Admin / Global Admin\"] --&gt; AdminPortal[\"Admin Portal Interface\"]\n\nAdminPortal --&gt; TenantSettings[\"Tenant Settings (Global Rules)\"]\nTenantSettings --&gt; ExportShare[\"Export &amp; Sharing Controls\"]\nTenantSettings --&gt; InfoProtect[\"Information Protection (Labels)\"]\nTenantSettings --&gt; AIFeatures[\"AI Feature Enablement\"]\nTenantSettings --&gt; FabricControls[\"Fabric Specific Settings\"]\nTenantSettings --&gt; DevSettings[\"Developer &amp; Embedding Controls\"]\n\nAdminPortal --&gt; CapacityMgmt[\"Capacity Settings (Premium/Fabric)\"]\nCapacityMgmt --&gt; AssignWorkspaces[\"Assign Workspaces to Capacity\"]\nCapacityMgmt --&gt; MonitorUsage[\"Monitor Capacity Health &amp; Utilization\"]\nCapacityMgmt --&gt; AutoscaleConfig[\"Configure Autoscale\"]\n\nAdminPortal --&gt; UsageMetricsTenant[\"Usage Metrics (Tenant Overview)\"]\nAdminPortal --&gt; EmbedCodesMgmt[\"Embed Codes (Publish to Web Review)\"]\nAdminPortal --&gt; OrgVisuals[\"Organizational Visuals (Custom Visuals Mgmt)\"]\nAdminPortal --&gt; ProtectionMetrics[\"Protection Metrics (Label Usage)\"]\nAdminPortal --&gt; AuditLogsLink[\"Link to Audit Logs (Purview)\"]\nend\nstyle TenantSettings fill:#cce5ff,stroke:#0056b3,stroke-width:2px\nstyle CapacityMgmt fill:#d1ecf1,stroke:#0c5460,stroke-width:2px\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#6-usage-metrics-monitoring","title":"6. Usage Metrics &amp; Monitoring","text":"<p>Effective monitoring of Power BI usage, performance, and activities is crucial for governance, cost  management, performance optimization, and understanding user adoption. Power BI provides several tools  and methods for this.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#61-builtin-usage-metrics-per-reportdashboardapp","title":"6.1 Built\u2011in Usage Metrics (Per Report/Dashboard/App)","text":"<p>Individual reports, dashboards, and Power BI Apps come with their own built-in usage metrics reports.  These provide insights specific to that particular item:</p> <ul> <li>Report Usage Metrics: Shows views, unique viewers, views per user, average time  spent, sharing activity, and performance trends for a specific report. You can often \"Save a copy\"  of this usage report to customize it further (e.g., add slicers, change visuals).</li> <li>Dashboard Usage Metrics: Similar insights for dashboards.</li> <li>App Usage Metrics: Provides an overview of how users are interacting with a  published Power BI App, including active users and usage by report within the app.</li> </ul> <p>These are useful for content creators and workspace owners to understand the popularity and impact of  their specific artifacts.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#62-activity-logs-tenant-wide-auditing","title":"6.2 Activity Logs (Tenant-Wide Auditing)","text":"<p>Power BI Activity Logs provide a comprehensive audit trail of user and admin activities across the entire  tenant. These logs are invaluable for security audits, compliance reporting, and understanding detailed  usage patterns. They capture a wide range of event types (currently over 32, and growing), including:</p> <ul> <li><code>ViewReport</code>, <code>ViewDashboard</code>, <code>ViewTile</code></li> <li><code>CreateReport</code>, <code>CreateDashboard</code>, <code>CreateDataset</code>,  <code>CreateDataflow</code></li> <li><code>UpdateReport</code>, <code>DeleteDataset</code></li> <li><code>ExportReport</code>, <code>ExportData</code> (exporting data from a visual)</li> <li><code>ShareReport</code>, <code>PublishToWebReport</code></li> <li><code>AnalyzedByExcel</code> (Analyze in Excel usage)</li> <li>And many more admin activities like changes to tenant settings.</li> </ul> <p>Activity Logs can be accessed via:</p> <ul> <li>Power BI REST API: <code>GET https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime=[...]&amp;endDateTime=[...]</code>.  This allows programmatic retrieval and storage of logs, often into a data lake or monitoring  database for long-term analysis.</li> <li>PowerShell Cmdlets: Using the <code>Get-PowerBIActivityEvent</code> cmdlet.</li> <li>Microsoft Purview Compliance Portal: Audit logs from Power BI are also surfaced  here, allowing for unified auditing across Microsoft 365 services.</li> </ul> <p>Best Practice: Regularly extract and store Activity Logs (they are typically retained  for 30-90 days by default in the service, depending on license) into a durable storage solution (e.g.,  Azure Data Lake Storage) and build a dedicated monitoring solution (e.g., a Power BI report) on top of  these logs for deeper analysis and trend identification.</p> 6.2 Activity Logs (Tenant-Wide Auditing) Diagram <pre><code>graph TD\nsubgraph \"Power BI Activity Logging &amp; Monitoring Workflow\"\nPBI\\_Service[\"Power BI Service (User &amp; Admin Activities)\"] -- \"Generates Events\" --&gt;\nActivityLogStream[\"Internal Activity Log Stream\"]\n\nActivityLogStream -- \"Accessible Via\" --&gt; API\\_Access[\"Power BI REST API (activityevents endpoint)\"]\nActivityLogStream -- \"Accessible Via\" --&gt; PowerShell\\_Access[\"PowerShell (Get-PowerBIActivityEvent)\"]\nActivityLogStream -- \"Surfaced In\" --&gt; PurviewAudit[\"Microsoft Purview Audit Log\"]\n\nAPI\\_Access --&gt; CustomSolution[\"Custom ETL Process (e.g., Azure Data Factory, Logic Apps)\"]\nPowerShell\\_Access --&gt; CustomSolution\n\nCustomSolution -- \"Stores Logs In\" --&gt; DataLake[\"Azure Data Lake Storage (Long-term Retention)\"]\nDataLake -- \"Data Source For\" --&gt; MonitoringDataset[\"Power BI Monitoring Dataset\"]\nMonitoringDataset -- \"Powers\" --&gt; AdminDashboards[\"Admin Monitoring Dashboards &amp; Reports\"]\n\nAdminDashboards -- \"Provides Insights On\" --&gt; UsageTrends[\"Usage Trends\"]\nAdminDashboards -- \"Provides Insights On\" --&gt; SecurityAudits[\"Security &amp; Compliance Audits\"]\nAdminDashboards -- \"Provides Insights On\" --&gt; AdoptionTracking[\"Adoption Tracking\"]\nend\nstyle PBI\\_Service fill:#cce5ff,stroke:#0056b3\nstyle DataLake fill:#d1ecf1,stroke:#0c5460\nstyle AdminDashboards fill:#d4edda,stroke:#155724\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#63-fabric-capacity-metrics-app","title":"6.3 Fabric Capacity Metrics App","text":"<p>For organizations utilizing Power BI Premium Gen2 or Microsoft Fabric capacities, Microsoft provides a  dedicated Capacity Metrics App. This app, once deployed from AppSource, connects to your capacity's  telemetry data and provides detailed dashboards and reports on:</p> <ul> <li>CPU and Memory Utilization: Track overall capacity load and identify peak times or  resource-intensive operations.</li> <li>Query Performance: Analyze query durations, wait times, and success rates.</li> <li>Dataset Refresh Performance: Monitor refresh success, duration, and resource  consumption.</li> <li>Throttling and Overload Events: Identify if and when your capacity is experiencing  overload, leading to request throttling or interactive query delays.</li> <li>Workload Management: See how resources are distributed across different workloads  (Datasets, Dataflows, Paginated Reports, AI).</li> <li>Autoscale Activity: If autoscale is enabled, monitor when it triggers and the  impact.</li> </ul> <p>This app is essential for capacity administrators to understand performance, optimize resource  allocation, and plan for future capacity needs. It's common to configure notifications based on metrics  from this app (e.g., alert if average CPU &gt; 80% for an extended period).</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#7-data-lineage-impact-analysis","title":"7. Data Lineage &amp; Impact Analysis","text":"<p>As BI environments grow and datasets are reused across multiple reports and dashboards, understanding the  flow of data and the dependencies between artifacts becomes critical. Data lineage helps visualize these  connections, while impact analysis helps assess the potential consequences of changes to upstream  components.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#71-lineage-view-visualizing-data-flow","title":"7.1 Lineage View: Visualizing Data Flow","text":"<p>Power BI Service provides a Lineage View that offers a visual graph of the relationships between  artifacts within a workspace and even across workspaces (for some connections like shared datasets).</p> <p>It typically shows the flow from:</p> <ul> <li>Data Sources: (e.g., SQL Server, Excel files, APIs) - though full external source  lineage might be limited in the UI itself.</li> <li>\u2192 Dataflows: If used for data preparation.</li> <li>\u2192 Datasets: The core semantic models.</li> <li>\u2192 Reports: Visualizations built on datasets.</li> <li>\u2192 Dashboards: Tiles pinned from reports.</li> <li>\u2192 Apps: Bundles containing these reports/dashboards.</li> </ul> <p>This view helps users understand:</p> <ul> <li>Where a dataset gets its data from.</li> <li>Which reports and dashboards depend on a specific dataset.</li> <li>The overall complexity of data flows within their BI solutions.</li> </ul> 7.1 Lineage View: Visualizing Data Flow Diagram <pre><code>graph LR\nsubgraph \"Power BI Lineage View Example\"\nExternalSource[\"External Source (e.g., SQL DB)\"] --&gt; DF1[\"Dataflow: StagingSales\"]\nDF1 --&gt; DS\\_Sales[\"Dataset: SalesAnalytics (Certified)\"]\n\nAnotherExternalSource[\"Source: CRM API\"] --&gt; DF2[\"Dataflow: CustomerDim\"]\nDF2 --&gt; DS\\_Sales\nDF2 --&gt; DS\\_Customer[\"Dataset: CustomerDetails\"]\n\nDS\\_Sales --&gt; R\\_SalesSummary[\"Report: Sales Summary QTD\"]\nDS\\_Sales --&gt; R\\_ProductPerf[\"Report: Product Performance Deep Dive\"]\nDS\\_Customer --&gt; R\\_CustomerSeg[\"Report: Customer Segmentation\"]\n\nR\\_SalesSummary -- \"Tile Pinned To\" --&gt; DB\\_ExecSales[\"Dashboard: Executive Sales Overview\"]\nR\\_ProductPerf -- \"Tile Pinned To\" --&gt; DB\\_ExecSales\nR\\_CustomerSeg -- \"Tile Pinned To\" --&gt; DB\\_Marketing[\"Dashboard: Marketing KPIs\"]\n\nDB\\_ExecSales -- \"Included In\" --&gt; App\\_Sales[\"App: Sales Insights\"]\nDB\\_Marketing -- \"Included In\" --&gt; App\\_Sales\nR\\_ProductPerf -- \"Included In\" --&gt; App\\_Sales\nend\nstyle ExternalSource fill:#e9ecef,stroke:#6c757d\nstyle DF1 fill:#d1ecf1,stroke:#0c5460\nstyle DF2 fill:#d1ecf1,stroke:#0c5460\nstyle DS\\_Sales fill:#cde4ff,stroke:#0056b3,stroke-width:3px\nstyle R\\_SalesSummary fill:#d4edda,stroke:#155724\nstyle DB\\_ExecSales fill:#fff3cd,stroke:#856404\nstyle App\\_Sales fill:#f8d7da,stroke:#721c24\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#72-impact-analysis-assessing-change-consequences","title":"7.2 Impact Analysis: Assessing Change Consequences","text":"<p>When you select an artifact in the Lineage View (especially a dataset or dataflow), Power BI often  provides an Impact Analysis feature. This lists all known downstream artifacts that depend on the  selected item.</p> <p>For example, if you select a dataset, impact analysis will show:</p> <ul> <li>All reports that use this dataset.</li> <li>All dashboards that have tiles pinned from those reports.</li> <li>Any other datasets that might be sourcing data from it (e.g., composite models).</li> </ul> <p>This is crucial before making changes to a dataset (like removing a column, changing a measure's logic,  or altering data sources). It helps identify:  * Which reports might break or display incorrect information. * Who the owners of those dependent artifacts are (so they can be notified). * The potential \"blast radius\" of a change.</p> <p>Some impact analysis views also provide risk scoring elements like the number of views on dependent reports,  their refresh status, or endorsement level, helping prioritize communication or testing.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#73-best-practices-for-lineage-and-impact-management","title":"7.3 Best Practices for Lineage and Impact Management","text":"<ul> <li>Regularly Review Lineage: Periodically check lineage views to understand data flows  and identify any unexpected dependencies or orphaned items.</li> <li>Delete Orphans: Remove unused artifacts (e.g., datasets with no dependent reports,  old dataflows) to declutter workspaces and reduce confusion.</li> <li>Promote Dataset Reusability: Encourage the use of shared, certified datasets as  \"golden datasets\" or \"single sources of truth\" for key data domains. This simplifies lineage and  improves consistency.</li> <li>Document Breaking Change Policies: Establish clear communication protocols for  planned changes to shared datasets or dataflows. For example, provide a 2-week notice period via a  dedicated Teams channel or email distribution list to owners of dependent artifacts.</li> <li>Use Descriptions and Contacts: Ensure datasets and other key artifacts have clear  descriptions and contact information for their owners, making it easier for users to understand  their purpose and who to contact with questions.</li> <li>Leverage Third-Party Tools: For very complex environments, consider third-party  data catalog and lineage tools that can offer more extensive cross-platform lineage capabilities  beyond what's natively in Power BI.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#8-dataflows-incremental-refresh-computed-entities","title":"8. Dataflows, Incremental Refresh &amp; Computed Entities","text":"<p>Dataflows in Power BI are a powerful cloud-based data preparation technology. They use the familiar Power  Query engine (the same one found in Power BI Desktop and Excel) but run entirely within the Power BI  service. Dataflows allow you to ingest, clean, transform, shape, and enrich data, storing the results in  Azure Data Lake Storage (Gen1 or Gen2, with Gen2 being the standard for newer dataflows and Fabric  integration via OneLake). They play a crucial role in decoupling data transformation logic from  datasets, promoting reusability and better data management.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#81-authoring-modes-generations","title":"8.1 Authoring Modes &amp; Generations","text":"<p>Dataflows have evolved, and understanding the types is important:</p> <ul> <li>Standard Dataflows (Gen1): The original dataflows, primarily focused on Power Query  transformations. They output data to an Azure Data Lake Storage Gen2 account managed by Power BI or  to a customer-owned ADLS Gen2. They are good for many ETL scenarios.</li> <li>Dataflows Gen2 (Fabric): The newer generation of dataflows, deeply integrated with  Microsoft Fabric.<ul> <li>Output Destinations: Can output data directly to a Fabric Lakehouse (as  tables or files), a Fabric Warehouse, or Azure SQL Database (via data pipelines).</li> <li>Enhanced Engine: Leverage a more powerful backend engine for better  performance with large data volumes.</li> <li>Features: Include capabilities like high-scale data processing and better  integration with other Fabric workloads.</li> </ul> </li> </ul> <p>You author dataflows directly in the Power BI service using a web-based Power Query interface.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#82-incremental-refresh-for-dataflows","title":"8.2 Incremental Refresh for Dataflows","text":"<p>For dataflows that handle large and growing datasets (e.g., historical sales data, IoT logs), refreshing  the entire dataset each time can be time-consuming and resource-intensive. Incremental refresh for  dataflows solves this by partitioning the data (typically by date) and only refreshing new or changed  partitions.</p> <p>Setting up incremental refresh involves:</p> <ol> <li>Defining Date Parameters: Create two special date/time parameters in Power Query  named <code>RangeStart</code> and <code>RangeEnd</code> (case-sensitive).</li> <li>Filtering Data: Apply a filter to your date column in the Power Query editor using  these parameters (e.g., <code>[OrderDate] &gt;= RangeStart AND [OrderDate] &lt; RangeEnd</code>).</li> <li>Configuring Refresh Policy: In the dataflow settings, configure the incremental  refresh policy. You specify:<ul> <li>How many past periods to store (e.g., \"Store data from the last 5 Years\").</li> <li>How many recent periods to refresh (e.g., \"Refresh data from the last 10 Days\").</li> <li>Optional: Detect data changes using a separate date/time column.</li> </ul> </li> </ol> <p>This significantly speeds up refresh times and reduces load on source systems.</p> 8.2 Incremental Refresh for Dataflows Diagram <pre><code>graph TD\nsubgraph \"Incremental Refresh in Dataflows\"\nA[\"Full Historical Data Source (e.g., Sales Orders)\"] --&gt; B{\"Define RangeStart &amp; RangeEnd Parameters in\nPower Query\"}\nB --&gt; C[\"Filter Data by Date using RangeStart/RangeEnd\"]\nC --&gt; D[\"Configure Refresh Policy in Dataflow Settings\"]\nD --&gt; E[\"Initial Full Load (Data Partitioned by Date)\"]\nE --&gt; F[\"Subsequent Refreshes: Only New/Changed Data in 'Refresh Window' is Processed\"]\nF --&gt; G[\"Partitioned Data Stored in OneLake/ADLS Gen2\"]\nend\nstyle D fill:#fff3cd,stroke:#856404\nstyle G fill:#d1ecf1,stroke:#0c5460\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#83-computed-entities-linked-entities","title":"8.3 Computed Entities &amp; Linked Entities","text":"<p>Dataflows allow you to build modular and reusable data preparation logic through computed and linked  entities:</p> <ul> <li>Linked Entities: Allow you to reference an entity (table) from another dataflow  within your current dataflow. The data is not copied; your dataflow simply points to the output of  the source dataflow. This is excellent for reusing common data preparation steps (e.g., a centrally  managed \"Date Dimension\" dataflow). Refreshes are managed by the source dataflow.</li> <li>Computed Entities: An entity within the same dataflow that performs  transformations on data sourced from other entities within that dataflow (including linked  entities). Computed entities are useful for staging data or performing complex calculations in  multiple steps. They require Premium/Fabric capacity to refresh if they reference other entities  that are not simple source queries.</li> </ul> <p>Using these features helps avoid redundant data transformations, reduces gateway load (by transforming  data in the cloud), and promotes a \"transform once, use many times\" paradigm.</p> 8.3 Computed Entities &amp; Linked Entities Diagram <pre><code>graph TD\nsubgraph \"Dataflow with Linked and Computed Entities\"\nExternalSource1[\"Source: ERP Database\"] --&gt; DF1\\_EntityA[\"Dataflow 1: Entity A (Raw Orders)\"]\nExternalSource2[\"Source: CRM System\"] --&gt; DF1\\_EntityB[\"Dataflow 1: Entity B (Raw Customers)\"]\n\nDF1\\_EntityA --&gt; DF1\\_ComputedC[\"Dataflow 1: Computed Entity C (Cleaned Orders - Joins A &amp; B)\"]\nDF1\\_EntityB --&gt; DF1\\_ComputedC\n\nsubgraph \"Dataflow 2 (Sales Analytics Prep)\"\nLinked\\_DF1\\_EntityC[\"Linked Entity: (from Dataflow 1 - Entity C)\"] -- \"References\" --&gt; DF1\\_ComputedC\nExternalSource3[\"Source: Product Catalog CSV\"] --&gt; DF2\\_EntityD[\"Dataflow 2: Entity D (Product Details)\"]\n\nLinked\\_DF1\\_EntityC --&gt; DF2\\_ComputedE[\"Dataflow 2: Computed Entity E (Enriched Sales Data - Joins Linked\nC &amp; D)\"]\nDF2\\_EntityD --&gt; DF2\\_ComputedE\nend\n\nDF2\\_ComputedE -- \"Loads To\" --&gt; FinalDataset[\"Power BI Dataset: SalesAnalytics\"]\nend\nstyle DF1\\_ComputedC fill:#e6f2ff,stroke:#0056b3\nstyle DF2\\_ComputedE fill:#e6f2ff,stroke:#0056b3\nstyle Linked\\_DF1\\_EntityC fill:#d1ecf1,stroke:#0c5460\n\n**Key Benefits of Dataflows:**\n\n</code></pre> <ul> <li>Reusable Data Preparation: Define ETL logic once and reuse across multiple  datasets.</li> <li>Reduced Load on Gateways/Sources: Transformations happen in the Power BI service.</li> <li>Improved Dataset Refresh Times: Datasets can load pre-prepared data from dataflows.</li> <li>Democratization of Data Prep: Empowers business analysts with Power Query skills to  prepare data.</li> <li>Integration with Fabric: Dataflows Gen2 are a core part of the Microsoft Fabric  data integration story.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#9-object-level-column-level-security-olscls","title":"9. Object-Level &amp; Column-Level Security (OLS/CLS)","text":"<p>Beyond Row-Level Security (RLS) which filters rows of data, Power BI offers more granular control over  the visibility of tables and columns through Object-Level Security (OLS) and Column-Level Security  (CLS). These are advanced security features typically requiring Power BI Premium or Fabric capacity, as  they are often configured using external tools like Tabular Editor or by scripting against the XMLA  endpoint.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#91-recap-row-level-security-rls","title":"9.1 Recap: Row-Level Security (RLS)","text":"<p>RLS is the most common form of data security in Power BI datasets. It allows you to define security roles  and DAX filter expressions that restrict the rows of data users can see within a table based on their  identity or role membership. For example, a sales manager sees data for their region only, while a sales  representative sees only their own sales data.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#92-object-level-security-ols","title":"9.2 Object-Level Security (OLS)","text":"<p>Object-Level Security (OLS) provides a robust way to secure your data model by hiding entire tables or  specific columns from users based on their roles. For example, an 'EmployeeSalaries' table might be  completely hidden from users in a 'Sales' role, or a 'SocialSecurityNumber' column within an 'Employees'  table could be hidden from most users.</p> <p>Key characteristics of OLS:</p> <ul> <li>Granularity: Hides entire tables or entire columns.</li> <li>Implementation: Configured by defining roles (typically in Tabular Editor or via  XMLA script) and setting the metadata permission for specific tables/columns to \"None\" for those  roles. It's not configurable directly in Power BI Desktop's main interface for roles in the same way  RLS is.</li> <li>Impact: If an object (table or column) is hidden for a user via OLS, it's as if  that object does not exist in the data model for that user. Any measures, visuals, or queries in a  report that attempt to access a hidden object will result in an error for that user.</li> <li>Use Cases: Securing highly sensitive tables (e.g., HR salary data) or columns  containing PII from unauthorized roles.</li> </ul> 9.2 Object-Level Security (OLS) Diagram <pre><code>graph TD\nsubgraph \"OLS Example: Securing Salary Information\"\nUserModel[\"Data Model (Dataset)\"]\nUserModel --&gt; Table\\_Employees[\"Table: Employees (Columns: EmpID, Name, Department, Salary)\"]\nUserModel --&gt; Table\\_Sales[\"Table: Sales (Columns: OrderID, EmpID, Amount)\"]\n\nRole\\_HR[\"Role: HR\\_Manager\"]\nRole\\_Sales[\"Role: Sales\\_Rep\"]\n\nRole\\_HR -- \"OLS Permission: Default (Read)\" --&gt; Table\\_Employees\nRole\\_HR -- \"OLS Permission: Default (Read)\" --&gt; Column\\_Salary[\"Column: Salary (in Employees)\"]\n\nRole\\_Sales -- \"OLS Permission: Default (Read)\" --&gt; Table\\_Employees\nRole\\_Sales -- \"OLS Permission: None (Hidden)\" --&gt; Column\\_Salary\nRole\\_Sales -- \"OLS Permission: None (Hidden)\" --&gt; Table\\_HR\\_Sensitive[\"Table: HR\\_Sensitive\\_Data (OLS\nHides Entire Table)\"]\n\n\nUser\\_Alice[\"User: Alice (in HR\\_Manager Role)\"] -- \"Views Report\" --&gt; Report\\_HR[\"HR Report\"]\nReport\\_HR -- \"Accesses Salary Column\" --&gt; Query\\_Salary\\_Visible[\"Salary Data Visible to Alice\"]\n\nUser\\_Bob[\"User: Bob (in Sales\\_Rep Role)\"] -- \"Views Report\" --&gt; Report\\_Sales[\"Sales Report (or HR\nReport)\"]\nReport\\_Sales -- \"Attempts to Access Salary Column\" --&gt; Query\\_Salary\\_Error[\"Error for Bob (Salary Column\nHidden by OLS)\"]\nReport\\_Sales -- \"Attempts to Access HR\\_Sensitive\\_Data Table\" --&gt; Query\\_Table\\_Error[\"Error for Bob (Table\nHidden by OLS)\"]\nend\nstyle Column\\_Salary fill:#ffdddd,stroke:#dc3545\nstyle Table\\_HR\\_Sensitive fill:#ffdddd,stroke:#dc3545\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#93-column-level-security-cls","title":"9.3 Column-Level Security (CLS)","text":"<p>Column-Level Security (CLS) offers a more granular way to secure data at the column level compared to  OLS. Instead of completely hiding a column (making it non-existent for a role), CLS allows you to  specifically deny read access to a column for a given role. If a user tries to query a column for which  they have been denied CLS permissions, they will receive an error for that specific column, but they can  still see and query other columns in the same table (provided they have access and those columns are not  also restricted).</p> <p>Key characteristics of CLS:</p> <ul> <li>Granularity: Denies read access to specific columns, while the table and other  columns in it remain visible and queryable.</li> <li>Implementation: Similar to OLS, it's configured by defining roles and setting  column permissions (specifically, denying \"Read\" permission on a column for a role) using tools like  Tabular Editor or XMLA scripts.</li> <li>Impact: Users are blocked from seeing data in specific columns they are not  authorized for, but can still work with the rest of the table. This is subtly different from OLS  where the column effectively disappears from the model for that role.</li> <li>Use Cases: Useful when you want to allow access to most of a table but restrict a  few sensitive columns within it, without breaking reports that might use other columns from that  same table.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#94-dynamic-data-masking-using-dax","title":"9.4 Dynamic Data Masking (Using DAX)","text":"<p>While OLS and CLS provide metadata-level security, Dynamic Data Masking (DDM) is often implemented using  DAX logic within measures or calculated columns to conditionally obscure or replace sensitive data based  on user context (e.g., their role or username). This is not a separate \"security feature\" like OLS/CLS  but a technique.</p> <p>Example: A measure could return the actual salary for HR users but <code>BLANK()</code>, <code>\"[Masked]\"</code>, or an  aggregated/anonymized value for other users.</p> <pre><code>Masked Salary =\nIF (\n    CONTAINSSTRING ( USERPRINCIPALNAME(), \"@hrdomain.com\" ) || USERNAME() IN { \"AdminUser1\", \"AdminUser2\" },\n    SUM ( Employees[Salary] ), // Show actual salary for HR or specific admin users\n    \"*****\" // Show masked value for others\n)\n</code></pre> <p>Note: DAX-based masking can be effective for presentation, but it doesn't prevent  determined users with model access from potentially inferring data if not carefully implemented. OLS/CLS  provide stronger, engine-enforced security at the metadata layer.</p> <p>Choosing between OLS, CLS, and RLS:</p> <ul> <li>Use RLS to filter rows.</li> <li>Use OLS to hide entire tables or columns from roles.</li> <li>Use CLS to deny read access to specific columns while keeping the table/other  columns visible.</li> <li>Combine these as needed for comprehensive security. For instance, a user might have RLS applied to a  table, and OLS applied to hide certain columns within the rows they can see.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#10-sensitivity-labels-microsoft-information-protection-mip","title":"10. Sensitivity Labels &amp; Microsoft Information Protection (MIP)","text":"<p>In today's data-driven world, protecting sensitive information is paramount. Power BI integrates deeply  with Microsoft Purview Information Protection (MIP) to allow organizations to classify and protect their  BI content using sensitivity labels. This ensures that data governance policies extend  from source systems through to analytics and even to exported content.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#101-what-are-sensitivity-labels","title":"10.1 What are Sensitivity Labels?","text":"<p>Sensitivity labels are metadata tags that classify the sensitivity of data (e.g., \"Non-Business,\"  \"General,\" \"Confidential,\" \"Highly Confidential,\" \"Secret\"). These labels are defined centrally in the  Microsoft Purview compliance portal and can be applied across various Microsoft services, including  Office apps, SharePoint, Teams, and Power BI.</p> <p>When applied in Power BI, labels can trigger protection settings such as:</p> <ul> <li>Visual markings: Watermarks, headers, or footers on reports and dashboards.</li> <li>Export controls: Preventing export of data from content with certain labels, or  ensuring labels and protection persist on exported files.</li> <li>Encryption: (Applied to exported files) If the label is configured with encryption,  exported files (like Excel, PDF, PowerPoint) will be encrypted and require authentication to open.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#102-label-inheritance-and-persistence","title":"10.2 Label Inheritance and Persistence","text":"<p>A key strength of MIP integration in Power BI is label inheritance and persistence:</p> <ul> <li>Dataset to Downstream Content: If a sensitivity label is applied to a Power BI  dataset, that label is automatically inherited by any new reports, dashboards, or dataflows created  from that dataset.</li> <li>Data Source Inheritance (Preview): In some scenarios (e.g., connecting to Azure  Synapse Analytics or Azure SQL Database that have sensitivity labels applied at the source), Power  BI can inherit these labels into datasets.</li> <li>Persistence on Export: When users export data from Power BI (e.g., to  <code>.xlsx</code>, <code>.pdf</code>, <code>.pptx</code> files), the sensitivity label applied to  the Power BI content (and any associated protection like encryption) is persisted on the exported  file. This ensures data remains classified and protected even when it leaves the Power BI service.</li> <li>Cross-Service Consistency: Labels are consistent across the Microsoft ecosystem. A  \"Confidential\" label in Power BI means the same as in Outlook or SharePoint.</li> </ul> 10.2 Label Inheritance and Persistence Diagram <pre><code>graph TD\nsubgraph \"Sensitivity Label Flow in Power BI\"\nPurview[\"Microsoft Purview Compliance Portal (Define Labels &amp; Policies)\"] --&gt; Label\\_Confidential[\"Label:\n'Confidential' (with Encryption &amp; Watermark)\"]\n\nLabel\\_Confidential -- \"Applied To\" --&gt; PBI\\_Dataset[\"Power BI Dataset: FinancialData\"]\nstyle PBI\\_Dataset fill:#ffdddd,stroke:#dc3545\n\nPBI\\_Dataset -- \"Inherited By (Auto)\" --&gt; PBI\\_Report[\"Power BI Report: QuarterlyFinancials\"]\nstyle PBI\\_Report fill:#ffdddd,stroke:#dc3545\n\nPBI\\_Report -- \"Inherited By (Auto)\" --&gt; PBI\\_Dashboard[\"Power BI Dashboard: Finance KPIs\"]\nstyle PBI\\_Dashboard fill:#ffdddd,stroke:#dc3545\n\nPBI\\_Report -- \"User Exports To\" --&gt; Exported\\_PDF[\"Exported PDF File\"]\nExported\\_PDF --&gt; Label\\_Persists\\_PDF[\"PDF has 'Confidential' Label, Watermark &amp; Encryption\"]\n\nPBI\\_Dataset -- \"User Exports To (Analyze in Excel)\" --&gt; Exported\\_XLSX[\"Exported Excel File\"]\nExported\\_XLSX --&gt; Label\\_Persists\\_XLSX[\"Excel File has 'Confidential' Label &amp; Encryption\"]\nend\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#103-autolabeling-policies-mandatory-labeling","title":"10.3 Auto\u2011Labeling Policies &amp; Mandatory Labeling","text":"<p>To improve compliance and reduce user burden, administrators can configure:</p> <ul> <li>Default Labeling Policies: Set a default sensitivity label for new content in Power  BI. For example, all new datasets could start with a \"General\" label.</li> <li>Mandatory Labeling Policies: Require users to apply a sensitivity label before they  can save or publish content (datasets, reports, etc.).</li> <li>Auto-Labeling Policies (via Microsoft Purview): For content in Premium/Fabric  capacities, Purview can be configured to automatically scan datasets for sensitive information types  (e.g., credit card numbers, social security numbers, custom sensitive info types). If detected,  Purview can recommend or automatically apply a specific sensitivity label.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#104-auditing-and-monitoring-label-usage","title":"10.4 Auditing and Monitoring Label Usage","text":"<p>All sensitivity labeling activities in Power BI (applying, changing, removing labels) are captured in the  audit logs (Activity Logs and Microsoft Purview audit logs). This allows administrators to:</p> <ul> <li>Track how labels are being used.</li> <li>Monitor compliance with labeling policies.</li> <li>Investigate incidents related to sensitive data handling.</li> </ul> <p>The Power BI Admin Portal also provides \"Protection Metrics\" to visualize label distribution and usage  across the tenant.</p> <p>Key Benefits:</p> <ul> <li>Consistent Data Classification: Standardized labels across the organization.</li> <li>Enhanced Data Protection: Automatic application of protection measures like  encryption.</li> <li>Regulatory Compliance: Helps meet requirements of data protection regulations  (e.g., GDPR, CCPA).</li> <li>Increased User Awareness: Visual labels remind users of data sensitivity.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#11-rest-api-powershell-cli-automation","title":"11. REST API, PowerShell &amp; CLI Automation","text":"<p>Automating Power BI administrative tasks, deployments, and management operations is essential for  efficiency, consistency, and scalability in enterprise environments. Power BI provides robust REST APIs,  along with PowerShell cmdlets and a Command Line Interface (CLI), to enable a wide range of automation  scenarios.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#111-power-bi-rest-apis-the-foundation-for-automation","title":"11.1 Power BI REST APIs: The Foundation for Automation","text":"<p>The Power BI REST APIs are the core programmatic interface to the Power BI service. They allow developers  and administrators to interact with nearly every aspect of Power BI, including:</p> <ul> <li>Workspace Management: Create, delete, update workspaces (groups). Get workspace  lists.</li> <li>Content Management:<ul> <li>Import/Deploy PBIX files (<code>POST /groups/{groupId}/imports</code>).</li> <li>Update dataset parameters and data source credentials  (<code>PATCH /datasets/{datasetId}</code>,  <code>POST /datasets/{datasetId}/Default.SetAllConnections</code>).</li> <li>Refresh datasets (<code>POST /datasets/{datasetId}/refreshes</code>). Get refresh history.</li> <li>Manage reports, dashboards, tiles.</li> </ul> </li> <li>Gateway Management: Manage on-premises data gateway clusters and data sources.</li> <li>Embedding: Generate embed tokens for reports, dashboards, and tiles.</li> <li>Administration:<ul> <li>Access tenant settings, activity logs (<code>GET /admin/activityevents</code>).</li> <li>Manage capacities (<code>GET /admin/capacities</code>), get capacity workloads.</li> <li>Manage users, features, organizational visuals.</li> </ul> </li> <li>Deployment Pipelines: Programmatically manage deployment pipeline operations  (create pipeline, deploy content).</li> </ul> <p>Authentication with the APIs is typically done using Azure Active Directory (Azure AD) tokens, obtained  via user authentication or service principals (for unattended automation).</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#112-powershell-cmdlets-for-power-bi","title":"11.2 PowerShell Cmdlets for Power BI","text":"<p>Microsoft provides a dedicated PowerShell module (<code>MicrosoftPowerBIMgmt</code>) that wraps many of  the REST API functionalities into easy-to-use cmdlets. This is often preferred by administrators  familiar with PowerShell for scripting routine tasks.</p> <p>Example cmdlets:</p> <ul> <li><code>Login-PowerBIServiceAccount</code> / <code>Connect-PowerBIServiceAccount</code></li> <li><code>Get-PowerBIWorkspace</code>, <code>New-PowerBIWorkspace</code></li> <li><code>Get-PowerBIDataset</code>, <code>New-PowerBIDataflow</code></li> <li><code>Invoke-PowerBIRestMethod</code> (for calling any REST API endpoint directly)</li> <li><code>Start-PowerBIDatasetRefresh</code></li> </ul> <pre><code># Example: Refreshing a dataset in a specific workspace\nConnect-PowerBIServiceAccount\n$workspaceId = \"YOUR_WORKSPACE_ID\"\n$datasetId = \"YOUR_DATASET_ID\"\nInvoke-PowerBIRestMethod -Url \"groups/$workspaceId/datasets/$datasetId/refreshes\" -Method Post\nWrite-Host \"Dataset refresh initiated.\"\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#113-power-bi-cli-command-line-interface","title":"11.3 Power BI CLI (Command Line Interface)","text":"<p>The Power BI CLI is another command-line tool, often used in cross-platform scripting environments or  integrated into DevOps pipelines. It also leverages the REST APIs.</p> <pre><code># Example: Login using a service principal (common in CI/CD)\npbicli login --service-principal --tenant YOUR_TENANT_ID --username YOUR_APP_ID --password YOUR_APP_SECRET\n\n# Example: Create a deployment pipeline\npbicli pipelines create --name \"Sales Analytics Pipeline\" --display-name \"Sales Analytics Pipeline\"\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#114-devops-integration-common-automation-scenarios","title":"11.4 DevOps Integration &amp; Common Automation Scenarios","text":"<p>Automation is key for robust CI/CD (Continuous Integration/Continuous Deployment) for Power BI content:</p> <ul> <li>Automated Deployment: Use Azure DevOps, GitHub Actions, or other CI/CD tools to  automate the deployment of PBIX files (or PBIP projects) through Dev, Test, and Prod workspaces  using deployment pipelines. Scripts can call REST APIs or CLI commands.</li> <li>Parameter &amp; Credential Management: Automate the update of dataset parameters (e.g.,  server names) and data source credentials for different environments.</li> <li>Scheduled Refreshes &amp; Monitoring: While Power BI has built-in scheduling, APIs can  be used for more complex refresh orchestration or to trigger refreshes based on external events.  Custom scripts can monitor refresh status and send notifications.</li> <li>Workspace Provisioning: Automate the creation of new workspaces based on templates  or requests.</li> <li>Tenant Auditing &amp; Reporting: Regularly extract activity logs and other admin data  via APIs to build custom monitoring and governance dashboards.</li> <li>Backup &amp; Restore (for datasets): While Power BI has some native capabilities, APIs  can be used to export PBIX files as a form of backup (though this doesn't capture all service  configurations).</li> </ul> 11.4 DevOps Integration &amp; Common Automation Scenarios Diagram <pre><code>graph TD\nsubgraph \"CI/CD Pipeline for Power BI with Automation\"\nDevRepo[\"Developer Commits PBIP/PBIX to Git Repository\"] --&gt; TriggerPipeline[\"Azure DevOps/GitHub\nActions Pipeline Triggered\"]\n\nTriggerPipeline --&gt; BuildStage[\"Build Stage\"]\nBuildStage --&gt; ValidatePBI[\"Validate PBIX/PBIP (e.g., using Tabular Editor CLI for Best Practices)\"]\n\nValidatePBI --&gt; DeployDev[\"Deploy to DEV Workspace (via API/CLI)\"]\nDeployDev --&gt; UpdateParamsDev[\"Update DEV Parameters/Credentials (API/CLI)\"]\nUpdateParamsDev --&gt; RunTestsDev[\"Run Automated Tests on DEV (e.g., DAX query tests)\"]\n\nRunTestsDev -- \"Approval Gate\" --&gt; DeployTest[\"Deploy to TEST Workspace (Pipeline Operation - API/CLI)\"]\nDeployTest --&gt; UpdateParamsTest[\"Update TEST Parameters/Credentials\"]\nUpdateParamsTest --&gt; UAT[\"User Acceptance Testing (Manual/Automated UI)\"]\n\nUAT -- \"Approval Gate\" --&gt; DeployProd[\"Deploy to PROD Workspace (Pipeline Operation - API/CLI)\"]\nDeployProd --&gt; UpdateParamsProd[\"Update PROD Parameters/Credentials\"]\nDeployProd --&gt; MonitorProd[\"Monitor Production Content\"]\nend\nstyle DevRepo fill:#e9ecef,stroke:#6c757d\nstyle TriggerPipeline fill:#cce5ff,stroke:#0056b3\nstyle DeployDev fill:#d4edda,stroke:#155724\nstyle DeployTest fill:#d4edda,stroke:#155724\nstyle DeployProd fill:#d4edda,stroke:#155724\n\nBy leveraging these automation tools, organizations can significantly improve the reliability, speed, and\ngovernance of their Power BI deployments and operations.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#12-report-subscriptions-data-driven-alerts","title":"12. Report Subscriptions &amp; Data-Driven Alerts","text":"<p>Power BI provides features to keep users proactively informed about their data through scheduled email  snapshots of reports (Subscriptions) and notifications triggered by data changes meeting specific  criteria (Data Alerts). These help users stay up-to-date without having to manually check reports  constantly.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#121-report-dashboard-subscriptions","title":"12.1 Report &amp; Dashboard Subscriptions","text":"<p>Subscriptions allow users to receive an email containing an image or PDF snapshot of a  report page or a dashboard, along with a link back to the content in Power BI. These emails are sent on  a schedule defined by the user (e.g., daily, weekly, monthly, or after data refresh).</p> <ul> <li>Who can subscribe: Users with Pro/PPU licenses (or Free users if content is in  Premium/Fabric) can subscribe themselves. Users with edit permissions on the content can also  subscribe others or Azure AD groups.</li> <li>Content: You can subscribe to a specific report page or an entire dashboard.</li> <li>Format: Emails typically include an image of the content. For reports, you can  often choose to attach a PDF or PowerPoint of the report page.</li> <li>Frequency: Max 24 subscriptions per report or dashboard per user. Schedule can be  time-based or triggered after a dataset refresh (for reports).</li> <li>Data Context:<ul> <li>Personal Subscriptions (Subscribing yourself): The snapshot you receive  respects any Row-Level Security (RLS) and other permissions applied to you. You see the data  as you would if you viewed the report directly.</li> <li>Subscriptions for Others (Organizational): When a report owner/editor  subscribes other users or groups, the data snapshot is generated based on the  subscriber's (the person setting up the subscription) security context and permissions  at the time of setting it up. This means all recipients of this specific  subscription instance see the same data view. If RLS needs to be applied per recipient, each  recipient should ideally set up their own subscription, or the publisher needs to manage  multiple subscriptions carefully.</li> </ul> </li> <li>Limitations: Not all visuals are supported for subscriptions (e.g., some custom  visuals, R/Python visuals might not render). Full report interactivity is not available in the  email; users must click the link to go to Power BI.</li> </ul> 12.1 Report &amp; Dashboard Subscriptions Diagram <pre><code>graph TD\nsubgraph \"Email Subscription Workflow\"\nUser[\"User (Pro/PPU)\"] -- \"Accesses\" --&gt; ReportDashboard[\"Report Page / Dashboard\"]\nReportDashboard -- \"User Clicks 'Subscribe'\" --&gt; SubscriptionSetup[\"Subscription Setup Pane\"]\n\nSubscriptionSetup -- \"Configure\" --&gt; Recipients[\"Recipients (Self, Others, Groups)\"]\nSubscriptionSetup -- \"Configure\" --&gt; Schedule[\"Schedule (Daily, Weekly, On Refresh)\"]\nSubscriptionSetup -- \"Configure\" --&gt; Format[\"Format (Image, PDF/PPT for reports)\"]\nSubscriptionSetup -- \"Configure\" --&gt; OptionalMessage[\"Optional Message\"]\n\nSchedule -- \"Triggers At Scheduled Time/Event\" --&gt; SnapshotGeneration[\"Power BI Service Generates\nSnapshot\"]\nSnapshotGeneration -- \"Data Context Applied (RLS)\" --&gt; EmailSending[\"Email Sent to Recipients\"]\nEmailSending --&gt; EmailClient[\"Recipients Receive Email with Image/Attachment &amp; Link\"]\nend\nstyle SubscriptionSetup fill:#fff3cd,stroke:#856404\nstyle EmailClient fill:#d4edda,stroke:#155724\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#122-data-alerts-on-dashboard-tiles","title":"12.2 Data Alerts on Dashboard Tiles","text":"<p>Data Alerts can be set on specific types of dashboard tiles to notify users when the  data on those tiles meets a defined threshold. This is useful for monitoring key metrics and being  alerted to important changes.</p> <ul> <li>Supported Tiles: Alerts can only be set on tiles displaying a single number: Cards  (single number, KPI, multi-row card), KPIs, and Gauges.</li> <li>Conditions: You can set alerts to trigger if a value goes \"Above\" or \"Below\" a  numeric threshold.</li> <li>Frequency: Power BI checks for alert conditions approximately every hour (or every  15 minutes for tiles on dashboards connected to datasets on Premium/Fabric capacity, if  DirectQuery/LiveConnection with \"Automatic page refresh\" or streaming datasets are used).</li> <li>Notifications: When an alert condition is met:<ul> <li>An email notification is sent to the user who set the alert.</li> <li>A notification appears in the user's Power BI Notification Center (in the service).</li> <li>A notification appears on the Power BI mobile app.</li> <li>Crucially, an alert can trigger a Microsoft Power Automate (Flow), enabling  a wide range of custom actions (e.g., send a message to a Teams channel, create a task in  Planner, log the alert to a database, call a custom API).</li> </ul> </li> <li>Ownership: Only the user who created an alert can see it and receive its direct  notifications (unless shared via Power Automate). Alerts are personal.</li> </ul> 12.2 Data Alerts on Dashboard Tiles Diagram <pre><code>graph LR\nsubgraph \"Data Alert Workflow\"\nDashboardTile[\"Dashboard Tile (Card, KPI, Gauge)\"] -- \"User Sets Alert On\" --&gt; AlertConfig[\"Alert\nConfiguration (e.g., Sales &gt; $1M)\"]\n\nPowerBI\\_Checks[\"Power BI Service (Periodically Checks Data)\"] -- \"Data Meets Threshold?\" --&gt;\nAlertTriggered{\"Alert Triggered!\"}\n\nAlertTriggered -- \"Yes\" --&gt; EmailNotify[\"Email to User\"]\nAlertTriggered -- \"Yes\" --&gt; PBI\\_Notification[\"Notification in Power BI Service &amp; Mobile\"]\nAlertTriggered -- \"Yes\" --&gt; PowerAutomate[\"Trigger Power Automate Flow (Optional)\"]\n\nPowerAutomate --&gt; CustomAction1[\"Custom Action 1 (e.g., Post to Teams)\"]\nPowerAutomate --&gt; CustomAction2[\"Custom Action 2 (e.g., Create CRM Task)\"]\nend\nstyle AlertConfig fill:#fff3cd,stroke:#856404\nstyle AlertTriggered fill:#f8d7da,stroke:#721c24\n\n**Tip for Alerts:** To reduce notification noise, carefully consider thresholds. For\ncritical operational metrics, integrating alerts with Power Automate can provide more robust and\nactionable follow-up processes than just email notifications.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#13-scorecards-goals-metrics","title":"13. Scorecards (Goals) &amp; Metrics","text":"<p>Scorecards in Power BI (often referred to by their underlying component, \"Goals\" or \"Metrics\") are a  powerful feature for defining, tracking, and sharing progress on key business objectives and Key  Performance Indicators (KPIs). They help organizations translate strategic plans into measurable  outcomes, assign ownership, and monitor performance collaboratively.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#131-what-are-scorecards-and-metrics","title":"13.1 What are Scorecards and Metrics?","text":"<ul> <li>Metric (Goal): A single, trackable objective. Each metric has:<ul> <li>Name: A clear description of what's being measured (e.g., \"Increase  Quarterly Sales Revenue,\" \"Improve Customer Satisfaction Score\").</li> <li>Current Value: The actual current value of the metric. This can be manually  entered or connected to data in an existing Power BI report visual.</li> <li>Target Value: The desired future value for the metric.</li> <li>Status: A visual indicator of progress (e.g., \"On Track,\" \"At Risk,\"  \"Behind,\" \"Completed,\" \"Not Started\"). Statuses can often be automated based on rules  comparing current to target values.</li> <li>Owner(s): The individual(s) or team(s) responsible for the metric.</li> <li>Start Date &amp; Due Date: Timeframe for achieving the metric.</li> </ul> </li> <li>Scorecard: A collection of related metrics, often grouped by strategic area or  business unit. Scorecards provide a consolidated view of performance against multiple objectives.  They can also include sub-metrics (hierarchical goals).</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#132-creating-and-connecting-metrics","title":"13.2 Creating and Connecting Metrics","text":"<p>The process of setting up metrics in a scorecard typically involves:</p> <ol> <li>Create a New Scorecard: Within a Power BI workspace, select the option to create a  new scorecard.</li> <li>Add Metrics (Goals):<ul> <li>Define the metric name, owner(s), and due date.</li> <li>For Current Value and Target Value:<ul> <li>Connect to data in a Power BI report: Select an existing report and  a specific visual/data point within that visual to dynamically pull the current  and/or target values. This ensures the metric stays up-to-date as the underlying  data refreshes.</li> <li>Manually enter values: If the data isn't in a Power BI report or  needs manual input.</li> </ul> </li> </ul> </li> <li>Set Status Rules (Optional but Recommended): Define rules that automatically set  the status of a metric based on how the current value compares to the target value (e.g., if Current <p>= 90% of Target, status is \"On Track\").</p> </li> <li>Organize Metrics: Group related metrics, create sub-metrics for more detailed  tracking.</li> </ol> 13.2 Creating and Connecting Metrics Diagram <pre><code>graph TD\nsubgraph \"Creating &amp; Connecting a Metric in a Scorecard\"\nCreateScorecard[\"1. Create New Scorecard in Workspace\"] --&gt; AddMetric[\"2. Add New Metric/Goal\"]\n\nAddMetric -- \"Define\" --&gt; MetricDetails[\"Name, Owner, Due Date\"]\nAddMetric -- \"Set Values\" --&gt; CurrentValue{\"Current Value\"}\nAddMetric -- \"Set Values\" --&gt; TargetValue{\"Target Value\"}\n\nCurrentValue -- \"Option A: Connect to Data\" --&gt; ConnectToReport[\"Select Power BI Report &amp; Visual/Data\nPoint\"]\nTargetValue -- \"Option A: Connect to Data\" --&gt; ConnectToReport\nConnectToReport --&gt; DynamicData[\"Values Dynamically Pulled\"]\n\nCurrentValue -- \"Option B: Manual Entry\" --&gt; ManualInputCurrent[\"Enter Current Value Manually\"]\nTargetValue -- \"Option B: Manual Entry\" --&gt; ManualInputTarget[\"Enter Target Value Manually\"]\n\nAddMetric -- \"Configure\" --&gt; StatusRules[\"3. Set Status Rules (e.g., On Track if Current &gt;= 90% of\nTarget)\"]\nStatusRules --&gt; AutomatedStatus[\"Metric Status (Automated or Manual)\"]\n\nAutomatedStatus --&gt; DisplayInScorecard[\"4. Metric Displayed in Scorecard\"]\nend\nstyle ConnectToReport fill:#d1ecf1,stroke:#0c5460\nstyle StatusRules fill:#fff3cd,stroke:#856404\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#133-checkins-and-collaboration","title":"13.3 Check\u2011ins and Collaboration","text":"<p>Scorecards are designed for collaboration and progress tracking:</p> <ul> <li>Check-ins: Metric owners (or anyone with permissions) can perform \"check-ins\" on a  metric. During a check-in, they can:<ul> <li>Update the current value (if manually entered).</li> <li>Update the status (if not fully automated or if manual override is needed).</li> <li>Add notes or comments: This is crucial for providing context, explaining progress,  highlighting challenges, or outlining next steps. These notes create a historical log for  the metric.</li> </ul> </li> <li>Activity Feed: Scorecards often have an activity feed showing recent check-ins,  status changes, and comments, facilitating team communication around goals.</li> <li>Sharing and Permissions: Scorecards can be shared like other Power BI content, and  permissions control who can view or edit metrics and perform check-ins.</li> <li>Integration with Teams: Scorecards can be embedded in Microsoft Teams channels for  better visibility and collaborative discussions.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#134-benefits-of-using-scorecards","title":"13.4 Benefits of Using Scorecards","text":"<ul> <li>Alignment: Helps align teams and individuals with strategic organizational  objectives.</li> <li>Accountability: Clear ownership for each metric.</li> <li>Visibility: Provides a transparent view of performance against goals.</li> <li>Data-Driven Decisions: Encourages tracking progress with actual data.</li> <li>Collaboration: Facilitates discussions and problem-solving around performance.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#14-paginated-reports-integration","title":"14. Paginated Reports Integration","text":"<p>Paginated reports (based on Microsoft's Report Definition Language - RDL, the same technology used in SQL  Server Reporting Services - SSRS) are designed for creating highly formatted, \"pixel-perfect\" documents  that are optimized for printing or PDF generation. They are ideal for operational reports, invoices,  financial statements, transcripts, certificates, and any scenario where precise layout and presentation  are critical, often spanning multiple pages with repeating headers/footers.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#141-key-characteristics-of-paginated-reports","title":"14.1 Key Characteristics of Paginated Reports","text":"<ul> <li>Fixed Layout: Unlike interactive Power BI reports that dynamically resize,  paginated reports have a defined page size and layout. Content flows across pages in a structured  manner.</li> <li>Pixel-Perfect Control: Authors have precise control over the placement, size, and  appearance of every element on the page.</li> <li>Data-Driven: Can connect to a wide variety of data sources, including Power BI  datasets, SQL Server, Oracle, Azure SQL Database, and more.</li> <li>Parameters: Support rich parameterization, allowing users to customize the report  output at runtime (e.g., select a date range, a specific region).</li> <li>Export Formats: Excel at exporting to formats like PDF, Word, Excel (with more  layout fidelity than standard Power BI report exports), CSV, XML, PowerPoint, and image files.</li> <li>Subscription &amp; Delivery: Can be subscribed to for scheduled email delivery, often  with various export formats.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#142-authoring-tools-for-paginated-reports","title":"14.2 Authoring Tools for Paginated Reports","text":"<p>Paginated reports are created using dedicated design tools:</p> <ul> <li>Power BI Report Builder: A free, stand-alone Windows application provided by  Microsoft. It's the primary tool for creating and editing paginated reports for the Power BI  service. It offers a drag-and-drop interface as well as expression-based customization.</li> <li>SQL Server Data Tools (SSDT) for Visual Studio: For developers who prefer a Visual  Studio environment, SSDT includes a Report Designer project type. This is often used when reports  need to be managed under source control (e.g., Git) as part of a larger development project. The  <code>.rdl</code> files created here can be published to Power BI.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#143-data-sources-connectivity","title":"14.3 Data Sources &amp; Connectivity","text":"<p>Paginated reports in Power BI can connect to a variety of data sources:</p> <ul> <li>Power BI Datasets (via XMLA endpoint): A powerful feature allowing paginated  reports to leverage existing, curated Power BI datasets (including those with RLS/OLS). This  promotes a single source of truth. Requires Premium/PPU or Fabric capacity for the workspace hosting  the dataset.</li> <li>Azure SQL Database &amp; Azure Synapse Analytics</li> <li>SQL Server (on-premises or Azure VM) (often via a Power BI Gateway)</li> <li>Oracle, Teradata, and other relational databases</li> <li>Azure Analysis Services</li> <li>Enter Data: Small, static datasets can be embedded directly.</li> <li>DAX, MDX, SQL, and other query languages can be used depending on the source.</li> </ul> 14.3 Data Sources &amp; Connectivity Diagram <pre><code>graph TD\nsubgraph \"Paginated Report Creation &amp; Consumption Workflow\"\nAuthoringTool[\"Authoring: Power BI Report Builder / Visual Studio (SSDT)\"] -- \"Creates\" --&gt;\nRDL\\_File[\".RDL File (Report Definition)\"]\n\nRDL\\_File -- \"Published To\" --&gt; PBI\\_Workspace[\"Power BI Workspace (Premium/PPU/Fabric)\"]\n\nPBI\\_Workspace -- \"Connects To Data Source(s)\" --&gt; DataSources{\"Data Sources\"}\nDataSources --&gt; PBI\\_Dataset[\"Power BI Dataset (via XMLA)\"]\nDataSources --&gt; SQL\\_DB[\"SQL Server / Azure SQL\"]\nDataSources --&gt; OtherDBs[\"Oracle, Teradata, etc.\"]\n\nUser[\"End User\"] -- \"Accesses Report via Power BI Service/App\" --&gt; RenderedReport[\"Rendered Paginated\nReport\"]\nRenderedReport -- \"User Interacts (Parameters)\" --&gt; CustomizedOutput[\"Customized Report Output\"]\nCustomizedOutput -- \"User Exports To\" --&gt; PDF\\_Export[\"PDF\"]\nCustomizedOutput -- \"User Exports To\" --&gt; Excel\\_Export[\"Excel\"]\nCustomizedOutput -- \"User Exports To\" --&gt; Word\\_Export[\"Word\"]\nCustomizedOutput -- \"User Subscribes To\" --&gt; Email\\_Delivery[\"Scheduled Email Delivery\"]\nend\nstyle AuthoringTool fill:#e9ecef,stroke:#6c757d\nstyle PBI\\_Workspace fill:#cce5ff,stroke:#0056b3\nstyle DataSources fill:#fff3cd,stroke:#856404\nstyle PDF\\_Export fill:#d4edda,stroke:#155724\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#144-licensing-and-publishing","title":"14.4 Licensing and Publishing","text":"<ul> <li>Licensing: To publish, view, and manage paginated reports in the Power BI service,  the workspace containing them must be backed by a Power BI Premium (P SKU), Premium Per User  (PPU), or Fabric (F SKU) capacity. Standard shared capacity does not support paginated  reports. End users consuming these reports also typically need a Pro/PPU license unless the content  is in a P/F SKU capacity that allows free user consumption.</li> <li>Publishing: <code>.rdl</code> files are published from Report Builder or deployed  from Visual Studio directly to a workspace in the Power BI service.</li> <li>Integration: Paginated reports can be included in Power BI Apps alongside  interactive Power BI reports and dashboards, providing a consolidated experience for users. They can  also be embedded into other applications.</li> </ul> <p>Paginated reports fill a critical gap for highly formatted, operational reporting needs that standard  interactive Power BI reports are not designed for.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#15-mobile-app-experience-offline-caching","title":"15. Mobile App Experience &amp; Offline Caching","text":"<p>Power BI mobile applications (available for iOS, Android, and Windows devices) extend the reach of  business intelligence, allowing users to access their reports, dashboards, and data insights anytime,  anywhere. The mobile experience is optimized for touch interaction and on-the-go consumption.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#151-optimized-mobile-layouts","title":"15.1 Optimized Mobile Layouts","text":"<p>While standard Power BI reports can be viewed on mobile devices, they may not always provide the best  experience due to screen size and orientation differences. To address this, Power BI Desktop allows  report creators to design a specific Phone Layout for their reports.</p> <ul> <li>Separate Design Surface: In Power BI Desktop, you can switch to a phone layout view  and arrange, resize, or even hide visuals specifically for portrait orientation on mobile phones.</li> <li>Responsive Visuals: Many native Power BI visuals are responsive and adapt to  smaller screen sizes.</li> <li>Automatic Detection: When a user opens a report in the Power BI mobile app, the app  automatically detects if a phone layout is available and displays it. If not, it shows the standard  web layout.</li> </ul> <p>Designing effective phone layouts is crucial for ensuring reports are easily consumable and actionable on  mobile devices.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#152-offline-mode-and-data-caching","title":"15.2 Offline Mode and Data Caching","text":"<p>The Power BI mobile apps offer offline capabilities to ensure users can access some data even without an  active internet connection:</p> <ul> <li>Dashboard Tile Caching: Dashboard tiles that a user has recently viewed are  automatically cached on their device. This cached data is typically available for a set period  (e.g., up to 24-48 hours, though this can vary).</li> <li>Report Data Caching (Limited): Some report data might also be cached, especially  for reports recently accessed. However, full offline interactivity for complex reports is generally  limited.</li> <li>Background Refresh: The app may attempt to refresh cached data in the background  when an internet connection is available.</li> <li>Favorites Access: Items marked as \"Favorites\" are often prioritized for caching and  easier offline access.</li> </ul> <p>Security Considerations for Offline Data:</p> <ul> <li>Data security policies like Row-Level Security (RLS) and sensitivity labels (MIP) that were applied  when the data was last refreshed online are generally respected for cached data.</li> <li>Access to the Power BI mobile app itself can be secured using device-level authentication methods  like Face ID, Touch ID, or a PIN code, adding an extra layer of protection for cached data.</li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#153-key-mobile-app-features","title":"15.3 Key Mobile App Features","text":"<ul> <li>Push Notifications: Users receive real-time push notifications for:<ul> <li>Data Alerts: When a data alert set on a dashboard tile is triggered.</li> <li>Mentions &amp; Comments: When @mentioned in comments on reports or dashboards.</li> <li>Scorecard (Goal) Updates: For important updates or check-ins on metrics  they own or follow.</li> <li>App Updates: When a Power BI App they use is updated by the publisher.</li> </ul> </li> <li>Annotation &amp; Sharing: Users can annotate report snapshots directly within the app  and share these annotated images via email or other messaging apps.</li> <li>QR Codes &amp; Barcode Scanning: Link real-world objects (via QR codes) or products  (via barcodes) directly to relevant Power BI reports for contextual insights.</li> <li>Voice Commands (Cortana Integration - platform dependent): Some platforms allow  users to interact with Power BI using voice commands.</li> <li>Secure Access: Integration with mobile device management (MDM) solutions like  Microsoft Intune for enhanced security and compliance in enterprise deployments.</li> </ul> 15.3 Key Mobile App Features Diagram <pre><code>graph TD\nsubgraph \"Power BI Mobile App Ecosystem\"\nPBI\\_Service[\"Power BI Service (Cloud Backend)\"]\n\nPBI\\_Service -- \"Syncs Content &amp; Data\" --&gt; MobileApp[\"Power BI Mobile App (iOS, Android, Windows)\"]\n\nMobileApp --&gt; OptimizedView[\"Optimized Report Viewing (Phone Layouts)\"]\nMobileApp --&gt; DashboardAccess[\"Dashboard Access\"]\nMobileApp --&gt; AppConsumption[\"Power BI App Consumption\"]\n\nMobileApp -- \"Features\" --&gt; OfflineCache[\"Offline Data Caching (Tiles, Recent Reports)\"]\nMobileApp -- \"Features\" --&gt; PushNotify[\"Push Notifications (Alerts, Mentions, Goal Updates)\"]\nMobileApp -- \"Features\" --&gt; AnnotationShare[\"Annotation &amp; Sharing Snapshots\"]\nMobileApp -- \"Features\" --&gt; QRCodes[\"QR Code &amp; Barcode Scanning\"]\nMobileApp -- \"Features\" --&gt; SecureAccess[\"Secure Access (FaceID/PIN, Intune Integration)\"]\n\nUserDevice[\"User's Mobile Device\"] --&gt; MobileApp\nOfflineCache --&gt; UserDeviceStorage[\"Local Device Storage (Encrypted)\"]\nend\nstyle MobileApp fill:#cce5ff,stroke:#0056b3,stroke-width:2px\nstyle OfflineCache fill:#fff3cd,stroke:#856404\nstyle PushNotify fill:#d4edda,stroke:#155724\n\nProviding a good mobile experience is essential for organizations where users need quick access to data\nwhile away from their desks, such as field sales teams, executives, or operations staff.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#16-real-time-dashboards-streaming-datasets","title":"16. Real-Time Dashboards &amp; Streaming Datasets","text":"<p>Power BI supports real-time analytics, enabling dashboards to display data as it streams in with very low  latency. This is crucial for monitoring scenarios in dynamic environments like IoT sensor data, social  media feeds, manufacturing process control, financial market data, and operational dashboards where  immediate insights are vital.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#161-real-time-dataset-types-in-power-bi","title":"16.1 Real-Time Dataset Types in Power BI","text":"<p>Power BI offers different types of datasets to handle real-time data streams:</p> <ul> <li>Push Datasets:<ul> <li>Data is actively \"pushed\" into a Power BI dataset using the Power BI REST API or tools like  Power Automate.</li> <li>The data is stored in a database within Power BI (this database is part of the dataset).</li> <li>Because data is stored, you can create reports on this historical push data, not just  real-time dashboard tiles.</li> <li>Dashboards with tiles from push datasets update in real-time as new data arrives.</li> <li>There are limits on the rate of data push (e.g., rows per second/hour, requests per minute).</li> </ul> </li> <li>Streaming Datasets:<ul> <li>Data is streamed directly to dashboard tiles. The data is temporarily cached on the tile  itself (for a short window, e.g., up to an hour) and is not stored in an underlying Power  BI database.</li> <li>This means historical analysis is generally not possible with pure streaming datasets; they  are for viewing the immediate, live data stream with very low latency.</li> <li>You cannot build traditional Power BI reports on pure streaming datasets because no  historical data is stored.</li> <li>Setup involves defining a schema (fields and data types) for the incoming data stream.</li> </ul> </li> <li>Hybrid (Push + Streaming) Datasets:<ul> <li>This type combines the benefits of both push and streaming datasets.</li> <li>Data is pushed to a Power BI database (like a push dataset), allowing for historical  reporting and analysis.</li> <li>AND, data can also be streamed directly to dashboard tiles for a low-latency, real-time  view (like a streaming dataset).</li> <li>This offers the best of both worlds: immediate monitoring and historical analysis from the  same data stream.</li> </ul> </li> <li>PubNub Streaming Datasets (Legacy): A specific type of streaming dataset that  integrates with the PubNub real-time messaging service.</li> </ul> 16.1 Real-Time Dataset Types in Power BI Diagram <pre><code>graph TD\nsubgraph \"Real-Time Dataset Options &amp; Characteristics\"\ndirection LR\nPush[\"Push Dataset\"]\nStreaming[\"Streaming Dataset\"]\nHybrid[\"Hybrid (Push + Streaming) Dataset\"]\n\nPush -- \"Data Stored in PBI?\" --&gt; Push\\_Store[\"Yes, in Dataset DB\"]\nPush -- \"Historical Reporting?\" --&gt; Push\\_Report[\"Yes\"]\nPush -- \"Real-time Tiles?\" --&gt; Push\\_Tiles[\"Yes\"]\nPush -- \"Latency\" --&gt; Push\\_Latency[\"Low to Moderate\"]\n\nStreaming -- \"Data Stored in PBI?\" --&gt; Stream\\_Store[\"No (Temp Cache on Tile)\"]\nStreaming -- \"Historical Reporting?\" --&gt; Stream\\_Report[\"No\"]\nStreaming -- \"Real-time Tiles?\" --&gt; Stream\\_Tiles[\"Yes (Very Low Latency)\"]\nStreaming -- \"Latency\" --&gt; Stream\\_Latency[\"Very Low\"]\n\nHybrid -- \"Data Stored in PBI?\" --&gt; Hybrid\\_Store[\"Yes, in Dataset DB\"]\nHybrid -- \"Historical Reporting?\" --&gt; Hybrid\\_Report[\"Yes\"]\nHybrid -- \"Real-time Tiles?\" --&gt; Hybrid\\_Tiles[\"Yes (Very Low Latency for Stream part)\"]\nHybrid -- \"Latency\" --&gt; Hybrid\\_Latency[\"Very Low (Stream) / Low (Push)\"]\nend\nstyle Push fill:#cce5ff,stroke:#0056b3\nstyle Streaming fill:#d1ecf1,stroke:#0c5460\nstyle Hybrid fill:#d4edda,stroke:#155724\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#162-technologies-for-implementing-real-time-solutions","title":"16.2 Technologies for Implementing Real-Time Solutions","text":"<ul> <li>Power BI REST API: The primary method for pushing data into push or hybrid  datasets. Applications or services can make HTTP POST requests to the dataset's API endpoint.</li> <li>Azure Stream Analytics (ASA): A fully managed event-processing engine in Azure. ASA  can ingest data from sources like Azure Event Hubs, IoT Hub, or Blob Storage, perform real-time  transformations or aggregations, and output directly to a Power BI push/streaming dataset. This is a  very common pattern for IoT and high-throughput scenarios.</li> <li>Microsoft Power Automate (Flow): Can be used to create flows that trigger on  various events (e.g., a new item in a SharePoint list, a new tweet) and then push data to a Power BI  push dataset. Suitable for lower-volume, event-driven scenarios.</li> <li>Microsoft Fabric Eventstream: A feature in Microsoft Fabric that provides a no-code  experience to ingest, capture, transform, and route real-time events from various sources (e.g.,  Azure Event Hubs, Kafka, sample data) to multiple destinations, including KQL databases, Lakehouses,  and directly into Power BI streaming datasets. This simplifies building end-to-end real-time  analytics pipelines within Fabric.</li> <li>DirectQuery with Automatic Page Refresh: For some DirectQuery sources, you can  enable \"Automatic Page Refresh\" in Power BI reports. This causes the report page (and its visuals)  to periodically re-query the underlying source, providing a near real-time view. This is different  from streaming datasets but achieves a similar outcome for supported sources. Requires  Premium/Fabric capacity for frequent refreshes.</li> </ul> 16.2 Technologies for Implementing Real-Time Solutions Diagram <pre><code>graph LR\nsubgraph \"Real-Time Data Ingestion Pipeline Example (IoT)\"\nIoT\\_Device[\"IoT Device/Sensor\"] -- \"Sends Telemetry\" --&gt; IoT\\_Hub[\"Azure IoT Hub\"]\nIoT\\_Hub -- \"Streams Data To\" --&gt; ASA\\_Fabric[\"Azure Stream Analytics / Fabric Eventstream\"]\n\nASA\\_Fabric -- \"Processes &amp; Transforms Data\" --&gt; PBI\\_PushAPI[\"Power BI Push/Streaming API Endpoint\"]\n\nPBI\\_PushAPI -- \"Populates\" --&gt; PBI\\_Dataset[\"Power BI Streaming/Hybrid Dataset\"]\n\nPBI\\_Dataset -- \"Powers\" --&gt; LiveDashboard[\"Live Dashboard Tiles (Real-Time Updates)\"]\nPBI\\_Dataset -- \"(If Hybrid/Push)\" --&gt; HistoricalReport[\"Power BI Report (Historical Analysis)\"]\nend\nstyle IoT\\_Device fill:#e9ecef,stroke:#6c757d\nstyle ASA\\_Fabric fill:#d1ecf1,stroke:#0c5460\nstyle LiveDashboard fill:#fff3cd,stroke:#856404\n\nBuilding real-time dashboards requires careful consideration of data volume, velocity, desired latency,\nand whether historical data storage is needed. The choice of dataset type and ingestion technology\ndepends heavily on these factors.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#17-embedding-scenarios","title":"17. Embedding Scenarios","text":"<p>Power BI Embedding allows you to integrate Power BI reports, dashboards, and tiles directly into your  custom applications, websites, or portals. This creates a seamless user experience by bringing  interactive analytics to where your users already work, leveraging Power BI's powerful engine while  maintaining your application's branding and context.</p> <p>There are two primary embedding scenarios, often referred to as \"User-Owns-Data\" and \"App-Owns-Data\":</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#171-embed-for-your-organization-user-owns-data-secure-embed","title":"17.1 Embed for Your Organization (User-Owns-Data / Secure Embed)","text":"<p>This scenario is for embedding Power BI content for internal users within your  organization who have Power BI accounts.</p> <ul> <li>Authentication: Users authenticate using their own Azure Active Directory (Azure  AD) credentials. Single Sign-On (SSO) is typically seamless.</li> <li>Licensing:<ul> <li>The user viewing the embedded content generally needs a Power BI Pro or Premium Per  User (PPU) license.</li> <li>Alternatively, the workspace containing the Power BI content must be assigned to a  Power BI Premium (P SKU) or Fabric (F SKU) capacity. In this case, even  users with Free Power BI licenses (or no license if configured) can view the content if they  have permissions to the content itself.</li> </ul> </li> <li>Permissions: Users see data based on their own Power BI permissions to the  underlying reports, dashboards, and datasets, including any Row-Level Security (RLS) applied to  them.</li> <li>Implementation: Typically involves using the Power BI JavaScript SDK. You obtain an  Azure AD access token for the logged-in user and use it to embed the content.</li> <li>Use Cases:<ul> <li>Embedding reports in SharePoint Online pages.</li> <li>Integrating dashboards into Microsoft Teams tabs.</li> <li>Adding analytics to custom internal web applications or portals.</li> </ul> </li> </ul>"},{"location":"courses/power-bi-service/powerbi-service-docs/#172-embed-for-your-customers-app-owns-data-isv-embedding","title":"17.2 Embed for Your Customers (App-Owns-Data / ISV Embedding)","text":"<p>This scenario is designed for Independent Software Vendors (ISVs) or organizations that want to embed  analytics into applications for external users (their customers) who typically do not  have (and do not need) their own Power BI accounts or licenses.</p> <ul> <li>Authentication: Your application is responsible for authenticating its own users  (e.g., using its own identity system, Azure AD B2C, etc.). Power BI itself does not authenticate  these external end-users.</li> <li>Service Identity: The application uses a dedicated Azure AD identity \u2013 either a  Service Principal (recommended) or a \"master user\" account (less recommended for  security and throttling reasons) \u2013 to authenticate with Power BI and access the content. This  service identity must have appropriate permissions (e.g., Admin/Member access) to the Power BI  workspace containing the content.</li> <li>Licensing: External end-users do not need individual Power BI  licenses. Instead, the organization providing the application must purchase and assign the Power BI  workspace to a Power BI Embedded (A SKU) capacity or a Fabric (F SKU) capacity.  Billing is based on the capacity purchased, not per user.</li> <li>Embed Tokens: Your application backend requests an \"embed token\" from the Power BI  service for each specific piece of content an external user needs to view. This token grants  temporary, specific access. RLS can be enforced by passing identity information when requesting the  embed token.</li> <li>Implementation: Requires more backend development. The Power BI JavaScript SDK is  used on the client-side to render the content using the embed token.</li> <li>Use Cases:<ul> <li>SaaS applications offering built-in analytics to their subscribers.</li> <li>Customer-facing portals providing personalized dashboards.</li> <li>Any application where you want to provide rich analytics to external users without requiring  them to have Power BI licenses.</li> </ul> </li> </ul> 17.2 Embed for Your Customers (App-Owns-Data / ISV Embedding) Diagram <pre><code>graph TD\nsubgraph \"Power BI Embedding Scenarios\"\ndirection LR\n\nsubgraph \"Embed for Your Organization (User-Owns-Data)\"\nUser\\_Internal[\"Internal User (with PBI License or Content in Premium/Fabric)\"] -- \"Logs into App/Portal\nwith Azure AD\" --&gt; App\\_Internal[\"Custom Internal App / SharePoint / Teams\"]\nApp\\_Internal -- \"Requests PBI Content (using User's AAD Token)\" --&gt; PBI\\_Service\\_UserOwns[\"Power BI\nService\"]\nPBI\\_Service\\_UserOwns -- \"Returns Content (RLS Applied)\" --&gt; App\\_Internal\nApp\\_Internal -- \"Renders Content via JS SDK\" --&gt; Embedded\\_Content\\_Internal[\"Embedded Report/Dashboard\"]\nstyle User\\_Internal fill:#cce5ff,stroke:#0056b3\nend\n\nsubgraph \"Embed for Your Customers (App-Owns-Data)\"\nUser\\_External[\"External Customer (No PBI License Needed)\"] -- \"Logs into ISV App\" --&gt; App\\_External[\"ISV\nSaaS Application\"]\nApp\\_External -- \"Authenticates User (App's Own System)\" --&gt; App\\_Backend[\"Application Backend\"]\nApp\\_Backend -- \"Authenticates to PBI with Service Principal\" --&gt; PBI\\_Service\\_AppOwns[\"Power BI Service\n(Workspace in A/F SKU Capacity)\"]\nPBI\\_Service\\_AppOwns -- \"Issues Embed Token (Can include RLS rules)\" --&gt; App\\_Backend\nApp\\_Backend -- \"Passes Embed Token to Client\" --&gt; App\\_External\nApp\\_External -- \"Renders Content via JS SDK using Embed Token\" --&gt; Embedded\\_Content\\_External[\"Embedded\nReport/Dashboard for Customer\"]\nstyle User\\_External fill:#d4edda,stroke:#155724\nstyle PBI\\_Service\\_AppOwns fill:#fff3cd,stroke:#856404\nend\nend\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#173-power-bi-javascript-sdk","title":"17.3 Power BI JavaScript SDK","text":"<p>Regardless of the scenario, the Power BI JavaScript SDK is crucial for client-side embedding. It allows  you to:</p> <ul> <li>Embed reports, dashboards, tiles, Q&amp;A, and paginated reports.</li> <li>Configure embed settings (e.g., show/hide filter pane, page navigation).</li> <li>Handle events (e.g., report loaded, page changed, data selected).</li> <li>Interact with embedded content programmatically (e.g., apply filters, update settings, export data).</li> </ul> <p>Example snippet for embedding a report (conceptual):</p> <pre><code>// 1. Get a reference to the container div\nconst reportContainer = document.getElementById('reportContainer');\n\n// 2. Define the embed configuration\nconst embedConfig = {\n    type: 'report', // report, dashboard, tile, etc.\n    id: 'REPORT_ID_HERE',\n    embedUrl: 'EMBED_URL_FROM_PBI_OR_API',\n    accessToken: 'ACCESS_TOKEN_OR_EMBED_TOKEN_HERE', // AAD token for User-Owns-Data, Embed token for App-Owns-Data\n    tokenType: models.TokenType.Aad, // or models.TokenType.Embed\n    settings: {\n        panes: {\n            filters: { expanded: false, visible: true },\n            pageNavigation: { visible: true, position: models.PageNavigationPosition.Bottom }\n        }\n        // ... other settings\n    }\n};\n\n// 3. Embed the report\nlet report = powerbi.embed(reportContainer, embedConfig);\n\n// 4. (Optional) Handle events\nreport.on('loaded', function() {\n    console.log('Report loaded!');\n});\nreport.on('error', function(event) {\n    console.error(event.detail);\n});\n</code></pre> <p>Choosing the right embedding scenario depends heavily on your target audience (internal vs. external) and  your licensing/cost model.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#18-ai-powered-features","title":"18. AI-Powered Features","text":"<p>Power BI integrates a suite of Artificial Intelligence (AI) capabilities, often leveraging Azure  Cognitive Services and Automated Machine Learning (AutoML) technologies. These features are designed to  help users discover deeper insights, understand data drivers, identify anomalies, and even generate  narratives or DAX measures with assistance from AI.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#181-ai-visuals-insights","title":"18.1 AI Visuals &amp; Insights","text":"<p>Power BI includes several built-in visuals and features that use AI to provide automated insights  directly within reports:</p> <ul> <li>Key Influencers Visual: Analyzes your data and identifies the main factors  (influencers) that drive a particular metric or outcome. For example, it can show what factors  contribute most to customer churn, high sales, or product defects. It presents results in an  easy-to-understand format.</li> <li>Decomposition Tree Visual: Allows users to explore data by drilling down into  different dimensions or categories to understand the composition of a metric. The visual can also  use AI to find interesting splits in the data (\"AI Splits\"), guiding the user to high or low values  of a chosen metric across different dimensions.</li> <li>Anomaly Detection (on Line Charts): When enabled on a time series line chart, this  feature automatically detects and highlights anomalies (unexpected data points or outliers) in the  data. It also provides a confidence band (expected range) and allows users to explore potential  explanations for these anomalies.</li> <li>Smart Narratives Visual: Automatically generates textual summaries and insights  based on the data in your report visuals. It can describe trends, key takeaways, and significant  changes in natural language. The narrative is dynamic and updates as data or filters change.</li> <li>Q&amp;A (Question &amp; Answer): Allows users to ask questions about their data in natural  language (e.g., \"What were the total sales last quarter by region?\") and Power BI attempts to answer  by generating appropriate visuals or results.</li> <li>Quick Insights (on Datasets): Users can run \"Quick Insights\" on a dataset, and  Power BI will automatically search for interesting patterns, trends, correlations, and outliers in  the data, presenting them as a set of pre-built visuals.</li> </ul> 18.1 AI Visuals &amp; Insights Diagram <pre><code>graph TD\nsubgraph \"Key AI Visuals &amp; Insights in Power BI\"\nData[\"Power BI Dataset\"] --&gt; KIVisual[\"Key Influencers Visual (Finds Drivers)\"]\nData --&gt; DecompTree[\"Decomposition Tree (AI Splits for Exploration)\"]\nData --&gt; AnomalyDetect[\"Anomaly Detection (on Line Charts - Finds Outliers)\"]\nData --&gt; SmartNarrative[\"Smart Narratives (Auto-Generated Text Summaries)\"]\nData --&gt; QnA[\"Q&amp;A Visual (Natural Language Queries)\"]\nData -- \"Run On\" --&gt; QuickInsights[\"Quick Insights (Automated Pattern Discovery)\"]\n\nKIVisual --&gt; Insights\\_Drivers[\"Insights: 'What drives Metric X?'\"]\nDecompTree --&gt; Insights\\_Breakdown[\"Insights: 'How is Metric Y composed?'\"]\nAnomalyDetect --&gt; Insights\\_Outliers[\"Insights: 'Unexpected spikes/dips in Metric Z'\"]\nSmartNarrative --&gt; Insights\\_Summary[\"Insights: 'Textual summary of report findings'\"]\nend\nstyle Data fill:#cce5ff,stroke:#0056b3\nstyle KIVisual fill:#d4edda,stroke:#155724\nstyle SmartNarrative fill:#d4edda,stroke:#155724\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#182-automl-integration-in-dataflows-datamarts","title":"18.2 AutoML Integration (in Dataflows &amp; Datamarts)","text":"<p>Power BI allows users (typically with Premium/Fabric capacity) to build, train, and apply machine  learning models directly within Power BI dataflows (and by extension, datamarts) using an Automated  Machine Learning (AutoML) interface. This democratizes machine learning by simplifying the model  creation process.</p> <p>The general AutoML workflow in Power BI involves:</p> <ol> <li>Select Data: Choose an entity (table) in your dataflow that contains historical  data with known outcomes.</li> <li>Choose Model Type: Select the type of prediction you want to make (e.g., Binary  Prediction like churn/no-churn, Classification like customer segmentation, Regression like sales  forecasting).</li> <li>Configure Model: Specify the outcome field (what you want to predict) and map input  data fields. Power BI helps with feature selection and preprocessing.</li> <li>Train Model: Power BI's AutoML engine then automatically trains multiple different  algorithms, tunes hyperparameters, and selects the best-performing model based on the chosen  evaluation metric.</li> <li>Review Model Performance: A training report is generated showing model accuracy,  key influencers, and other performance details.</li> <li>Apply Model: The trained model can then be applied to the dataflow (or new data) to  generate predictions. These predictions are often output as a new entity (table) in the dataflow,  which can then be used in Power BI datasets and reports.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#183-copilot-for-power-bi-generative-ai-assistance-preview","title":"18.3 Copilot for Power BI (Generative AI Assistance - Preview)","text":"<p>Copilot for Power BI is an AI assistant, leveraging large language models (LLMs), designed to help users  work more efficiently and creatively with their data. It's currently in preview and its capabilities are  evolving.</p> <p>Potential Copilot functionalities include:</p> <ul> <li>DAX Measure Generation: Users can describe the calculation they need in plain  language (e.g., \"Calculate year-over-year sales growth\"), and Copilot can suggest the DAX code for  the measure.</li> <li>Report Creation Assistance: Users might describe the type of report or visuals they  want, and Copilot could help generate a starting point.</li> <li>Data Summarization &amp; Narrative Generation: Similar to Smart Narratives, but  potentially more interactive and conversational, helping to summarize visuals or entire report  pages.</li> <li>Insight Discovery: Help users find interesting patterns or insights in their data  by understanding natural language questions.</li> </ul> <p>Using Copilot features typically requires a specific Copilot for Power BI license and may have  other prerequisites related to capacity and tenant settings.</p> <p>These AI features aim to make data analysis more accessible, efficient, and insightful for a broader  range of users, from business analysts to data scientists.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#19-microsoft-fabric-onelake-integration","title":"19. Microsoft Fabric &amp; OneLake Integration","text":"<p>Microsoft Fabric is an all-in-one, SaaS-based analytics platform that unifies various data and analytics  workloads into a single, integrated environment. It aims to simplify data estates by bringing together  data engineering, data science, data warehousing, real-time analytics, and business intelligence (Power  BI) onto a shared foundation. Power BI is a core experience within Fabric, and its integration is  pivotal.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#191-onelake-the-onedrive-for-data","title":"19.1 OneLake: The OneDrive for Data","text":"<p>OneLake is a single, unified, logical data lake for the entire organization,  automatically provisioned with every Microsoft Fabric tenant. It's designed to be the central repository  for all analytical data.</p> <ul> <li>Unified Storage: All Fabric data items (Lakehouses, Warehouses, KQL Databases)  store their data in OneLake in an open format, primarily Delta Parquet. This eliminates data silos  and reduces data duplication and movement.</li> <li>Tenant-Wide &amp; Domain-Specific: While there's one logical lake for the tenant, data  can be organized into domains and workspaces for better governance and discoverability.</li> <li>Open Format: Data stored in Delta Parquet format is accessible by various compute  engines within Fabric (Spark, T-SQL, KQL, Analysis Services for Power BI) and also by external tools  that understand these formats.</li> <li>Shortcuts: OneLake supports \"Shortcuts,\" which are like symbolic links to data  residing in other storage locations (e.g., Azure Data Lake Storage Gen2, Amazon S3, Google Cloud  Storage - planned). This allows you to include external data in OneLake without physically copying  or moving it, enabling analysis across disparate data sources.</li> </ul> 19.1 OneLake: The OneDrive for Data Diagram <pre><code>graph TD\nsubgraph \"OneLake in Microsoft Fabric\"\nTenant[\"Microsoft Fabric Tenant\"] -- \"Contains One Logical\" --&gt; OneLake[\"OneLake (Unified Data Lake)\"]\n\nOneLake -- \"Stores Data For\" --&gt; Lakehouse[\"Fabric Lakehouse (Delta/Parquet)\"]\nOneLake -- \"Stores Data For\" --&gt; Warehouse[\"Fabric Warehouse (Delta/Parquet)\"]\nOneLake -- \"Stores Data For\" --&gt; KQL\\_DB[\"Fabric KQL Database\"]\n\nLakehouse -- \"Accessible By\" --&gt; SparkEngine[\"Spark Engine (Data Engineering/Science)\"]\nWarehouse -- \"Accessible By\" --&gt; SQLEngine[\"SQL Engine (T-SQL for Warehousing)\"]\nKQL\\_DB -- \"Accessible By\" --&gt; KQLEngine[\"KQL Engine (Real-Time Analytics)\"]\n\nOneLakeData[\"Data in OneLake (Delta/Parquet)\"] --&gt; PBI\\_Engine[\"Power BI Engine (Analysis Services)\"]\nPBI\\_Engine --&gt; PBI\\_Reports[\"Power BI Reports &amp; Datasets\"]\n\nExternalADLS[\"External ADLS Gen2\"] -- \"Shortcut To\" --&gt; OneLake\nExternalS3[\"External Amazon S3\"] -- \"Shortcut To\" --&gt; OneLake\nOneLake -- \"Provides Virtualized Access To\" --&gt; DataViaShortcuts[\"Data via Shortcuts\"]\nDataViaShortcuts --&gt; SparkEngine\nDataViaShortcuts --&gt; SQLEngine\nDataViaShortcuts --&gt; PBI\\_Engine\nend\nstyle OneLake fill:#cce5ff,stroke:#0056b3,stroke-width:3px\nstyle Lakehouse fill:#d1ecf1,stroke:#0c5460\nstyle Warehouse fill:#d1ecf1,stroke:#0c5460\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#192-direct-lake-mode-for-power-bi-datasets","title":"19.2 Direct Lake Mode for Power BI Datasets","text":"<p>Direct Lake mode is a groundbreaking storage mode for Power BI datasets within the  Fabric ecosystem. It allows Power BI to directly query Delta Parquet files stored in a Fabric Lakehouse  (which resides in OneLake) without needing to import or duplicate the data into a separate Power BI  in-memory cache (as in Import mode) or constantly query a relational source (as in DirectQuery against a  traditional DB).</p> <ul> <li>No Data Duplication: Power BI reads data directly from the Delta files in OneLake.</li> <li>High Performance: Leverages the V-Order optimization of Delta Parquet files for  fast read performance, comparable to import mode.</li> <li>Real-time Data Access: As data is updated in the Lakehouse (e.g., by Spark jobs or  data pipelines), those changes are almost instantly available for querying in Power BI, eliminating  the need for traditional dataset refreshes for the Delta table portion.</li> <li>Scalability: Bypasses the memory constraints of traditional import mode datasets,  enabling analysis over very large datasets.</li> <li>Unified Semantic Layer: The Power BI dataset (semantic model) still defines  measures, relationships, hierarchies, etc., on top of the Lakehouse data.</li> </ul> <p>Direct Lake mode effectively combines the performance of import mode with the data freshness of  DirectQuery, specifically for data stored in Fabric Lakehouses.</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#193-fabric-experiences-and-power-bi-integration","title":"19.3 Fabric Experiences and Power BI Integration","text":"<p>Power BI is one of the core \"experiences\" within Fabric, alongside others like Data Factory, Synapse Data  Engineering, Synapse Data Science, Synapse Data Warehouse, and Synapse Real-Time Analytics.</p> <ul> <li>Shared Workspace: All Fabric items, including Power BI reports and datasets, reside  in Fabric workspaces.</li> <li>Unified Governance: Fabric leverages and extends Power BI's governance  capabilities, including tenant settings, information protection (sensitivity labels), lineage, and  monitoring.</li> <li>End-to-End Scenarios: Fabric enables seamless end-to-end analytics scenarios. For  example:<ol> <li>Use Data Factory (or Synapse Pipelines) to ingest data into a Lakehouse.</li> <li>Use Spark (Synapse Data Engineering) or SQL (Synapse Data Warehouse) to transform and  prepare data in the Lakehouse/Warehouse.</li> <li>Build Power BI Direct Lake datasets on top of this prepared data for interactive reporting  and analysis.</li> </ol> </li> <li>Simplified Capacity Model: Fabric uses a unified capacity model (F SKUs) that can  be used across all Fabric workloads, including Power BI. This simplifies purchasing and management  compared to separate capacities for different services.</li> </ul> 19.3 Fabric Experiences and Power BI Integration Diagram <pre><code>graph LR\nsubgraph \"End-to-End Analytics in Microsoft Fabric\"\nSourceSystems[\"Various Source Systems (DBs, APIs, Files)\"] --&gt; DataFactory[\"Fabric Data Factory\n(Ingestion &amp; Orchestration)\"]\nDataFactory -- \"Loads Data Into\" --&gt; Lakehouse\\_Fabric[\"Fabric Lakehouse (OneLake - Delta/Parquet)\"]\n\nLakehouse\\_Fabric -- \"Processed By\" --&gt; Spark\\_DataEng[\"Fabric Spark (Data Engineering -\nTransformations)\"]\nLakehouse\\_Fabric -- \"Queried/Managed By\" --&gt; SQL\\_Warehouse[\"Fabric SQL Endpoint / Warehouse (Data\nWarehousing)\"]\nLakehouse\\_Fabric -- \"Analyzed By\" --&gt; Python\\_DataSci[\"Fabric Notebooks (Data Science - ML)\"]\n\nSpark\\_DataEng -- \"Writes Processed Data To\" --&gt; Lakehouse\\_Fabric\nSQL\\_Warehouse -- \"Can Read/Write To\" --&gt; Lakehouse\\_Fabric\nPython\\_DataSci -- \"Reads/Writes To\" --&gt; Lakehouse\\_Fabric\n\nLakehouse\\_Fabric -- \"Direct Lake Mode Connection\" --&gt; PBI\\_Dataset\\_Fabric[\"Power BI Direct Lake Dataset\"]\nPBI\\_Dataset\\_Fabric -- \"Powers\" --&gt; PBI\\_Reports\\_Fabric[\"Power BI Reports &amp; Dashboards\"]\n\nPBI\\_Reports\\_Fabric -- \"Consumed By\" --&gt; BusinessUsers\\_Fabric[\"Business Users\"]\nend\nstyle Lakehouse\\_Fabric fill:#cce5ff,stroke:#0056b3,stroke-width:2px\nstyle PBI\\_Dataset\\_Fabric fill:#d4edda,stroke:#155724,stroke-width:2px\n\nMicrosoft Fabric represents a significant evolution, aiming to provide a more integrated, simplified, and\nAI-powered analytics experience, with Power BI playing a central role in the \"last mile\" of delivering\ninsights to users.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-docs/#20-consolidated-best-practices-for-power-bi-service","title":"20. Consolidated Best Practices for Power BI Service","text":"<p>Successfully deploying and managing Power BI in an enterprise requires adherence to best practices across  various domains, from security and governance to performance and user enablement. Here\u2019s a consolidated  list:</p>"},{"location":"courses/power-bi-service/powerbi-service-docs/#a-governance-security","title":"A. Governance &amp; Security","text":"<ol> <li>Security by Design with AAD Groups: Prioritize using Azure Active Directory (AAD)  security groups for managing access to workspaces, apps, and sharing. Avoid assigning permissions to  individual users wherever possible to simplify administration and auditing.</li> <li>Principle of Least Privilege: Assign users the minimum level of access (Admin,  Member, Contributor, Viewer) necessary for their roles.</li> <li>Tiered Workspace Strategy (Dev/Test/Prod): Implement separate workspaces for  Development, User Acceptance Testing (UAT)/Test, and Production environments. Utilize Power BI  Deployment Pipelines to automate and govern content promotion.</li> <li>Centralized &amp; Certified Datasets: Promote data reusability by creating and  endorsing \"golden\" or \"certified\" datasets for key business domains. This reduces data silos,  ensures consistency, and builds trust.</li> <li>Information Protection: Implement Microsoft Information Protection (MIP)  sensitivity labels to classify and protect sensitive data in datasets, reports, and exported files.  Configure default and mandatory labeling policies where appropriate.</li> <li>Tenant Settings Management: Regularly review and configure Power BI tenant settings  in the Admin Portal. Adopt a cautious approach: pilot new features with a small group before  enabling them tenant-wide. Disable high-risk features like \"Publish to Web\" unless explicitly  governed.</li> <li>Gateway Management: Keep on-premises data gateways updated. Use gateway clusters  for high availability and load balancing. Securely manage data source credentials.</li> <li>Audit Log Monitoring: Regularly extract and analyze Power BI Activity Logs to  monitor usage, sharing patterns, security events, and compliance. Build custom monitoring reports on  this data.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#b-development-performance","title":"B. Development &amp; Performance","text":"<ol> <li>Data Modeling Best Practices:<ul> <li>Optimize data models in Power BI Desktop (or Tabular Editor) before publishing: star schema  design, proper relationships, minimize column cardinality, remove unnecessary columns/rows.</li> <li>Write efficient DAX measures. Avoid complex iterative functions over large tables if  possible.</li> </ul> </li> <li>Data Ingestion &amp; Transformation:<ul> <li>Use Dataflows (especially Gen2 in Fabric) for reusable ETL and data preparation in the  cloud.</li> <li>Implement Incremental Refresh for large datasets and dataflows to reduce refresh times and  resource consumption.</li> </ul> </li> <li>Report Design for Performance:<ul> <li>Limit the number of visuals per report page.</li> <li>Optimize custom visuals or use certified ones.</li> <li>Minimize complex DAX in visuals; pre-calculate in measures or calculated columns where  appropriate.</li> <li>Use the Performance Analyzer in Power BI Desktop to identify bottlenecks.</li> </ul> </li> <li>Choose Appropriate Storage Mode:<ul> <li>Import: Best for performance for small to medium datasets. Data is  compressed and cached in memory.</li> <li>DirectQuery: For very large datasets or when near real-time data is needed  from the source. Performance depends heavily on the source system.</li> <li>Composite Models: Combine Import and DirectQuery tables in one model.</li> <li>Direct Lake (Fabric): Best for data in Fabric Lakehouses, offering  import-like performance with DirectQuery freshness.</li> </ul> </li> <li>Aggregation Tables: For large import or DirectQuery models, create pre-aggregated  tables to significantly speed up queries for summary-level visuals.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#c-deployment-operations-cicd","title":"C. Deployment &amp; Operations (CI/CD)","text":"<ol> <li>Version Control: Store Power BI Project (<code>.pbip</code>) files or Tabular Model  (<code>.bim</code>) files in a Git repository for version control, collaboration, and history  tracking.</li> <li>Automated Deployments: Use Power BI Deployment Pipelines, or script deployments  using REST APIs/PowerShell/CLI integrated with Azure DevOps or GitHub Actions for CI/CD.</li> <li>Parameterization: Parameterize data sources, connection strings, and other settings  to easily manage configurations across different environments (Dev/Test/Prod).</li> <li>Automated Testing: Implement automated tests for your data models (e.g., DAX query  tests, schema validation) as part of your CI/CD pipeline.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#d-user-enablement-adoption","title":"D. User Enablement &amp; Adoption","text":"<ol> <li>Training &amp; Skill Development: Provide regular training sessions for different user  personas (consumers, analysts, developers). Foster a community of practice.</li> <li>Clear Documentation: Document datasets, reports, data sources, and business logic.  Embed wikis or links to documentation within workspaces or apps. Use tools like Tabular Editor's  \"Document your model\" feature.</li> <li>Publish Design Guidelines: Establish and share internal design guidelines for  report creation to ensure consistency, branding, and usability.</li> <li>Champion Network: Identify and empower Power BI champions within business units to  drive adoption and provide local support.</li> <li>Feedback Mechanisms: Create channels for users to provide feedback on reports and  data, and to request new features or support.</li> </ol>"},{"location":"courses/power-bi-service/powerbi-service-docs/#e-cost-optimization-premiumfabric","title":"E. Cost Optimization (Premium/Fabric)","text":"<ol> <li>Right-Sizing Capacity: Monitor capacity utilization using the Fabric Capacity  Metrics App and adjust SKU sizes as needed.</li> <li>Autoscaling: Utilize autoscale features for Premium Gen2 or Fabric capacities to  handle demand fluctuations while controlling costs.</li> <li>Pause Capacities: If using A SKUs (Power BI Embedded) or F SKUs (Fabric) that  support pausing, pause capacities during off-peak hours (nights/weekends) if usage patterns allow.</li> <li>Optimize Workloads: Efficiently configure workloads (datasets, dataflows, paginated  reports) within capacities to maximize resource utilization.</li> <li>Monitor Storage: Keep an eye on OneLake storage consumption in Fabric.</li> </ol> E. Cost Optimization (Premium/Fabric) Diagram <pre><code>graph LR\nsubgraph \"Pillars of Power BI Enterprise Excellence\"\nGovSec[\"A. Governance &amp; Security\"] -- Leads to --&gt; TrustedEnvironment[\"Trusted &amp; Secure BI Environment\"]\nDevPerf[\"B. Development &amp; Performance\"] -- Leads to --&gt; EfficientInsights[\"Efficient &amp; Performant\nInsights\"]\nDeployOps[\"C. Deployment &amp; Operations (CI/CD)\"] -- Leads to --&gt; ReliableDelivery[\"Reliable &amp; Agile\nDelivery\"]\nUserAdopt[\"D. User Enablement &amp; Adoption\"] -- Leads to --&gt; DataDrivenCulture[\"Data-Driven Culture\"]\nCostOpt[\"E. Cost Optimization\"] -- Leads to --&gt; SustainableValue[\"Sustainable BI Value\"]\n\nTrustedEnvironment --&gt; OverallSuccess[\"Overall BI Success\"]\nEfficientInsights --&gt; OverallSuccess\nReliableDelivery --&gt; OverallSuccess\nDataDrivenCulture --&gt; OverallSuccess\nSustainableValue --&gt; OverallSuccess\nend\nstyle GovSec fill:#cce5ff,stroke:#0056b3\nstyle DevPerf fill:#d1ecf1,stroke:#0c5460\nstyle DeployOps fill:#e9ecef,stroke:#6c757d\nstyle UserAdopt fill:#d4edda,stroke:#155724\nstyle CostOpt fill:#fff3cd,stroke:#856404\nstyle OverallSuccess fill:#f8d7da,stroke:#721c24,stroke-width:3px\n\nBy focusing on these best practices, organizations can maximize the value of their Power BI investment,\nfoster a data-driven culture, and ensure their BI solutions are robust, secure, and scalable.\n\n\n\n\n\u00a9 2025 Power BI Service Enterprise Guide (Enhanced Edition). All rights reserved.\n\n\n\n</code></pre>"},{"location":"courses/power-bi-service/powerbi-service-overview/","title":"\ud83d\udcdd Power BI Service","text":"<p>A concise, quick-reference collection for busy data professionals.</p>"},{"location":"courses/power-bi-service/powerbi-service-overview/#available-topics","title":"Available Topics","text":"Topic Link Power BI Service Cheat Sheet Overview &amp; Quick Reference <p>\u2728 More topics coming soon\u2026 </p>"},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/","title":"Platform Foundations & Architecture","text":""},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#platform-foundations-architecture","title":"\ud83c\udfd7\ufe0f Platform Foundations &amp; Architecture","text":""},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#service-overview-licensing-capacity-data-models","title":"\ud83d\udef0\ufe0f Service Overview \u2014 Licensing, Capacity &amp; Data Models","text":"<p>Power BI/Fabric comes in multiple licensing tiers, each with different capacity models and dataset behaviours.</p> Tier / License Capacity Model Storage / Refresh Typical Audience Free Shared 10\u00a0GB (personal) Personal prototyping Pro Shared 10\u00a0GB/user \u00a0\u00b7\u00a0 8 refreshes/day Team collaboration Premium / Fabric Dedicated (F\u2011SKU) Up\u00a0to\u00a0100\u00a0TB/workspace \u00a0\u00b7\u00a0 48 refreshes/day Enterprise &amp; external sharing PPU Personal Premium Same as Premium (per\u2011user) Power users needing Paginated, AI"},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#import-vs-directquery-vs-composite","title":"Import vs DirectQuery vs Composite","text":"Mode Where data lives Pros Cons Import Cached in Power\u00a0BI Super\u2011fast, full DAX Dataset refresh overhead DirectQuery Source DB Real\u2011time; no cache Source load, limited DAX Composite Mix Best of both Complexity, model constraints \ud83c\udfd7\ufe0f Capacity &amp; Model Architecture Diagram <pre><code>graph TB\n    subgraph \"Capacity Tiers\"\n        A[\"Shared Capacity (Pro)\"]\n        B[\"Dedicated Capacity (Premium/Fabric)\"]\n    end\n    C[\"Workspaces\"] --&gt;|Import| D[\"Dataset Cache\"]\n    C --&gt;|DirectQuery| E[\"Source DB\"]\n    C --&gt;|Composite| D &amp; E\n    A --&gt; C\n    B --&gt; C</code></pre>"},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#tenant-architecture-regions-capacities-workspaces","title":"\ud83c\udf10 Tenant Architecture \u2014 Regions, Capacities, Workspaces","text":"<p>A tenant spans a geographic region and may hold multiple capacities. Each capacity contains workspaces, and every workspace contains items (reports, datasets, dataflows, notebooks, Lakehouses, etc.).</p> Layer Key Points Tenant Bound to an Azure region; governed by Tenant settings Capacity Resource envelope (v\u2011cores, memory) for Premium/Fabric workloads Workspace Security boundary &amp; Dev/Test/Prod landing zone Items Reports, Dashboards, Dataflows, Lakehouse objects, Notebooks, etc."},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#roles-responsibilities-raci-matrix","title":"\ud83d\udc65 Roles &amp; Responsibilities \u2014 RACI Matrix","text":"Role R A C I Typical Tasks Tenant Admin \u2705 \u2705 \u2705 Licensing, tenant settings, audit logs Capacity Admin \u2705 \u2705 \u2705 \u2705 Manage Premium/Fabric capacities Workspace Admin \u2705 \u2705 \u2705 Add members, publish apps, schedules Developer / Author \u2705 \u2705 Build reports, datasets, dataflows Consumer / Viewer \u2705 View &amp; interact, set personal alerts <p>\u2714\ufe0f = Responsibility / Accountability level as per RACI.</p>"},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#deployment-lifecycle-dev-test-prod-with-cicd","title":"\ud83d\ude80 Deployment Lifecycle \u2014 Dev \u2192 Test \u2192 Prod with CI/CD","text":"<p>A simple three\u2011workspace promotion pattern keeps artefacts isolated yet traceable across stages.</p> \ud83d\udd04 Promotion Flow Diagram <pre><code>sequenceDiagram\n    participant Dev  as \"Dev Workspace\"\n    participant Test as \"Test Workspace\"\n    participant Prod as \"Prod Workspace\"\n    participant Git  as \"Git Repo\"\n    participant AZP  as \"Azure DevOps Pipeline\"\n\n    Dev  --&gt;&gt; Git  : Commit .PBIX / JSON\n    Git  --&gt;&gt; AZP  : Trigger pipeline\n    AZP  --&gt;&gt; Test : Deploy artefacts\n    Test --&gt;&gt; AZP  : Validation gates\n    AZP  --&gt;&gt; Prod : Promote via Deployment Pipeline\n    Prod --&gt;&gt; Users: Publish App</code></pre> <p>Sample Azure DevOps <code>azure-pipelines.yml</code></p> <pre><code>trigger:\n  branches: [ main ]\n\nvariables:\n  envName: 'test'\n\nstages:\n- stage: Build\n  jobs:\n  - job: Export_PBIX\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - script: |\n        echo \"Export PBIX from source control\"\n- stage: Deploy\n  dependsOn: Build\n  jobs:\n  - deployment: Deploy_to_$(envName)\n    environment: $(envName)\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - task: PowerPlatformImportSolution@0\n            inputs:\n              authenticationType: 'ServicePrincipal'\n              solutionInputFile: '**/*.pbix'\n              environmentUrl: 'https://app.powerbi.com'\n</code></pre> <p>Tip: Swap the <code>envName</code> variable to <code>prod</code> and add manual approval for production gates.</p>"},{"location":"courses/power-bi-service/documents/platform-foundations-architecture/#key-takeaways","title":"\ud83c\udfaf Key Take\u2011aways","text":"<ol> <li>Know your capacity model. Import \u2260 DirectQuery, shared \u2260 dedicated.  </li> <li>Separate workspaces by stage to keep dev churn away from exec dashboards.  </li> <li>Define clear RACI early; governance gaps cost more than extra capacity.  </li> <li>Automate promotes with YAML + deployment pipelines \u2014 humans shouldn\u2019t click \u201cPublish to Prod\u201d.  </li> </ol> <p>\ud83d\udcda Need a deeper dive? - Power BI admin portal docs - CI/CD with Fabric deployment pipelines - Pricing calculator </p>"},{"location":"courses/python/python-overview/","title":"Python","text":"<p>(Coming soon! )</p>"},{"location":"courses/sql-admin/sql-admin-overview/","title":"SQL Admin","text":"<p>Welcome!  We\u2019ll add roadmap, notes, questions, &amp; social Q&amp;A pages soon.</p>"},{"location":"roadmap/","title":"Overview","text":"\ud83d\ude80 Learning Road-map \ud83d\ude80 Learning Road-map (30 weeks) <p>Click any module card for detailed weekly tasks &amp; resources.</p> Module-1 DW &amp; SQL Foundations Module-2 Python ETL &amp; CI Module-3 Spark &amp; Delta Performance Module-4 Streaming &amp; Data Quality Module-5 Azure Lakehouse Module-6 ADF &amp; Synapse Module-7 Fabric Lakehouse &amp; Real-Time Module-8 Interview &amp; System Design Plan for the Week"},{"location":"roadmap/module1/","title":"\ud83d\ude80 Module 1 \u2013 DW & SQL Foundations","text":"Module-1 \u2013 DW &amp; SQL Foundations \u2190 Back to Road-map Module-1 \u2013 DW &amp; SQL Foundations \ud83c\udfaf Objectives <ul> <li>Grasp dimensional-modeling patterns (Kimball vs Inmon). </li> <li>Master advanced Slowly Changing Dimensions (Types 1\u20136). </li> <li>Optimize SQL queries for analytics. </li> <li>Design a small data-warehouse schema end-to-end.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 1 \u2013 Data-warehouse overview; Inmon vs Kimball; fact/grain choices. </li> <li>Week 2 \u2013 Deep dive into SCD Types 1, 2, 3, 4, 6 (hybrids). </li> <li>Week 3 \u2013 SQL performance tuning: explain plans, indexing, partitioning. </li> <li>Week 4 \u2013 OLAP vs relational, materialized views, ETL mapping.</li> </ul> \ud83d\udd11Key Concepts 1. Schema Patterns <ul> <li>Star schema: single fact table + conformed dimensions </li> <li>Snowflake schema: normalized dimension tables </li> <li>Galaxy schema: multiple fact tables sharing dims </li> </ul> 2. Dimension Table Types <ul> <li>Conformed dimensions: reused across facts </li> <li>Junk dimensions: grouping low-cardinality flags </li> <li>Degenerate dimensions: dimension stored in fact (no separate table) </li> </ul> 3. Slowly Changing Dimensions (SCD) <ul> <li>Type 1: overwrite old value </li> <li>Type 2: add new row + versioning columns (<code>effective_date</code>, <code>end_date</code>) </li> <li>Type 3: add new attribute column (e.g. <code>prev_address</code>) </li> <li>Type 4: history table separate from current table </li> <li>Type 6 (Hybrid): combine Types 1/2/3 for specific use cases </li> </ul> 4. Fact Table Granularity <ul> <li>Transaction facts: one row per event </li> <li>Periodic snapshot facts: one row per period summary </li> <li>Accumulating snapshot facts: track process lifecycle </li> </ul> 5. Keys &amp; Indexing <ul> <li>Surrogate vs natural keys: hidden identity vs business key </li> <li>Clustered vs non-clustered indexes </li> <li>Columnstore indexes: for high-performance analytics </li> </ul> 6. Partitioning Strategies <ul> <li>Range partitioning: by date or numeric range </li> <li>List/hHash partitioning: by category or hash function </li> <li>Benefits: improved prune, maintenance, parallelism </li> </ul> 7. Materialized Views <ul> <li>Pre-computed aggregations </li> <li>Refresh modes: on-demand vs incremental </li> <li>Use-cases: speeding up complex joins/aggregates </li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>Retail DW Prototype: Model &amp; implement a 3-star schema in Postgres (DDL + ERD). </li> <li>SCD Pipeline: Load CSV to Type-2 dimension with history tracking. </li> <li>SQL Tuning Lab: Optimize 10 complex analytical queries on a 1 GB dataset.</li> </ul> \ud83d\udcda Resources <ul> <li>Ralph Kimball\u2019s Dimensional Modeling Techniques </li> <li>SQL Performance Explained by Markus Winand </li> <li>PostgreSQL docs: partitioning &amp; indexing </li> </ul>"},{"location":"roadmap/module2/","title":"\ud83d\udc0d Module 2 \u2013 Python ETL & CI","text":"Module-2 \u2013 Python ETL &amp; CI \u2190 Back to Road-map Module-2 \u2013 Python ETL &amp; CI \ud83c\udfaf Objectives <ul> <li>Build modular ETL pipelines in Python (Polars/pandas).</li> <li>Apply best practices: testing, logging, error handling.</li> <li>Set up CI/CD workflows for data code.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 5 \u2013 pandas vs Polars; memory &amp; performance trade-offs.</li> <li>Week 6 \u2013 ETL patterns: idempotency, config-driven pipelines.</li> <li>Week 7 \u2013 Code quality: pytest fixtures, structured logging, retry logic.</li> <li>Week 8 \u2013 CI basics: GitHub Actions for tests &amp; linting; packaging with Poetry.</li> </ul> \ud83d\udd11Key Concepts 1. Data Libraries <ul> <li>pandas: DataFrame basics, IO APIs, groupbys.</li> <li>Polars: lazy vs eager execution, memory efficiency.</li> <li>PyArrow: zero-copy IPC, Parquet integration.</li> </ul> 2. ETL Design Patterns <ul> <li>Incremental loads: watermark columns, CDC.</li> <li>Idempotent pipelines: safe retries without duplicates.</li> <li>Configuration: YAML/JSON configs, environment variables.</li> </ul> 3. Testing &amp; Error Handling <ul> <li>pytest fixtures: reusable setup/teardown.</li> <li>Mocking I/O: `monkeypatch`, `requests-mock`.</li> <li>Logging: Python `logging` module, retry/backoff strategies.</li> </ul> 4. Packaging &amp; CI/CD <ul> <li>Poetry: `pyproject.toml`, lock files.</li> <li>GitHub Actions: workflows, matrix builds, artifacts.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>CSV\u2192Postgres ETL Library: create a Python package with unit tests, logging, error handling.</li> <li>API Loader: fetch data from a REST API with retry &amp; backoff.</li> <li>CI Pipeline: configure GitHub Actions to run tests &amp; lint on every push.</li> </ul> \ud83d\udcda Resources <ul> <li>Polars &amp; pandas documentation</li> <li>pytest official guide</li> <li>GitHub Actions for Python projects</li> </ul>"},{"location":"roadmap/module3/","title":"\u26a1 Module 3 \u2013 Spark & Delta Performance","text":"Module-3 \u2013 Spark &amp; Delta Performance \u2190 Back to Road-map Module-3 \u2013 Spark &amp; Delta Performance \ud83c\udfaf Objectives <ul> <li>Master Spark DataFrame API &amp; Catalyst optimizer.</li> <li>Optimize jobs: partitioning, caching, broadcast joins, AQE.</li> <li>Leverage Delta Lake: ACID, schema evolution, time travel.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 9 \u2013 Spark architecture: DAG, stages, tasks, executors.</li> <li>Week 10 \u2013 Performance tuning: shuffles, partitioning, bucketing, broadcast hints.</li> <li>Week 11 \u2013 AQE &amp; advanced caching strategies.</li> <li>Week 12 \u2013 Delta Lake deep dive: MERGE, Z-ordering, compaction, time travel.</li> </ul> \ud83d\udd11Key Concepts 1. Spark APIs <ul> <li>RDD vs DataFrame vs Dataset.</li> </ul> 2. Catalyst Optimizer <ul> <li>Logical plan \u2192 optimized logical \u2192 physical plan.</li> </ul> 3. Joins &amp; Shuffles <ul> <li>Broadcast joins vs shuffle joins, skew mitigation.</li> </ul> 4. File Formats <ul> <li>Parquet internals, Delta Lake commit log.</li> </ul> 5. Delta Lake Features <ul> <li>ACID transactions, time travel, schema evolution.</li> </ul> 6. Partitioning &amp; Z-Ordering <ul> <li>Static vs dynamic partitioning, Z-order clustering.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>Taxi Data Pipeline: Ingest 1 GB CSV \u2192 Delta Bronze/Silver/Gold on ADLS Gen2.</li> <li>Skew Handling: Benchmark &amp; fix a skewed join via broadcast/salting.</li> <li>Delta Demo: Implement schema evolution &amp; time-travel queries.</li> </ul> \ud83d\udcda Resources <ul> <li>Databricks Spark documentation</li> <li>Delta Lake official guide</li> <li>Spark: The Definitive Guide by Bill Chambers &amp; Matei Zaharia</li> </ul>"},{"location":"roadmap/module4/","title":"\ud83c\udf0a Module 4 \u2013 Streaming & Data Quality","text":"Module-4 \u2013 Streaming &amp; Data Quality \u2190 Back to Road-map Module-4 \u2013 Streaming &amp; Data Quality \ud83c\udfaf Objectives <ul> <li>Build robust streaming pipelines using Spark Structured Streaming.</li> <li>Ingest from Azure Event Hubs or Kafka.</li> <li>Implement data-quality checks with Great Expectations &amp; dbt.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 13 \u2013 Lambda vs Kappa; Event Hubs fundamentals.</li> <li>Week 14 \u2013 Structured Streaming: triggers, checkpoints, watermarking.</li> <li>Week 15 \u2013 Stateful ops: windowed aggregates, late data handling.</li> <li>Week 16 \u2013 Data quality: GE suites; dbt tests &amp; lineage.</li> </ul> \ud83d\udd11 Key Concepts Ingestion Systems <ul> <li>Event Hubs vs Kafka: partitions, consumer groups.</li> </ul> Structured Streaming API <ul> <li><code>readStream</code>/<code>writeStream</code>, output modes, triggers.</li> </ul> Fault Tolerance <ul> <li>Checkpoints &amp; write-ahead logs for exactly-once.</li> </ul> Stateful Processing <ul> <li>Window functions, watermark &amp; state cleanup.</li> </ul> Data Quality Frameworks <ul> <li>Great Expectations expectations &amp; checkpoints.</li> <li>dbt tests: unique, not_null, relationships.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>JSON Stream \u2192 Lakehouse: consume events \u2192 Bronze/Silver Delta.</li> <li>Late Data Handling: manage late events with watermark.</li> <li>Quality Suite: build GE validations + dbt test report.</li> </ul> \ud83d\udcda Resources <ul> <li>Spark Structured Streaming guide</li> <li>Great Expectations docs</li> <li>dbt official documentation</li> </ul>"},{"location":"roadmap/module5/","title":"\u2601\ufe0f Module 5 \u2013 Azure Lakehouse","text":"Module-5 \u2013 Azure Lakehouse \u2190 Back to Road-map Module-5 \u2013 Azure Lakehouse \ud83c\udfaf Objectives <ul> <li>Secure &amp; configure ADLS Gen2 storage.</li> <li>Use Azure Databricks &amp; Unity Catalog for governance.</li> <li>Implement Bronze/Silver/Gold medallion pattern.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 17 \u2013 ADLS Gen2: hierarchical namespace, RBAC, firewall.</li> <li>Week 18 \u2013 Databricks: workspace, clusters, notebooks, jobs.</li> <li>Week 19 \u2013 Unity Catalog: metastore, schemas, permissions.</li> <li>Week 20 \u2013 Lakehouse best practices &amp; folder layout.</li> </ul> \ud83d\udd11Key Concepts ADLS Gen2 Features <ul> <li>Hierarchical namespace, storage tiers, lifecycle policies.</li> </ul> Security &amp; Networking <ul> <li>RBAC vs POSIX ACLs, service principals, private endpoints.</li> </ul> Databricks Administration <ul> <li>All-purpose vs job clusters, autoscaling, init scripts.</li> </ul> Unity Catalog Governance <ul> <li>Centralized metastore, grant/revoke, lineage.</li> </ul> Medallion Architecture <ul> <li>Bronze (raw), Silver (clean), Gold (aggregates).</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>Secure ADLS: implement ACLs, private endpoint, firewall rules.</li> <li>DBX Ingest Job: schedule notebook to load into Bronze.</li> <li>Governance Demo: set up Unity Catalog roles &amp; access.</li> </ul> \ud83d\udcda Resources <ul> <li>ADLS Gen2 documentation</li> <li>Databricks Unity Catalog guide</li> <li>Microsoft Fabric Lakehouse whitepapers</li> </ul>"},{"location":"roadmap/module6/","title":"\u2699\ufe0f Module 6 \u2013 ADF & Synapse","text":"Module-6 \u2013 ADF &amp; Synapse \u2190 Back to Road-map Module-6 \u2013 ADF &amp; Synapse \ud83c\udfaf Objectives <ul> <li>Build advanced ETL/ELT pipelines in Azure Data Factory.</li> <li>Tune Synapse serverless &amp; dedicated SQL pools for performance &amp; cost.</li> <li>Automate CI/CD for data workloads.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 21 \u2013 ADF fundamentals: pipelines, activities, parameters.</li> <li>Week 22 \u2013 Mapping Data Flows: transformations &amp; expression builder.</li> <li>Week 23 \u2013 Synapse Analytics: serverless vs dedicated tuning.</li> <li>Week 24 \u2013 CI/CD: ARM templates, Git integration, releases.</li> </ul> \ud83d\udd11Key Concepts ADF Core Concepts <ul> <li>Linked services, datasets, pipelines, triggers.</li> </ul> Integration Runtimes <ul> <li>Azure vs self-hosted IR, cost &amp; performance trade-offs.</li> </ul> Mapping Data Flows <ul> <li>Derived columns, aggregations, join behavior.</li> </ul> Synapse Analytics <ul> <li>Serverless SQL pool vs dedicated SQL pool: MPP, DWUs, distributions.</li> </ul> CI/CD Practices <ul> <li>ARM templates, Git branch strategies, release pipelines.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>End-to-End ADF: API \u2192 ADLS \u2192 Databricks \u2192 Synapse.</li> <li>Complex Data Flow: multi-step transformation &amp; error handling.</li> <li>Pipeline CI: deploy via Azure DevOps or GitHub Actions.</li> </ul> \ud83d\udcda Resources <ul> <li>Azure Data Factory documentation</li> <li>Azure Synapse performance tuning guide</li> <li>Azure DevOps Pipelines docs</li> </ul>"},{"location":"roadmap/module7/","title":"\ud83d\udcca Module 7 \u2013 Fabric Lakehouse & Real-Time","text":"Module-7 \u2013 Fabric Lakehouse &amp; Real-Time \u2190 Back to Road-map Module-7 \u2013 Fabric Lakehouse &amp; Real-Time \ud83c\udfaf Objectives <ul> <li>Leverage Microsoft Fabric\u2019s OneLake unified storage.</li> <li>Build real-time dashboards via DirectLake &amp; KQL.</li> <li>Enforce workspace governance &amp; item lifecycles.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 25 \u2013 Fabric overview: OneLake &amp; shortcuts.</li> <li>Week 26 \u2013 Fabric Data Factory pipelines &amp; notebooks.</li> <li>Week 27 \u2013 DirectLake in Power BI: live query patterns.</li> <li>Week 28 \u2013 Governance: roles, promotion pipelines, lineage.</li> </ul> \ud83d\udd11Key Concepts Fabric Architecture <ul> <li>OneLake: single logical lake across workloads; Shortcuts.</li> </ul> Pipeline Orchestration <ul> <li>Fabric Data Factory: activities, triggers, monitoring.</li> <li>Notebook workflows: Python &amp; SQL scheduling.</li> </ul> DirectLake Mode <ul> <li>Live connection, query folding &amp; limitations.</li> </ul> Kusto Query Language (KQL) <ul> <li>Basic syntax: where, summarize, real-time tables.</li> </ul> Governance &amp; Lifecycle <ul> <li>Workspace roles: Admin/Member/Viewer; Git integration; lineage.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>OneLake Ingest: batch + streaming data into Lakehouse.</li> <li>Real-Time Dashboard: DirectLake Power BI report.</li> <li>Governance Setup: role-based access &amp; deployment pipeline.</li> </ul> \ud83d\udcda Resources <ul> <li>Microsoft Fabric (DP-700/DP-600) documentation</li> <li>OneLake best practices</li> <li>Power BI DirectLake guide</li> </ul>"},{"location":"roadmap/module8/","title":"\ud83e\udd1d Module 8 \u2013 Interview & System Design","text":"Module-8 \u2013 Interview &amp; System Design \u2190 Back to Road-map Module-8 \u2013 Interview &amp; System Design \ud83c\udfaf Objectives <ul> <li>Polish r\u00e9sum\u00e9, LinkedIn &amp; portfolio.</li> <li>Drill common Data-Engineering interview Q&amp;A.</li> <li>Master system-design thinking &amp; whiteboard skills.</li> </ul> \ud83d\uddd3\ufe0f Weekly Plan <ul> <li>Week 29 \u2013 R\u00e9sum\u00e9 &amp; LinkedIn optimization; project stories.</li> <li>Week 30 \u2013 Mock interviews: STAR, technical Q&amp;A &amp; design drills.</li> </ul> \ud83d\udd11Key Concepts R\u00e9sum\u00e9 &amp; Portfolio <ul> <li>Structure: summary, skills, experience, projects.</li> <li>Keywords: Azure, Spark, Fabric, ETL, data-warehouse.</li> </ul> Behavioral Interviews <ul> <li>STAR method: Situation, Task, Action, Result.</li> </ul> Technical Q&amp;A <ul> <li>SQL tuning, Spark internals, ADF/Databricks patterns.</li> </ul> System Design <ul> <li>Architectures: Lambda, Kappa, Medallion.</li> <li>Components: ingestion, storage, transform, serve, monitoring.</li> </ul> Trade-offs <ul> <li>Consistency vs availability, cost vs performance, batch vs streaming.</li> </ul> \ud83d\udd28 Mini-Projects <ul> <li>Interview Q&amp;A: record answers to 50+ Azure DE questions.</li> <li>Design Whiteboard: sketch an end-to-end data platform for retail.</li> <li>Portfolio Polish: finalize GitHub repo + demo video.</li> </ul> \ud83d\udcda Resources <ul> <li>Designing Data-Intensive Applications (storage &amp; distributed systems)</li> <li>GitHub portfolio best practices</li> <li>LeetCode &amp; Glassdoor question banks</li> </ul>"},{"location":"roadmap/weeklyplan/","title":"\ud83d\udcc5 30-Week Azure Data-Engineering Study Plan","text":"<p>Each week ~15\u201320 hrs. Click a week to expand details.</p>"},{"location":"roadmap/weeklyplan/#week-1-data-warehouse-overview-inmon-vs-kimball-factgrain-choices-20-h","title":"Week 1 \ud83d\ude80 Data-Warehouse Overview; Inmon vs Kimball; Fact/Grain Choices (20 h)","text":"<ul> <li>\ud83d\udcd6 Read IBM\u2019s \u201cWhat is a Data Warehouse?\u201d (1 h)</li> <li>\ud83d\udcd6 Read Inmon\u2019s Building the Data Warehouse Intro &amp; Chapter 1 (2 h)</li> <li>\ud83d\udcd6 Read Kimball Toolkit Chapters 1\u20132 (star schemas &amp; conformed dims) (2 h)</li> <li>\ud83d\udcf9 Watch \u201cInmon vs Kimball\u201d on YouTube (45 min)</li> <li>\ud83d\udcdd Sketch 3 fact-table grains for a sample domain (1 h)</li> <li>\ud83d\udcbb Hands-on: design ERD for transactional &amp; snapshot facts (2 h)</li> <li>\ud83d\udcdd Build one-page Inmon vs Kimball cheat-sheet (2 h)</li> <li>\ud83c\udf93 Create Anki flashcards for key terms (1 h)</li> <li>\ud83d\udcdd Answer 5 mini \u201cdesign a DW\u201d prompts (2 h)</li> <li>\ud83d\udcda Review &amp; record yourself explaining both approaches (2.5 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-2-slowly-changing-dimensions-types-16-18-h","title":"Week 2 \ud83d\udd04 Slowly Changing Dimensions (Types 1\u20136) (18 h)","text":"<ul> <li>\ud83d\udcd6 Read Kimball Toolkit Ch 3 on SCD patterns (2 h)</li> <li>\ud83d\udcd6 Read blog posts on Types 4 &amp; 6 from Kimball Group (1 h)</li> <li>\ud83d\udcbb Hands-on: implement SCD 1/2/3 in SQL on sample data (3 h)</li> <li>\ud83d\udcbb Build a Python/Polars pipeline for SCD 2 with history table (3 h)</li> <li>\ud83d\udcdd Write an SQL stored proc for Type 2 merge logic (2 h)</li> <li>\ud83c\udf93 Flashcards: pros/cons of each SCD type (1 h)</li> <li>\ud83d\udcdd Scenario Q&amp;A: 10 design prompts (2 h)</li> <li>\ud83d\udcda Review &amp; refine your implementations (4 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-3-advanced-sql-performance-tuning-18-h","title":"Week 3 \u26a1 Advanced SQL Performance Tuning (18 h)","text":"<ul> <li>\ud83d\udcd6 Read SQL Performance Explained Ch 2\u20134 (3 h)</li> <li>\ud83d\udcbb Hands-on: run EXPLAIN on 10 analytical queries (2 h)</li> <li>\ud83d\udcbb Add &amp; test clustered/non-clustered/columnstore indexes (3 h)</li> <li>\ud83d\udcbb Implement range &amp; hash partitioning; test pruning (2 h)</li> <li>\ud83d\udcd6 Deep dive into columnstore indexes blog (1 h)</li> <li>\ud83d\udd28 Mini-Project: optimize a dashboard query on 1 GB dataset (4 h)</li> <li>\ud83c\udf93 Flashcards: index &amp; partition concepts (1 h)</li> <li>\ud83d\udcdd Self-quiz on tuning strategies (2 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-4-olap-vs-relational-materialized-views-etl-mapping-16-h","title":"Week 4 \ud83e\udde9 OLAP vs Relational; Materialized Views &amp; ETL Mapping (16 h)","text":"<ul> <li>\ud83d\udcd6 Read articles on OLAP cube architectures (1 h)</li> <li>\ud83d\udcbb Create &amp; refresh materialized views in Postgres (2 h)</li> <li>\ud83d\udcdd Draft source-to-target mapping doc for OLTP\u2192DW (2 h)</li> <li>\ud83d\udcd6 Read Kimball ETL mapping templates (1 h)</li> <li>\ud83d\udd28 Mini-Project: build SSAS cube vs relational report, compare perf (5 h)</li> <li>\ud83c\udf93 Flashcards: OLAP vs OLTP trade-offs (1 h)</li> <li>\ud83d\udcdd Write summary of best practices (2 h)</li> <li>\ud83d\udcda Review &amp; self-test (2 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-5-pandas-vs-polars-performance-memory-16-h","title":"Week 5 \ud83d\udc3c pandas vs Polars \u2013 Performance &amp; Memory (16 h)","text":"<ul> <li>\ud83d\udcd6 Read pandas &amp; Polars docs on IO &amp; lazy APIs (2 h)</li> <li>\ud83d\udcbb Benchmark CSV\u2192Parquet with pandas vs Polars (3 h)</li> <li>\ud83d\udcd6 Deep dive into Polars lazy mode (1 h)</li> <li>\ud83d\udcbb Hands-on: build a sample ETL in both libs; measure memory (3 h)</li> <li>\ud83d\udd28 Mini-Project: Polars pipeline to clean &amp; write Parquet (4 h)</li> <li>\ud83c\udf93 Flashcards: key API differences (1 h)</li> <li>\ud83d\udcdd Write a short comparison blog snippet (2 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-6-etl-design-patterns-idempotency-16-h","title":"Week 6 \ud83d\udee0\ufe0f ETL Design Patterns &amp; Idempotency (16 h)","text":"<ul> <li>\ud83d\udcd6 Read articles on config-driven pipelines (1 h)</li> <li>\ud83d\udcbb Build YAML/JSON-driven ETL framework (3 h)</li> <li>\ud83d\udcbb Implement watermarking &amp; safe retry logic (2 h)</li> <li>\ud83d\udd28 Mini-Project: generic CSV\u2192DB loader with idempotency (5 h)</li> <li>\ud83c\udf93 Flashcards: design pattern names &amp; use-cases (1 h)</li> <li>\ud83d\udcdd Self-review &amp; refine code (4 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-7-testing-logging-in-python-16-h","title":"Week 7 \u2699\ufe0f Testing &amp; Logging in Python (16 h)","text":"<ul> <li>\ud83d\udcd6 Read pytest docs on fixtures &amp; parametrization (1 h)</li> <li>\ud83d\udcbb Write unit tests for ETL transforms (3 h)</li> <li>\ud83d\udcd6 Read Python logging cookbook (1 h)</li> <li>\ud83d\udcbb Implement structured JSON logging &amp; retries (2 h)</li> <li>\ud83d\udd28 Mini-Project: add tests &amp; logs to your Week 6 pipeline (6 h)</li> <li>\ud83c\udf93 Review test coverage &amp; log outputs (2 h)</li> <li>\ud83d\udcdd Quiz yourself on pytest &amp; logging concepts (1 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#week-8-ci-basics-with-github-actions-14-h","title":"Week 8 \ud83d\udea6 CI Basics with GitHub Actions (14 h)","text":"<ul> <li>\ud83d\udcd6 Read GH Actions Python CI guide (1 h)</li> <li>\ud83d\udcbb Create <code>.github/workflows/ci.yml</code> to run pytest &amp; flake8 (3 h)</li> <li>\ud83d\udcd6 Read Poetry packaging docs (1 h)</li> <li>\ud83d\udcbb Configure Poetry &amp; lock file for your project (2 h)</li> <li>\ud83d\udd28 Mini-Project: integrate CI into your Week 7 repo (5 h)</li> <li>\ud83c\udf93 Review CI logs &amp; fix failures (2 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-912-spark-delta-performance-module-18-hwk","title":"Weeks 9\u201312 \ud83d\udd25 Spark &amp; Delta Performance Module (~18 h/wk)","text":"<ul> <li>Week 9 \u2013 Spark internals (DAG, stages, executors): read Spark: The Definitive Guide Ch 1\u20132, watch internals video, hands-on DAG inspection, mini-project Spark job (18 h)</li> <li>Week 10 \u2013 Joins &amp; shuffles: read docs on broadcast vs sort-merge, fix skewed joins, project on sample dataset (18 h)</li> <li>Week 11 \u2013 AQE &amp; caching: read official blog, enable AQE, benchmark with/without cache, mini-project (18 h)</li> <li>Week 12 \u2013 Delta Lake deep dive: read Delta Lake guide, implement MERGE/Z-ordering, time-travel queries, project (18 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-1316-streaming-data-quality-module-17-hwk","title":"Weeks 13\u201316 \ud83c\udf0a Streaming &amp; Data Quality Module (~17 h/wk)","text":"<ul> <li>Week 13 \u2013 Lambda vs Kappa &amp; Event Hubs basics: articles &amp; hands-on ingestion (17 h)</li> <li>Week 14 \u2013 Structured Streaming APIs: triggers, output modes, checkpoints, code labs (17 h)</li> <li>Week 15 \u2013 Stateful processing: window ops, watermark cleanup, demos (17 h)</li> <li>Week 16 \u2013 Data quality frameworks: Great Expectations suites &amp; dbt tests, quality dashboard (17 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-1720-azure-lakehouse-module-16-hwk","title":"Weeks 17\u201320 \ud83c\udfde\ufe0f Azure Lakehouse Module (~16 h/wk)","text":"<ul> <li>Week 17 \u2013 ADLS Gen2 setup &amp; security (RBAC, ACLs, firewall) (16 h)</li> <li>Week 18 \u2013 Databricks workspace, clusters &amp; notebooks (16 h)</li> <li>Week 19 \u2013 Unity Catalog governance &amp; lineage (16 h)</li> <li>Week 20 \u2013 Medallion pattern: implement Bronze/Silver/Gold pipeline (16 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-2124-adf-synapse-module-16-hwk","title":"Weeks 21\u201324 \ud83d\udd27 ADF &amp; Synapse Module (~16 h/wk)","text":"<ul> <li>Week 21 \u2013 ADF pipelines: linked services, datasets, triggers (16 h)</li> <li>Week 22 \u2013 Mapping Data Flows: transformations &amp; expressions (16 h)</li> <li>Week 23 \u2013 Synapse SQL pools: serverless vs dedicated tuning (16 h)</li> <li>Week 24 \u2013 CI/CD: ARM templates &amp; Git integration for ADF/Synapse (16 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-2528-fabric-lakehouse-real-time-module-16-hwk","title":"Weeks 25\u201328 \ud83c\udf10 Fabric Lakehouse &amp; Real-Time Module (~16 h/wk)","text":"<ul> <li>Week 25 \u2013 Fabric architecture: OneLake &amp; shortcuts (16 h)</li> <li>Week 26 \u2013 Fabric Data Factory pipelines &amp; notebooks (16 h)</li> <li>Week 27 \u2013 DirectLake in Power BI: live query patterns (16 h)</li> <li>Week 28 \u2013 Governance &amp; lifecycle: roles, promotion pipelines (16 h)</li> </ul>"},{"location":"roadmap/weeklyplan/#weeks-2930-interview-system-design-module-15-hwk","title":"Weeks 29\u201330 \ud83c\udfaf Interview &amp; System Design Module (~15 h/wk)","text":"<ul> <li>Week 29 \u2013 R\u00e9sum\u00e9 &amp; LinkedIn optimization; project storytelling (15 h)</li> <li>Week 30 \u2013 Mock interviews: STAR, technical Q&amp;A &amp; system-design drills (15 h)</li> </ul>"}]}